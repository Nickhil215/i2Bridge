Package Analysis Report
=====================

Package: /tmp/git_analyzer_uwbaquvw/examples

Metrics:
- Total Files: 48
- Total Classes: 7
- Total Functions: 225
- Total Imports: 0
- Average Complexity: 3.44
- Total Lines of Code: 6610
- Analysis Errors: 0


Code Components:

  ../docs/conf.py:
    Name: conf.py
    Packages: ['os', 'sys']
    Imports: ['os', 'sys']
    Description: 
    Description Embedding: 

  ../pymdp/agent.py:
    Name: agent.py
    Packages: ['warnings', 'np', 'pymdp', 'copy']
    Imports: ['warnings', 'np', 'pymdp.inference', 'pymdp.control', 'pymdp.learning', 'pymdp.utils', 'pymdp.maths', 'copy']
    Description: 
    Description Embedding: 

  ../pymdp/algos/fpi.py:
    Name: fpi.py
    Packages: ['np', 'pymdp', 'itertools', 'copy']
    Imports: ['np', 'pymdp.maths.spm_dot', 'pymdp.maths.dot_likelihood', 'pymdp.maths.get_joint_likelihood', 'pymdp.maths.softmax', 'pymdp.maths.calc_free_energy', 'pymdp.maths.spm_log_single', 'pymdp.maths.spm_log_obj_array', 'pymdp.utils.to_obj_array', 'pymdp.utils.obj_array', 'pymdp.utils.obj_array_uniform', 'itertools.chain', 'copy.deepcopy']
    Description: 
    Description Embedding: 

  ../pymdp/algos/mmp.py:
    Name: mmp.py
    Packages: ['np', 'pymdp', 'copy']
    Imports: ['np', 'pymdp.utils.to_obj_array', 'pymdp.utils.get_model_dimensions', 'pymdp.utils.obj_array', 'pymdp.utils.obj_array_zeros', 'pymdp.utils.obj_array_uniform', 'pymdp.maths.spm_dot', 'pymdp.maths.spm_norm', 'pymdp.maths.softmax', 'pymdp.maths.calc_free_energy', 'pymdp.maths.spm_log_single', 'pymdp.maths.factor_dot_flex', 'copy']
    Description: 
    Description Embedding: 

  ../pymdp/control.py:
    Name: control.py
    Packages: ['itertools', 'np', 'pymdp', 'copy']
    Imports: ['itertools', 'np', 'pymdp.maths.softmax', 'pymdp.maths.softmax_obj_arr', 'pymdp.maths.spm_dot', 'pymdp.maths.spm_wnorm', 'pymdp.maths.spm_MDP_G', 'pymdp.maths.spm_log_single', 'pymdp.maths.kl_div', 'pymdp.maths.entropy', 'pymdp.inference.update_posterior_states_factorized', 'pymdp.inference.average_states_over_policies', 'pymdp.utils', 'copy']
    Description: 
    Description Embedding: 

  ../pymdp/envs/env.py:
    Name: env.py
    Packages: []
    Imports: []
    Description: 
    Description Embedding: 

  ../pymdp/envs/tmaze.py:
    Name: tmaze.py
    Packages: ['pymdp', 'np']
    Imports: ['pymdp.envs.Env', 'pymdp.utils', 'pymdp.maths', 'np']
    Description: 
    Description Embedding: 

  ../pymdp/inference.py:
    Name: inference.py
    Packages: ['np', 'pymdp']
    Imports: ['np', 'pymdp.utils', 'pymdp.maths.get_joint_likelihood_seq', 'pymdp.maths.get_joint_likelihood_seq_by_modality', 'pymdp.algos.run_vanilla_fpi', 'pymdp.algos.run_vanilla_fpi_factorized', 'pymdp.algos.run_mmp', 'pymdp.algos.run_mmp_factorized', 'pymdp.algos._run_mmp_testing']
    Description: 
    Description Embedding: 

  ../pymdp/jax/agent.py:
    Name: agent.py
    Packages: ['pymath', 'jnp', 'jtu', 'jax', '', 'equinox', 'typing', 'jaxtyping', 'functools']
    Imports: ['pymath', 'jnp', 'jtu', 'jax.nn', 'jax.vmap', 'jax.random', '.inference', '.control', '.learning', '.utils', '.maths', 'equinox.Module', 'equinox.field', 'equinox.tree_at', 'typing.List', 'typing.Optional', 'jaxtyping.Array', 'functools.partial']
    Description: 
    Description Embedding: 

  ../pymdp/jax/algos.py:
    Name: algos.py
    Packages: ['jnp', 'jtu', 'jax', 'maths', 'typing', 'functools']
    Imports: ['jnp', 'jtu', 'jax.jit', 'jax.vmap', 'jax.grad', 'jax.lax', 'jax.nn', 'maths.compute_log_likelihood', 'maths.compute_log_likelihood_per_modality', 'maths.log_stable', 'maths.MINVAL', 'maths.factor_dot', 'maths.factor_dot_flex', 'typing.Any', 'typing.List', 'functools.partial']
    Description: 
    Description Embedding: 

  ../pymdp/jax/control.py:
    Name: control.py
    Packages: ['itertools', 'jnp', 'jtu', 'typing', 'functools', 'jax', 'jaxtyping', 'pymdp']
    Imports: ['itertools', 'jnp', 'jtu', 'typing.List', 'typing.Tuple', 'typing.Optional', 'functools.partial', 'jax.scipy.special.xlogy', 'jax.lax', 'jax.jit', 'jax.vmap', 'jax.nn', 'jax.random as jr', 'itertools.chain', 'jaxtyping.Array', 'pymdp.jax.maths.*']
    Description: 
    Description Embedding: 

  ../pymdp/jax/inference.py:
    Name: inference.py
    Packages: ['jnp', 'algos', 'jax', 'jaxtyping']
    Imports: ['jnp', 'algos.run_factorized_fpi', 'algos.run_mmp', 'algos.run_vmp', 'jax.tree_util as jtu', 'jax.lax', 'jax.experimental.sparse._base.JAXSparse', 'jax.experimental.sparse', 'jaxtyping.Array', 'jaxtyping.ArrayLike']
    Description: 
    Description Embedding: 

  ../pymdp/jax/learning.py:
    Name: learning.py
    Packages: ['maths', 'jax', 'jaxtyping']
    Imports: ['maths.multidimensional_outer', 'maths.dirichlet_expected_value', 'jax.tree_util.tree_map', 'jaxtyping.Array', 'jax.vmap', 'jax.nn']
    Description: 
    Description Embedding: 

  ../pymdp/jax/likelihoods.py:
    Name: likelihoods.py
    Packages: ['jnp', 'dist', 'jax', 'numpyro']
    Imports: ['jnp', 'dist', 'jax.lax', 'numpyro.plate', 'numpyro.sample', 'numpyro.deterministic', 'numpyro.contrib.control_flow.scan']
    Description: 
    Description Embedding: 

  ../pymdp/jax/maths.py:
    Name: maths.py
    Packages: ['jnp', 'functools', 'typing', 'jax', 'opt_einsum']
    Imports: ['jnp', 'functools.partial', 'typing.Optional', 'typing.Tuple', 'typing.List', 'jax.tree_util', 'jax.nn', 'jax.jit', 'jax.vmap', 'jax.lax', 'jax.scipy.special.xlogy', 'opt_einsum.contract']
    Description: 
    Description Embedding: 

  ../pymdp/jax/task.py:
    Name: task.py
    Packages: ['typing', 'jaxtyping', 'functools', 'equinox', 'jax', 'jnp']
    Imports: ['typing.Optional', 'typing.List', 'typing.Dict', 'jaxtyping.Array', 'jaxtyping.PRNGKeyArray', 'functools.partial', 'equinox.Module', 'equinox.field', 'equinox.tree_at', 'jax.vmap', 'jax.random as jr', 'jax.tree_util as jtu', 'jnp']
    Description: 
    Description Embedding: 

  ../pymdp/jax/utils.py:
    Name: utils.py
    Packages: ['jnp', 'typing']
    Imports: ['jnp', 'typing.Any', 'typing.Callable', 'typing.List', 'typing.NamedTuple', 'typing.Optional', 'typing.Sequence', 'typing.Union', 'typing.Tuple']
    Description: 
    Description Embedding: 

  ../pymdp/learning.py:
    Name: learning.py
    Packages: ['np', 'pymdp', 'copy']
    Imports: ['np', 'pymdp.utils', 'pymdp.maths', 'copy']
    Description: 
    Description Embedding: 

  ../pymdp/maths.py:
    Name: maths.py
    Packages: ['np', 'scipy', 'pymdp', 'itertools', 'opt_einsum']
    Imports: ['np', 'scipy.special', 'pymdp.utils', 'itertools.chain', 'opt_einsum.contract']
    Description: 
    Description Embedding: 

  ../pymdp/utils.py:
    Name: utils.py
    Packages: ['np', 'pd', 'sns', 'plt', 'warnings', 'itertools']
    Imports: ['np', 'pd', 'sns', 'plt', 'warnings', 'itertools']
    Description: 
    Description Embedding: 

  ../setup.py:
    Name: setup.py
    Packages: ['setuptools']
    Imports: ['setuptools']
    Description: 
    Description Embedding: 

Class Hierarchy:

  ../pymdp/agent.py::Agent:
    Bases: object
    Functions: 21
    Attributes: 0
    Start Line: 16
    End Line: 939
    Decorators: None
    Docstring: The Agent class, the highest-level API that wraps together processes for action, perception, and learning under active inference.

    The basic usage is as follows:

    >>> my_agent = Agent(A = A, B = C, <more_params>)
    >>> observation = env.step(initial_action)
    >>> qs = my_agent.infer_states(observation)
    >>> q_pi, G = my_agent.infer_policies()
    >>> next_action = my_agent.sample_action()
    >>> next_observation = env.step(next_action)

    This represents one timestep of an active inference process. Wrapping this step in a loop with an ``Env()`` class that returns
    observations and takes actions as inputs, would entail a dynamic agent-environment interaction.
    Description: 
    Description Embedding: 

  ../pymdp/envs/env.py::Env:
    Bases: object
    Functions: 10
    Attributes: 0
    Start Line: 11
    End Line: 87
    Decorators: None
    Docstring: The Env base class, loosely-inspired by the analogous ``env`` class of the OpenAIGym framework. 

    A typical workflow is as follows:

    >>> my_env = MyCustomEnv(<some_params>)
    >>> initial_observation = my_env.reset(initial_state)
    >>> my_agent.infer_states(initial_observation)
    >>> my_agent.infer_policies()
    >>> next_action = my_agent.sample_action()
    >>> next_observation = my_env.step(next_action)

    This would be the first step of an active inference process, where a sub-class of ``Env``, ``MyCustomEnv`` is initialized, 
    an initial observation is produced, and these observations are fed into an instance of ``Agent`` in order to produce an action,
    that can then be fed back into the the ``Env`` instance.
    Description: 
    Description Embedding: 

  ../pymdp/envs/tmaze.py::TMazeEnv:
    Bases: Env
    Functions: 15
    Attributes: 0
    Start Line: 25
    End Line: 189
    Decorators: None
    Docstring: Implementation of the 3-arm T-Maze environment
    Description: 
    Description Embedding: 

  ../pymdp/envs/tmaze.py::TMazeEnvNullOutcome:
    Bases: Env
    Functions: 12
    Attributes: 0
    Start Line: 192
    End Line: 346
    Decorators: None
    Docstring: Implementation of the 3-arm T-Maze environment where there is an additional null outcome within the cue modality, so that the agent
    doesn't get a random cue observation, but a null one, when it visits non-cue locations
    Description: 
    Description Embedding: 

  ../pymdp/jax/agent.py::Agent:
    Bases: Module
    Functions: 11
    Attributes: 41
    Start Line: 21
    End Line: 496
    Decorators: None
    Docstring: The Agent class, the highest-level API that wraps together processes for action, perception, and learning under active inference.

    The basic usage is as follows:

    >>> my_agent = Agent(A = A, B = C, <more_params>)
    >>> observation = env.step(initial_action)
    >>> qs = my_agent.infer_states(observation)
    >>> q_pi, G = my_agent.infer_policies()
    >>> next_action = my_agent.sample_action()
    >>> next_observation = env.step(next_action)

    This represents one timestep of an active inference process. Wrapping this step in a loop with an ``Env()`` class that returns
    observations and takes actions as inputs, would entail a dynamic agent-environment interaction.
    Description: 
    Description Embedding: 

  ../pymdp/jax/task.py::PyMDPEnv:
    Bases: Module
    Functions: 3
    Attributes: 3
    Start Line: 25
    End Line: 76
    Decorators: None
    Docstring: None
    Description: 
    Description Embedding: 

  ../pymdp/utils.py::Dimensions:
    Bases: object
    Functions: 1
    Attributes: 0
    Start Line: 19
    End Line: 37
    Decorators: None
    Docstring: The Dimensions class stores all data related to the size and shape of a model.
    Description: 
    Description Embedding: 

Functions:

  ../pymdp/agent.py::get_future_qs:
    name: get_future_qs
    Module: None
    Signature: get_future_qs(self)
    function_exe_cmd: pymdp.agent.Agent.get_future_qs(self)
    Parameters: ['self']
    Returns: None
    Start Line: 455
    End Line: 475
    Complexity: 2
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Returns the last ``self.policy_len`` timesteps of each policy-conditioned belief
        over hidden states. This is a step of pre-processing that needs to be done before computing
        the expected free energy of policies. We do this to avoid computing the expected free energy of 
        policies using beliefs about hidden states in the past (so-called "post-dictive" beliefs).

        Returns
        ---------
        future_qs_seq: ``numpy.ndarray`` of dtype object
            Posterior beliefs over hidden states under a policy, in the future. This is a nested ``numpy.ndarray`` object array, with one
            sub-array ``future_qs_seq[p_idx]`` for each policy. The indexing structure is policy->timepoint-->factor, so that 
            ``future_qs_seq[p_idx][t_idx][f_idx]`` refers to beliefs about marginal factor ``f_idx`` expected under policy ``p_idx`` 
            at future timepoint ``t_idx``, relative to the current timestep.
    Description: 
    Description Embedding: 
    Comments: ['this grabs only the last `policy_len`+1 beliefs about hidden states, under each policy']

  ../pymdp/agent.py::infer_policies:
    name: infer_policies
    Module: None
    Signature: infer_policies(self)
    function_exe_cmd: pymdp.agent.Agent.infer_policies(self)
    Parameters: ['self']
    Returns: None
    Start Line: 608
    End Line: 693
    Complexity: 6
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Perform policy inference by optimizing a posterior (categorical) distribution over policies.
        This distribution is computed as the softmax of ``G * gamma + lnE`` where ``G`` is the negative expected
        free energy of policies, ``gamma`` is a policy precision and ``lnE`` is the (log) prior probability of policies.
        This function returns the posterior over policies as well as the negative expected free energy of each policy.
        In this version of the function, the expected free energy of policies is computed using known factorized structure 
        in the model, which speeds up computation (particular the state information gain calculations).

        Returns
        ----------
        q_pi: 1D ``numpy.ndarray``
            Posterior beliefs over policies, i.e. a vector containing one posterior probability per policy.
        G: 1D ``numpy.ndarray``
            Negative expected free energies of each policy, i.e. a vector containing one negative expected free energy per policy.
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/agent.py::infer_states:
    name: infer_states
    Module: None
    Signature: infer_states(self, observation, distr_obs)
    function_exe_cmd: pymdp.agent.Agent.infer_states(self, observation, distr_obs)
    Parameters: ['self', 'observation', 'distr_obs']
    Returns: None
    Start Line: 478
    End Line: 550
    Complexity: 7
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Update approximate posterior over hidden states by solving variational inference problem, given an observation.

        Parameters
        ----------
        observation: ``list`` or ``tuple`` of ints
            The observation input. Each entry ``observation[m]`` stores the index of the discrete
            observation for modality ``m``.
        distr_obs: ``bool``
            Whether the observation is a distribution over possible observations, rather than a single observation.

        Returns
        ---------
        qs: ``numpy.ndarray`` of dtype object
            Posterior beliefs over hidden states. Depending on the inference algorithm chosen, the resulting ``qs`` variable will have additional sub-structure to reflect whether
            beliefs are additionally conditioned on timepoint and policy.
            For example, in case the ``self.inference_algo == 'MMP' `` indexing structure is policy->timepoint-->factor, so that 
            ``qs[p_idx][t_idx][f_idx]`` refers to beliefs about marginal factor ``f_idx`` expected under policy ``p_idx`` 
            at timepoint ``t_idx``.
    Description: 
    Description Embedding: 
    Comments: ['variational free energy of each policy']

  ../pymdp/agent.py::reset:
    name: reset
    Module: None
    Signature: reset(self, init_qs)
    function_exe_cmd: pymdp.agent.Agent.reset(self, init_qs)
    Parameters: ['self', 'init_qs']
    Returns: None
    Start Line: 349
    End Line: 394
    Complexity: 8
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Resets the posterior beliefs about hidden states of the agent to a uniform distribution, and resets time to first timestep of the simulation's temporal horizon.
        Returns the posterior beliefs about hidden states.

        Returns
        ---------
        qs: ``numpy.ndarray`` of dtype object
           Initialized posterior over hidden states. Depending on the inference algorithm chosen and other parameters (such as the parameters stored within ``edge_handling_paramss),
           the resulting ``qs`` variable will have additional sub-structure to reflect whether beliefs are additionally conditioned on timepoint and policy.
            For example, in case the ``self.inference_algo == 'MMP' `, the indexing structure of ``qs`` is policy->timepoint-->factor, so that 
            ``qs[p_idx][t_idx][f_idx]`` refers to beliefs about marginal factor ``f_idx`` expected under policy ``p_idx`` 
            at timepoint ``t_idx``. In this case, the returned ``qs`` will only have entries filled out for the first timestep, i.e. for ``q[p_idx][0]``, for all 
            policy-indices ``p_idx``. Subsequent entries ``q[:][1, 2, ...]`` will be initialized to empty ``numpy.ndarray`` objects.
    Description: 
    Description Embedding: 
    Comments: ["in the case you're doing MMP (i.e. you have an inference_horizon > 1), we have to account for policy- and timestep-conditioned posterior beliefs", '+ 1 to include belief about current timestep']

  ../pymdp/agent.py::sample_action:
    name: sample_action
    Module: None
    Signature: sample_action(self)
    function_exe_cmd: pymdp.agent.Agent.sample_action(self)
    Parameters: ['self']
    Returns: None
    Start Line: 695
    End Line: 721
    Complexity: 3
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Sample or select a discrete action from the posterior over control states.
        This function both sets or cachÃ©s the action as an internal variable with the agent and returns it.
        This function also updates time variable (and thus manages consequences of updating the moving reference frame of beliefs)
        using ``self.step_time()``.


        Returns
        ----------
        action: 1D ``numpy.ndarray``
            Vector containing the indices of the actions for each control factor
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/agent.py::set_latest_beliefs:
    name: set_latest_beliefs
    Module: None
    Signature: set_latest_beliefs(self, last_belief)
    function_exe_cmd: pymdp.agent.Agent.set_latest_beliefs(self, last_belief)
    Parameters: ['self', 'last_belief']
    Returns: None
    Start Line: 420
    End Line: 453
    Complexity: 7
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Both sets and returns the penultimate belief before the first timestep of the backwards inference horizon. 
        In the case that the inference horizon includes the first timestep of the simulation, then the ``latest_belief`` is
        simply the first belief of the whole simulation, or the prior (``self.D``). The particular structure of the ``latest_belief``
        depends on the value of ``self.edge_handling_params['use_BMA']``.

        Returns
        ---------
        latest_belief: ``numpy.ndarray`` of dtype object
            Penultimate posterior beliefs over hidden states at the timestep just before the first timestep of the inference horizon. 
            Depending on the value of ``self.edge_handling_params['use_BMA']``, the shape of this output array will differ.
            If ``self.edge_handling_params['use_BMA'] == True``, then ``latest_belief`` will be a Bayesian model average 
            of beliefs about hidden states, where the average is taken with respect to posterior beliefs about policies.
            Otherwise, `latest_belief`` will be the full, policy-conditioned belief about hidden states, and will have indexing structure
            policies->factors, such that ``latest_belief[p_idx][f_idx]`` refers to the penultimate belief about marginal factor ``f_idx``
            under policy ``p_idx``.
    Description: 
    Description Embedding: 
    Comments: ['average the earliest marginals together using contemporaneous posterior over policies (`self.q_pi_hist[0]`)', 'average the earliest marginals together using posterior over policies (`self.q_pi`)']

  ../pymdp/agent.py::step_time:
    name: step_time
    Module: None
    Signature: step_time(self)
    function_exe_cmd: pymdp.agent.Agent.step_time(self)
    Parameters: ['self']
    Returns: None
    Start Line: 396
    End Line: 418
    Complexity: 5
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Advances time by one step. This involves updating the ``self.prev_actions``, and in the case of a moving
        inference horizon, this also shifts the history of post-dictive beliefs forward in time (using ``self.set_latest_beliefs()``),
        so that the penultimate belief before the beginning of the horizon is correctly indexed.

        Returns
        ---------
        curr_timestep: ``int``
            The index in absolute simulation time of the current timestep.
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/agent.py::update_A:
    name: update_A
    Module: None
    Signature: update_A(self, obs)
    function_exe_cmd: pymdp.agent.Agent.update_A(self, obs)
    Parameters: ['self', 'obs']
    Returns: None
    Start Line: 749
    End Line: 778
    Complexity: 1
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Update approximate posterior beliefs about Dirichlet parameters that parameterise the observation likelihood or ``A`` array.

        Parameters
        ----------
        observation: ``list`` or ``tuple`` of ints
            The observation input. Each entry ``observation[m]`` stores the index of the discrete
            observation for modality ``m``.

        Returns
        -----------
        qA: ``numpy.ndarray`` of dtype object
            Posterior Dirichlet parameters over observation model (same shape as ``A``), after having updated it with observations.
    Description: 
    Description Embedding: 
    Comments: ['set new prior to posterior', 'take expected value of posterior Dirichlet parameters to calculate posterior over A array']

  ../pymdp/agent.py::update_B:
    name: update_B
    Module: None
    Signature: update_B(self, qs_prev)
    function_exe_cmd: pymdp.agent.Agent.update_B(self, qs_prev)
    Parameters: ['self', 'qs_prev']
    Returns: None
    Start Line: 810
    End Line: 839
    Complexity: 1
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Update posterior beliefs about Dirichlet parameters that parameterise the transition likelihood 

        Parameters
        -----------
        qs_prev: 1D ``numpy.ndarray`` or ``numpy.ndarray`` of dtype object
            Marginal posterior beliefs over hidden states at previous timepoint.

        Returns
        -----------
        qB: ``numpy.ndarray`` of dtype object
            Posterior Dirichlet parameters over transition model (same shape as ``B``), after having updated it with state beliefs and actions.
    Description: 
    Description Embedding: 
    Comments: ['set new prior to posterior', 'take expected value of posterior Dirichlet parameters to calculate posterior over B array']

  ../pymdp/agent.py::update_D:
    name: update_D
    Module: None
    Signature: update_D(self, qs_t0)
    function_exe_cmd: pymdp.agent.Agent.update_D(self, qs_t0)
    Parameters: ['self', 'qs_t0']
    Returns: None
    Start Line: 871
    End Line: 921
    Complexity: 8
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Update Dirichlet parameters of the initial hidden state distribution 
        (prior beliefs about hidden states at the beginning of the inference window).

        Parameters
        -----------
        qs_t0: 1D ``numpy.ndarray``, ``numpy.ndarray`` of dtype object, or ``None``
            Marginal posterior beliefs over hidden states at current timepoint. If ``None``, the 
            value of ``qs_t0`` is set to ``self.qs_hist[0]`` (i.e. the initial hidden state beliefs at the first timepoint).
            If ``self.inference_algo == "MMP"``, then ``qs_t0`` is set to be the Bayesian model average of beliefs about hidden states
            at the first timestep of the backwards inference horizon, where the average is taken with respect to posterior beliefs about policies.

        Returns
        -----------
        qD: ``numpy.ndarray`` of dtype object
            Posterior Dirichlet parameters over initial hidden state prior (same shape as ``qs_t0``), after having updated it with state beliefs.
    Description: 
    Description Embedding: 
    Comments: ['get beliefs about policies at the time at the beginning of the inference horizon', 'beliefs about hidden states at the first timestep of the inference horizon', 'set new prior to posterior', 'take expected value of posterior Dirichlet parameters to calculate posterior over D array']

  ../pymdp/algos/fpi.py::run_vanilla_fpi:
    name: run_vanilla_fpi
    Module: None
    Signature: run_vanilla_fpi(A, obs, num_obs, num_states, prior, num_iter, dF, dF_tol, compute_vfe)
    function_exe_cmd: pymdp.algos.fpi.run_vanilla_fpi(A, obs, num_obs, num_states, prior, num_iter, dF, dF_tol, compute_vfe)
    Parameters: ['A', 'obs', 'num_obs', 'num_states', 'prior', 'num_iter', 'dF', 'dF_tol', 'compute_vfe']
    Returns: None
    Start Line: 11
    End Line: 158
    Complexity: 10
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Update marginal posterior beliefs over hidden states using mean-field variational inference, via
    fixed point iteration. 

    Parameters
    ----------
    A: ``numpy.ndarray`` of dtype object
        Sensory likelihood mapping or 'observation model', mapping from hidden states to observations. Each element ``A[m]`` of
        stores an ``np.ndarray`` multidimensional array for observation modality ``m``, whose entries ``A[m][i, j, k, ...]`` store 
        the probability of observation level ``i`` given hidden state levels ``j, k, ...``
    obs: numpy 1D array or numpy ndarray of dtype object
        The observation (generated by the environment). If single modality, this should be a 1D ``np.ndarray``
        (one-hot vector representation). If multi-modality, this should be ``np.ndarray`` of dtype object whose entries are 1D one-hot vectors.
    num_obs: list of ints
        List of dimensionalities of each observation modality
    num_states: list of ints
        List of dimensionalities of each hidden state factor
    prior: numpy ndarray of dtype object, default None
        Prior over hidden states. If absent, prior is set to be the log uniform distribution over hidden states (identical to the 
        initialisation of the posterior)
    num_iter: int, default 10
        Number of variational fixed-point iterations to run until convergence.
    dF: float, default 1.0
        Initial free energy gradient (dF/dt) before updating in the course of gradient descent.
    dF_tol: float, default 0.001
        Threshold value of the time derivative of the variational free energy (dF/dt), to be checked at 
        each iteration. If dF <= dF_tol, the iterations are halted pre-emptively and the final 
        marginal posterior belief(s) is(are) returned
    compute_vfe: bool, default True
        Whether to compute the variational free energy at each iteration. If False, the function runs through 
        all variational iterations.

    Returns
    ----------
    qs: numpy 1D array, numpy ndarray of dtype object, optional
        Marginal posterior beliefs over hidden states at current timepoint
    Description: 
    Description Embedding: 
    Comments: ['get model dimensions', 'log the prior', 'change stop condition for fixed point iterations based on whether we are computing the variational free energy or not', 'Initialise variational free energy', 'arg_list = [likelihood, list(range(n_factors))]', 'arg_list = arg_list + list(chain(*([qs_i,[i]] for i, qs_i in enumerate(qs)))) + [list(range(n_factors))]', 'LL_tensor = np.einsum(*arg_list)', 'qL = np.einsum(LL_tensor, list(range(n_factors)), 1.0/qs_i, [factor], [factor])', "print(f'Posteriors at iteration {curr_iter}:\\n')", 'print(qs[0])', 'print(qs[1])', 'List of orders in which marginal posteriors are sequentially multiplied into the joint likelihood:', 'First order loops over factors starting at index = 0, second order goes in reverse', 'factor_orders = [range(n_factors), range((n_factors - 1), -1, -1)]', 'iteratively marginalize out each posterior marginal from the joint log-likelihood', 'except for the one associated with a given factor', 'for factor_order in factor_orders:', 'for factor in factor_order:', 'qL = spm_dot(likelihood, qs, [factor])', 'qs[factor] = softmax(qL + prior[factor])', 'calculate new free energy', "print(f'VFE at iteration {curr_iter}: {vfe}\\n')", 'stopping condition - time derivative of free energy']

  ../pymdp/algos/fpi.py::run_vanilla_fpi_factorized:
    name: run_vanilla_fpi_factorized
    Module: None
    Signature: run_vanilla_fpi_factorized(A, obs, num_obs, num_states, mb_dict, prior, num_iter, dF, dF_tol, compute_vfe)
    function_exe_cmd: pymdp.algos.fpi.run_vanilla_fpi_factorized(A, obs, num_obs, num_states, mb_dict, prior, num_iter, dF, dF_tol, compute_vfe)
    Parameters: ['A', 'obs', 'num_obs', 'num_states', 'mb_dict', 'prior', 'num_iter', 'dF', 'dF_tol', 'compute_vfe']
    Returns: None
    Start Line: 160
    End Line: 322
    Complexity: 14
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Update marginal posterior beliefs over hidden states using mean-field variational inference, via
    fixed point iteration. 

    Parameters
    ----------
    A: ``numpy.ndarray`` of dtype object
        Sensory likelihood mapping or 'observation model', mapping from hidden states to observations. Each element ``A[m]`` of
        stores an ``np.ndarray`` multidimensional array for observation modality ``m``, whose entries ``A[m][i, j, k, ...]`` store 
        the probability of observation level ``i`` given hidden state levels ``j, k, ...``
    obs: numpy 1D array or numpy ndarray of dtype object
        The observation (generated by the environment). If single modality, this should be a 1D ``np.ndarray``
        (one-hot vector representation). If multi-modality, this should be ``np.ndarray`` of dtype object whose entries are 1D one-hot vectors.
    num_obs: ``list`` of ints
        List of dimensionalities of each observation modality
    num_states: ``list`` of ints
        List of dimensionalities of each hidden state factor
    mb_dict: ``Dict``
        Dictionary with two keys (``A_factor_list`` and ``A_modality_list``), that stores the factor indices that influence each modality (``A_factor_list``)
        and the modality indices influenced by each factor (``A_modality_list``).
    prior: numpy ndarray of dtype object, default None
        Prior over hidden states. If absent, prior is set to be the log uniform distribution over hidden states (identical to the 
        initialisation of the posterior)
    num_iter: int, default 10
        Number of variational fixed-point iterations to run until convergence.
    dF: float, default 1.0
        Initial free energy gradient (dF/dt) before updating in the course of gradient descent.
    dF_tol: float, default 0.001
        Threshold value of the time derivative of the variational free energy (dF/dt), to be checked at 
        each iteration. If dF <= dF_tol, the iterations are halted pre-emptively and the final 
        marginal posterior belief(s) is(are) returned
    compute_vfe: bool, default True
        Whether to compute the variational free energy at each iteration. If False, the function runs through 
        all variational iterations.

    Returns
    ----------
    qs: numpy 1D array, numpy ndarray of dtype object, optional
        Marginal posterior beliefs over hidden states at current timepoint
    Description: 
    Description Embedding: 
    Comments: ['get model dimensions', 'log the prior', 'add up all the log-likelihoods, since we know they will all have the same dimension in the case of a single hidden state factor', 'add up all the log-likelihoods after reshaping them to the global common dimensions of all hidden state factors', 'change stop condition for fixed point iterations based on whether we are computing the variational free energy or not', 'vfe = 0', 'vfe -= qL.sum() # accuracy part of vfe, sum of factor-level expected energies E_q(s_i/f)[ln P(o=obs|s)]', "print(f'Posteriors at iteration {curr_iter}:\\n')", 'print(qs[0])', 'print(qs[1])', 'calculate new free energy, leaving out the accuracy term', 'vfe += calc_free_energy(qs, prior, n_factors)', "print(f'VFE at iteration {curr_iter}: {vfe}\\n')", 'stopping condition - time derivative of free energy']

  ../pymdp/algos/mmp.py::run_mmp:
    name: run_mmp
    Module: None
    Signature: run_mmp(lh_seq, B, policy, prev_actions, prior, num_iter, grad_descent, tau, last_timestep)
    function_exe_cmd: pymdp.algos.mmp.run_mmp(lh_seq, B, policy, prev_actions, prior, num_iter, grad_descent, tau, last_timestep)
    Parameters: ['lh_seq', 'B', 'policy', 'prev_actions', 'prior', 'num_iter', 'grad_descent', 'tau', 'last_timestep']
    Returns: None
    Start Line: 10
    End Line: 132
    Complexity: 18
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Marginal message passing scheme for updating marginal posterior beliefs about hidden states over time, 
    conditioned on a particular policy.

    Parameters
    ----------
    lh_seq: ``numpy.ndarray`` of dtype object
        Log likelihoods of hidden states under a sequence of observations over time. This is assumed to already be log-transformed. Each ``lh_seq[t]`` contains
        the log likelihood of hidden states for a particular observation at time ``t``
    B: ``numpy.ndarray`` of dtype object
        Dynamics likelihood mapping or 'transition model', mapping from hidden states at ``t`` to hidden states at ``t+1``, given some control state ``u``.
        Each element ``B[f]`` of this object array stores a 3-D tensor for hidden state factor ``f``, whose entries ``B[f][s, v, u]`` store the probability
        of hidden state level ``s`` at the current time, given hidden state level ``v`` and action ``u`` at the previous time.
    policy: 2D ``numpy.ndarray``
        Matrix of shape ``(policy_len, num_control_factors)`` that indicates the indices of each action (control state index) upon timestep ``t`` and control_factor ``f` in the element ``policy[t,f]`` for a given policy.
    prev_actions: ``numpy.ndarray``, default None
        If provided, should be a matrix of previous actions of shape ``(infer_len, num_control_factors)`` that indicates the indices of each action (control state index) taken in the past (up until the current timestep).
    prior: ``numpy.ndarray`` of dtype object, default None
        If provided, the prior beliefs about initial states (at t = 0, relative to ``infer_len``). If ``None``, this defaults
        to a flat (uninformative) prior over hidden states.
    numiter: int, default 10
        Number of variational iterations.
    grad_descent: Bool, default True
        Flag for whether to use gradient descent (free energy gradient updates) instead of fixed point solution to the posterior beliefs
    tau: float, default 0.25
        Decay constant for use in ``grad_descent`` version. Tunes the size of the gradient descent updates to the posterior.
    last_timestep: Bool, default False
        Flag for whether we are at the last timestep of belief updating

    Returns
    ---------
    qs_seq: ``numpy.ndarray`` of dtype object
        Posterior beliefs over hidden states under the policy. Nesting structure is timepoints, factors,
        where e.g. ``qs_seq[t][f]`` stores the marginal belief about factor ``f`` at timepoint ``t`` under the policy in question.
    F: float
        Variational free energy of the policy.
    Description: 
    Description Embedding: 
    Comments: ['window', 'dimensions', 'beliefs', 'last message', 'prior', 'transposed transition', 'reset variational free energy (accumulated over time and factors, but reset per iteration)', 'likelihood', 'past message', 'future message', 'inference', 'save this as a separate variable so that it can be used in VFE computation', 'for numerical stability, before passing into the softmax', '@NOTE: not sure why Karl does this in SPM_MDP_VB_X, we should look into this']

  ../pymdp/algos/mmp.py::run_mmp_factorized:
    name: run_mmp_factorized
    Module: None
    Signature: run_mmp_factorized(lh_seq, mb_dict, B, B_factor_list, policy, prev_actions, prior, num_iter, grad_descent, tau, last_timestep)
    function_exe_cmd: pymdp.algos.mmp.run_mmp_factorized(lh_seq, mb_dict, B, B_factor_list, policy, prev_actions, prior, num_iter, grad_descent, tau, last_timestep)
    Parameters: ['lh_seq', 'mb_dict', 'B', 'B_factor_list', 'policy', 'prev_actions', 'prior', 'num_iter', 'grad_descent', 'tau', 'last_timestep']
    Returns: None
    Start Line: 134
    End Line: 296
    Complexity: 26
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Marginal message passing scheme for updating marginal posterior beliefs about hidden states over time, 
    conditioned on a particular policy.

    Parameters
    ----------
    lh_seq: ``numpy.ndarray`` of dtype object
        Log likelihoods of hidden states under a sequence of observations over time. This is assumed to already be log-transformed. Each ``lh_seq[t]`` contains
        the log likelihood of hidden states for a particular observation at time ``t``
    mb_dict: ``Dict``
        Dictionary with two keys (``A_factor_list`` and ``A_modality_list``), that stores the factor indices that influence each modality (``A_factor_list``)
        and the modality indices influenced by each factor (``A_modality_list``).
    B: ``numpy.ndarray`` of dtype object
        Dynamics likelihood mapping or 'transition model', mapping from hidden states at ``t`` to hidden states at ``t+1``, given some control state ``u``.
        Each element ``B[f]`` of this object array stores a 3-D tensor for hidden state factor ``f``, whose entries ``B[f][s, v, u]`` store the probability
        of hidden state level ``s`` at the current time, given hidden state level ``v`` and action ``u`` at the previous time.
    B_factor_list: ``list`` of ``list`` of ``int``
        List of lists of hidden state factors each hidden state factor depends on. Each element ``B_factor_list[i]`` is a list of the factor indices that factor i's dynamics depend on.
    policy: 2D ``numpy.ndarray``
        Matrix of shape ``(policy_len, num_control_factors)`` that indicates the indices of each action (control state index) upon timestep ``t`` and control_factor ``f` in the element ``policy[t,f]`` for a given policy.
    prev_actions: ``numpy.ndarray``, default None
        If provided, should be a matrix of previous actions of shape ``(infer_len, num_control_factors)`` that indicates the indices of each action (control state index) taken in the past (up until the current timestep).
    prior: ``numpy.ndarray`` of dtype object, default None
        If provided, the prior beliefs about initial states (at t = 0, relative to ``infer_len``). If ``None``, this defaults
        to a flat (uninformative) prior over hidden states.
    numiter: int, default 10
        Number of variational iterations.
    grad_descent: Bool, default True
        Flag for whether to use gradient descent (free energy gradient updates) instead of fixed point solution to the posterior beliefs
    tau: float, default 0.25
        Decay constant for use in ``grad_descent`` version. Tunes the size of the gradient descent updates to the posterior.
    last_timestep: Bool, default False
        Flag for whether we are at the last timestep of belief updating

    Returns
    ---------
    qs_seq: ``numpy.ndarray`` of dtype object
        Posterior beliefs over hidden states under the policy. Nesting structure is timepoints, factors,
        where e.g. ``qs_seq[t][f]`` stores the marginal belief about factor ``f`` at timepoint ``t`` under the policy in question.
    F: float
        Variational free energy of the policy.
    Description: 
    Description Embedding: 
    Comments: ['window', 'dimensions', 'beliefs', 'last message', 'prior', 'transposed transition', 'add up all the log-likelihoods after reshaping them to the global common dimensions of all hidden state factors', "compute inverse B dependencies, which is a list that for each hidden state factor, lists the indices of the other hidden state factors that it 'drives' or is a parent of in the HMM graphical model", 'reset variational free energy (accumulated over time and factors, but reset per iteration)', 'likelihood', 'past message', 'future message', 'list of future_msgs, one for each of the factors that factor f is driving', 'list of the marginalized B matrices, that correspond to mapping between the factor of interest `f` and each of its children factors `i`', 'loop over all the hidden state factors that are driven by f', 'loop over the list of factors that drive each child `i` of factor-of-interest `f` (i.e. the co-parents of `f`, with respect to child `i`)', 'marginalize out all parents of `i` besides `f`', 'inference', 'save this as a separate variable so that it can be used in VFE computation', '@NOTE: not sure why Karl does this in SPM_MDP_VB_X, we should look into this']

  ../pymdp/control.py::backwards_induction:
    name: backwards_induction
    Module: None
    Signature: backwards_induction(H, B, B_factor_list, threshold, depth)
    function_exe_cmd: pymdp.control.backwards_induction(H, B, B_factor_list, threshold, depth)
    Parameters: ['H', 'B', 'B_factor_list', 'threshold', 'depth']
    Returns: None
    Start Line: 1259
    End Line: 1314
    Complexity: 8
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Runs backwards induction of reaching a goal state H given a transition model B.

    Parameters
    ----------    
    H: ``numpy.ndarray`` of dtype object
       Prior over states
    B: ``numpy.ndarray`` of dtype object
        Dynamics likelihood mapping or 'transition model', mapping from hidden states at ``t`` to hidden states at ``t+1``, given some control state ``u``.
        Each element ``B[f]`` of this object array stores a 3-D tensor for hidden state factor ``f``, whose entries ``B[f][s, v, u]`` store the probability
        of hidden state level ``s`` at the current time, given hidden state level ``v`` and action ``u`` at the previous time.
    B_factor_list: ``list`` of ``list`` of ``int``
        List of lists of hidden state factors each hidden state factor depends on. Each element ``B_factor_list[i]`` is a list of the factor indices that factor i's dynamics depend on.
    threshold: ``float``
        The threshold for pruning transitions that are below a certain probability
    depth: ``int``
        The temporal depth of the backward induction

    Returns
    ----------
    I: ``numpy.ndarray`` of dtype object
        For each state factor, contains a 2D ``numpy.ndarray`` whose element i,j yields the probability 
        of reaching the goal state backwards from state j after i steps.
    Description: 
    Description Embedding: 
    Comments: ['TODO can this be done with arbitrary B_factor_list?', 'If there exists an action that allows transitioning', 'from state to next_state, with probability larger than threshold', 'set b[state, next_state] to 1', 'TODO stop when all 1s?']

  ../pymdp/control.py::calc_ambiguity_factorized:
    name: calc_ambiguity_factorized
    Module: None
    Signature: calc_ambiguity_factorized(qs_pi, A, A_factor_list)
    function_exe_cmd: pymdp.control.calc_ambiguity_factorized(qs_pi, A, A_factor_list)
    Parameters: ['qs_pi', 'A', 'A_factor_list']
    Returns: None
    Start Line: 1316
    End Line: 1350
    Complexity: 3
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Computes the Ambiguity term.

    Parameters
    ----------
    qs_pi: ``list`` of ``numpy.ndarray`` of dtype object
        Predictive posterior beliefs over hidden states expected under the policy, where ``qs_pi[t]`` stores the beliefs about
        hidden states expected under the policy at time ``t``
    A: ``numpy.ndarray`` of dtype object
        Sensory likelihood mapping or 'observation model', mapping from hidden states to observations. Each element ``A[m]`` of
        stores an ``numpy.ndarray`` multidimensional array for observation modality ``m``, whose entries ``A[m][i, j, k, ...]`` store 
        the probability of observation level ``i`` given hidden state levels ``j, k, ...``
    A_factor_list: ``list`` of ``list`` of ``int``
        List of lists, where ``A_factor_list[m]`` is a list of the hidden state factor indices that observation modality with the index ``m`` depends on

    Returns
    -------
    ambiguity: float
    Description: 
    Description Embedding: 
    Comments: ['TODO check if we do this correctly!', 'TODO why does spm_dot return an array here?', 'joint_x = maths.spm_cross(qs_pi[t][factor_idx])', 'ambiguity += (H_m * joint_x).sum()']

  ../pymdp/control.py::calc_expected_utility:
    name: calc_expected_utility
    Module: None
    Signature: calc_expected_utility(qo_pi, C)
    function_exe_cmd: pymdp.control.calc_expected_utility(qo_pi, C)
    Parameters: ['qo_pi', 'C']
    Returns: None
    Start Line: 619
    End Line: 661
    Complexity: 4
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Computes the expected utility of a policy, using the observation distribution expected under that policy and a prior preference vector.

    Parameters
    ----------
    qo_pi: ``list`` of ``numpy.ndarray`` of dtype object
        Predictive posterior beliefs over observations expected under the policy, where ``qo_pi[t]`` stores the beliefs about
        observations expected under the policy at time ``t``
    C: ``numpy.ndarray`` of dtype object
       Prior over observations or 'prior preferences', storing the "value" of each outcome in terms of relative log probabilities. 
       This is softmaxed to form a proper probability distribution before being used to compute the expected utility.

    Returns
    -------
    expected_util: float
        Utility (reward) expected under the policy in question
    Description: 
    Description Embedding: 
    Comments: ['initialise expected utility', 'loop over time points and modalities', "reformat C to be tiled across timesteps, if it's not already", 'make a deepcopy of C where it has been tiled across timesteps', 'convert relative log probabilities into proper probability distribution']

  ../pymdp/control.py::calc_inductive_cost:
    name: calc_inductive_cost
    Module: None
    Signature: calc_inductive_cost(qs, qs_pi, I, epsilon)
    function_exe_cmd: pymdp.control.calc_inductive_cost(qs, qs_pi, I, epsilon)
    Parameters: ['qs', 'qs_pi', 'I', 'epsilon']
    Returns: None
    Start Line: 910
    End Line: 951
    Complexity: 4
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Computes the inductive cost of a state.

    Parameters
    ----------
    qs: ``numpy.ndarray`` of dtype object
        Marginal posterior beliefs over hidden states at a given timepoint.
    qs_pi: ``list`` of ``numpy.ndarray`` of dtype object
        Predictive posterior beliefs over hidden states expected under the policy, where ``qs_pi[t]`` stores the beliefs about
        states expected under the policy at time ``t``
    I: ``numpy.ndarray`` of dtype object
        For each state factor, contains a 2D ``numpy.ndarray`` whose element i,j yields the probability 
        of reaching the goal state backwards from state j after i steps.

    Returns
    -------
    inductive_cost: float
        Cost of visited this state using backwards induction under the policy in question
    Description: 
    Description Embedding: 
    Comments: ['initialise inductive cost', 'loop over time points and modalities', 'we also assume precise beliefs here?!', 'm = arg max_n p_n < sup p', 'i.e. find first I idx equals 1 and m is the index before', 'we might find no path to goal (i.e. when no goal specified)']

  ../pymdp/control.py::calc_pA_info_gain:
    name: calc_pA_info_gain
    Module: None
    Signature: calc_pA_info_gain(pA, qo_pi, qs_pi)
    function_exe_cmd: pymdp.control.calc_pA_info_gain(pA, qo_pi, qs_pi)
    Parameters: ['pA', 'qo_pi', 'qs_pi']
    Returns: None
    Start Line: 727
    End Line: 762
    Complexity: 4
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Compute expected Dirichlet information gain about parameters ``pA`` under a policy

    Parameters
    ----------
    pA: ``numpy.ndarray`` of dtype object
        Dirichlet parameters over observation model (same shape as ``A``)
    qo_pi: ``list`` of ``numpy.ndarray`` of dtype object
        Predictive posterior beliefs over observations expected under the policy, where ``qo_pi[t]`` stores the beliefs about
        observations expected under the policy at time ``t``
    qs_pi: ``list`` of ``numpy.ndarray`` of dtype object
        Predictive posterior beliefs over hidden states expected under the policy, where ``qs_pi[t]`` stores the beliefs about
        hidden states expected under the policy at time ``t``

    Returns
    -------
    infogain_pA: float
        Surprise (about Dirichlet parameters) expected under the policy in question
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/control.py::calc_pA_info_gain_factorized:
    name: calc_pA_info_gain_factorized
    Module: None
    Signature: calc_pA_info_gain_factorized(pA, qo_pi, qs_pi, A_factor_list)
    function_exe_cmd: pymdp.control.calc_pA_info_gain_factorized(pA, qo_pi, qs_pi, A_factor_list)
    Parameters: ['pA', 'qo_pi', 'qs_pi', 'A_factor_list']
    Returns: None
    Start Line: 764
    End Line: 803
    Complexity: 4
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Compute expected Dirichlet information gain about parameters ``pA`` under a policy.
    In this version of the function, we assume that the observation model is factorized, i.e. that each observation modality depends on a subset of the hidden state factors.

    Parameters
    ----------
    pA: ``numpy.ndarray`` of dtype object
        Dirichlet parameters over observation model (same shape as ``A``)
    qo_pi: ``list`` of ``numpy.ndarray`` of dtype object
        Predictive posterior beliefs over observations expected under the policy, where ``qo_pi[t]`` stores the beliefs about
        observations expected under the policy at time ``t``
    qs_pi: ``list`` of ``numpy.ndarray`` of dtype object
        Predictive posterior beliefs over hidden states expected under the policy, where ``qs_pi[t]`` stores the beliefs about
        hidden states expected under the policy at time ``t``
    A_factor_list: ``list`` of ``list`` of ``int``
        List of lists, where ``A_factor_list[m]`` is a list of the hidden state factor indices that observation modality with the index ``m`` depends on

    Returns
    -------
    infogain_pA: float
        Surprise (about Dirichlet parameters) expected under the policy in question
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/control.py::calc_pB_info_gain:
    name: calc_pB_info_gain
    Module: None
    Signature: calc_pB_info_gain(pB, qs_pi, qs_prev, policy)
    function_exe_cmd: pymdp.control.calc_pB_info_gain(pB, qs_pi, qs_prev, policy)
    Parameters: ['pB', 'qs_pi', 'qs_prev', 'policy']
    Returns: None
    Start Line: 805
    End Line: 854
    Complexity: 5
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Compute expected Dirichlet information gain about parameters ``pB`` under a given policy

    Parameters
    ----------
    pB: ``numpy.ndarray`` of dtype object
        Dirichlet parameters over transition model (same shape as ``B``)
    qs_pi: ``list`` of ``numpy.ndarray`` of dtype object
        Predictive posterior beliefs over hidden states expected under the policy, where ``qs_pi[t]`` stores the beliefs about
        hidden states expected under the policy at time ``t``
    qs_prev: ``numpy.ndarray`` of dtype object
        Posterior over hidden states at beginning of trajectory (before receiving observations)
    policy: 2D ``numpy.ndarray``
        Array that stores actions entailed by a policy over time. Shape is ``(num_timesteps, num_factors)`` where ``num_timesteps`` is the temporal
        depth of the policy and ``num_factors`` is the number of control factors.

    Returns
    -------
    infogain_pB: float
        Surprise (about dirichlet parameters) expected under the policy in question
    Description: 
    Description Embedding: 
    Comments: ["the 'past posterior' used for the information gain about pB here is the posterior", 'over expected states at the timestep previous to the one under consideration', "if we're on the first timestep, we just use the latest posterior in the", 'entire action-perception cycle as the previous posterior', 'otherwise, we use the expected states for the timestep previous to the timestep under consideration', 'get the list of action-indices for the current timestep']

  ../pymdp/control.py::calc_pB_info_gain_interactions:
    name: calc_pB_info_gain_interactions
    Module: None
    Signature: calc_pB_info_gain_interactions(pB, qs_pi, qs_prev, B_factor_list, policy)
    function_exe_cmd: pymdp.control.calc_pB_info_gain_interactions(pB, qs_pi, qs_prev, B_factor_list, policy)
    Parameters: ['pB', 'qs_pi', 'qs_prev', 'B_factor_list', 'policy']
    Returns: None
    Start Line: 856
    End Line: 908
    Complexity: 5
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Compute expected Dirichlet information gain about parameters ``pB`` under a given policy

    Parameters
    ----------
    pB: ``numpy.ndarray`` of dtype object
        Dirichlet parameters over transition model (same shape as ``B``)
    qs_pi: ``list`` of ``numpy.ndarray`` of dtype object
        Predictive posterior beliefs over hidden states expected under the policy, where ``qs_pi[t]`` stores the beliefs about
        hidden states expected under the policy at time ``t``
    qs_prev: ``numpy.ndarray`` of dtype object
        Posterior over hidden states at beginning of trajectory (before receiving observations)
    B_factor_list: ``list`` of ``list`` of ``int``
        List of lists, where ``B_factor_list[f]`` is a list of the hidden state factor indices that hidden state factor with the index ``f`` depends on
    policy: 2D ``numpy.ndarray``
        Array that stores actions entailed by a policy over time. Shape is ``(num_timesteps, num_factors)`` where ``num_timesteps`` is the temporal
        depth of the policy and ``num_factors`` is the number of control factors.

    Returns
    -------
    infogain_pB: float
        Surprise (about dirichlet parameters) expected under the policy in question
    Description: 
    Description Embedding: 
    Comments: ["the 'past posterior' used for the information gain about pB here is the posterior", 'over expected states at the timestep previous to the one under consideration', "if we're on the first timestep, we just use the latest posterior in the", 'entire action-perception cycle as the previous posterior', 'otherwise, we use the expected states for the timestep previous to the timestep under consideration', 'get the list of action-indices for the current timestep']

  ../pymdp/control.py::calc_states_info_gain:
    name: calc_states_info_gain
    Module: None
    Signature: calc_states_info_gain(A, qs_pi)
    function_exe_cmd: pymdp.control.calc_states_info_gain(A, qs_pi)
    Parameters: ['A', 'qs_pi']
    Returns: None
    Start Line: 664
    End Line: 691
    Complexity: 2
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Computes the Bayesian surprise or information gain about states of a policy, 
    using the observation model and the hidden state distribution expected under that policy.

    Parameters
    ----------
    A: ``numpy.ndarray`` of dtype object
        Sensory likelihood mapping or 'observation model', mapping from hidden states to observations. Each element ``A[m]`` of
        stores an ``numpy.ndarray`` multidimensional array for observation modality ``m``, whose entries ``A[m][i, j, k, ...]`` store 
        the probability of observation level ``i`` given hidden state levels ``j, k, ...``
    qs_pi: ``list`` of ``numpy.ndarray`` of dtype object
        Predictive posterior beliefs over hidden states expected under the policy, where ``qs_pi[t]`` stores the beliefs about
        hidden states expected under the policy at time ``t``

    Returns
    -------
    states_surprise: float
        Bayesian surprise (about states) or salience expected under the policy in question
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/control.py::calc_states_info_gain_factorized:
    name: calc_states_info_gain_factorized
    Module: None
    Signature: calc_states_info_gain_factorized(A, qs_pi, A_factor_list)
    function_exe_cmd: pymdp.control.calc_states_info_gain_factorized(A, qs_pi, A_factor_list)
    Parameters: ['A', 'qs_pi', 'A_factor_list']
    Returns: None
    Start Line: 693
    End Line: 724
    Complexity: 3
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Computes the Bayesian surprise or information gain about states of a policy, 
    using the observation model and the hidden state distribution expected under that policy.

    Parameters
    ----------
    A: ``numpy.ndarray`` of dtype object
        Sensory likelihood mapping or 'observation model', mapping from hidden states to observations. Each element ``A[m]`` of
        stores an ``numpy.ndarray`` multidimensional array for observation modality ``m``, whose entries ``A[m][i, j, k, ...]`` store 
        the probability of observation level ``i`` given hidden state levels ``j, k, ...``
    qs_pi: ``list`` of ``numpy.ndarray`` of dtype object
        Predictive posterior beliefs over hidden states expected under the policy, where ``qs_pi[t]`` stores the beliefs about
        hidden states expected under the policy at time ``t``
    A_factor_list: ``list`` of ``list`` of ``int``
        List of lists, where ``A_factor_list[m]`` is a list of the hidden state factor indices that observation modality with the index ``m`` depends on

    Returns
    -------
    states_surprise: float
        Bayesian surprise (about states) or salience expected under the policy in question
    Description: 
    Description Embedding: 
    Comments: ['list of the hidden state factor indices that observation modality with the index `m` depends on']

  ../pymdp/control.py::construct_policies:
    name: construct_policies
    Module: None
    Signature: construct_policies(num_states, num_controls, policy_len, control_fac_idx)
    function_exe_cmd: pymdp.control.construct_policies(num_states, num_controls, policy_len, control_fac_idx)
    Parameters: ['num_states', 'num_controls', 'policy_len', 'control_fac_idx']
    Returns: None
    Start Line: 953
    End Line: 993
    Complexity: 5
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Generate a ``list`` of policies. The returned array ``policies`` is a ``list`` that stores one policy per entry.
    A particular policy (``policies[i]``) has shape ``(num_timesteps, num_factors)`` 
    where ``num_timesteps`` is the temporal depth of the policy and ``num_factors`` is the number of control factors.

    Parameters
    ----------
    num_states: ``list`` of ``int``
        ``list`` of the dimensionalities of each hidden state factor
    num_controls: ``list`` of ``int``, default ``None``
        ``list`` of the dimensionalities of each control state factor. If ``None``, then is automatically computed as the dimensionality of each hidden state factor that is controllable
    policy_len: ``int``, default 1
        temporal depth ("planning horizon") of policies
    control_fac_idx: ``list`` of ``int``
        ``list`` of indices of the hidden state factors that are controllable (i.e. those state factors ``i`` where ``num_controls[i] > 1``)

    Returns
    ----------
    policies: ``list`` of 2D ``numpy.ndarray``
        ``list`` that stores each policy as a 2D array in ``policies[p_idx]``. Shape of ``policies[p_idx]`` 
        is ``(num_timesteps, num_factors)`` where ``num_timesteps`` is the temporal
        depth of the policy and ``num_factors`` is the number of control factors.
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/control.py::get_expected_obs:
    name: get_expected_obs
    Module: None
    Signature: get_expected_obs(qs_pi, A)
    function_exe_cmd: pymdp.control.get_expected_obs(qs_pi, A)
    Parameters: ['qs_pi', 'A']
    Returns: None
    Start Line: 543
    End Line: 578
    Complexity: 4
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Compute the expected observations under a policy, also known as the posterior predictive density over observations

    Parameters
    ----------
    qs_pi: ``list`` of ``numpy.ndarray`` of dtype object
        Predictive posterior beliefs over hidden states expected under the policy, where ``qs_pi[t]`` stores the beliefs about
        hidden states expected under the policy at time ``t``
    A: ``numpy.ndarray`` of dtype object
        Sensory likelihood mapping or 'observation model', mapping from hidden states to observations. Each element ``A[m]`` of
        stores an ``numpy.ndarray`` multidimensional array for observation modality ``m``, whose entries ``A[m][i, j, k, ...]`` store 
        the probability of observation level ``i`` given hidden state levels ``j, k, ...``

    Returns
    -------
    qo_pi: ``list`` of ``numpy.ndarray`` of dtype object
        Predictive posterior beliefs over observations expected under the policy, where ``qo_pi[t]`` stores the beliefs about
        observations expected under the policy at time ``t``
    Description: 
    Description Embedding: 
    Comments: ['each element of the list is the PPD at a different timestep', 'initialise expected observations', 'compute expected observations over time']

  ../pymdp/control.py::get_expected_obs_factorized:
    name: get_expected_obs_factorized
    Module: None
    Signature: get_expected_obs_factorized(qs_pi, A, A_factor_list)
    function_exe_cmd: pymdp.control.get_expected_obs_factorized(qs_pi, A, A_factor_list)
    Parameters: ['qs_pi', 'A', 'A_factor_list']
    Returns: None
    Start Line: 580
    End Line: 617
    Complexity: 4
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Compute the expected observations under a policy, also known as the posterior predictive density over observations

    Parameters
    ----------
    qs_pi: ``list`` of ``numpy.ndarray`` of dtype object
        Predictive posterior beliefs over hidden states expected under the policy, where ``qs_pi[t]`` stores the beliefs about
        hidden states expected under the policy at time ``t``
    A: ``numpy.ndarray`` of dtype object
        Sensory likelihood mapping or 'observation model', mapping from hidden states to observations. Each element ``A[m]`` of
        stores an ``numpy.ndarray`` multidimensional array for observation modality ``m``, whose entries ``A[m][i, j, k, ...]`` store 
        the probability of observation level ``i`` given hidden state levels ``j, k, ...``
    A_factor_list: ``list`` of ``list`` of ``int``
        List of lists of hidden state factor indices that each observation modality depends on. Each element ``A_factor_list[i]`` is a list of the factor indices that modality i's observation model depends on.
    Returns
    -------
    qo_pi: ``list`` of ``numpy.ndarray`` of dtype object
        Predictive posterior beliefs over observations expected under the policy, where ``qo_pi[t]`` stores the beliefs about
        observations expected under the policy at time ``t``
    Description: 
    Description Embedding: 
    Comments: ['each element of the list is the PPD at a different timestep', 'initialise expected observations', 'compute expected observations over time', 'list of the hidden state factor indices that observation modality with the index `modality` depends on']

  ../pymdp/control.py::get_expected_states:
    name: get_expected_states
    Module: None
    Signature: get_expected_states(qs, B, policy)
    function_exe_cmd: pymdp.control.get_expected_states(qs, B, policy)
    Parameters: ['qs', 'B', 'policy']
    Returns: None
    Start Line: 470
    End Line: 503
    Complexity: 3
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Compute the expected states under a policy, also known as the posterior predictive density over states

    Parameters
    ----------
    qs: ``numpy.ndarray`` of dtype object
        Marginal posterior beliefs over hidden states at a given timepoint.
    B: ``numpy.ndarray`` of dtype object
        Dynamics likelihood mapping or 'transition model', mapping from hidden states at ``t`` to hidden states at ``t+1``, given some control state ``u``.
        Each element ``B[f]`` of this object array stores a 3-D tensor for hidden state factor ``f``, whose entries ``B[f][s, v, u]`` store the probability
        of hidden state level ``s`` at the current time, given hidden state level ``v`` and action ``u`` at the previous time.
    policy: 2D ``numpy.ndarray``
        Array that stores actions entailed by a policy over time. Shape is ``(num_timesteps, num_factors)`` where ``num_timesteps`` is the temporal
        depth of the policy and ``num_factors`` is the number of control factors.

    Returns
    -------
    qs_pi: ``list`` of ``numpy.ndarray`` of dtype object
        Predictive posterior beliefs over hidden states expected under the policy, where ``qs_pi[t]`` stores the beliefs about
        hidden states expected under the policy at time ``t``
    Description: 
    Description Embedding: 
    Comments: ['initialise posterior predictive density as a list of beliefs over time, including current posterior beliefs about hidden states as the first element', 'get expected states over time']

  ../pymdp/control.py::get_expected_states_interactions:
    name: get_expected_states_interactions
    Module: None
    Signature: get_expected_states_interactions(qs, B, B_factor_list, policy)
    function_exe_cmd: pymdp.control.get_expected_states_interactions(qs, B, B_factor_list, policy)
    Parameters: ['qs', 'B', 'B_factor_list', 'policy']
    Returns: None
    Start Line: 505
    End Line: 541
    Complexity: 3
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Compute the expected states under a policy, also known as the posterior predictive density over states

    Parameters
    ----------
    qs: ``numpy.ndarray`` of dtype object
        Marginal posterior beliefs over hidden states at a given timepoint.
    B: ``numpy.ndarray`` of dtype object
        Dynamics likelihood mapping or 'transition model', mapping from hidden states at ``t`` to hidden states at ``t+1``, given some control state ``u``.
        Each element ``B[f]`` of this object array stores a 3-D tensor for hidden state factor ``f``, whose entries ``B[f][s, v, u]`` store the probability
        of hidden state level ``s`` at the current time, given hidden state level ``v`` and action ``u`` at the previous time.
    B_factor_list: ``list`` of ``list`` of ``int``
        List of lists of hidden state factors each hidden state factor depends on. Each element ``B_factor_list[i]`` is a list of the factor indices that factor i's dynamics depend on.
    policy: 2D ``numpy.ndarray``
        Array that stores actions entailed by a policy over time. Shape is ``(num_timesteps, num_factors)`` where ``num_timesteps`` is the temporal
        depth of the policy and ``num_factors`` is the number of control factors.

    Returns
    -------
    qs_pi: ``list`` of ``numpy.ndarray`` of dtype object
        Predictive posterior beliefs over hidden states expected under the policy, where ``qs_pi[t]`` stores the beliefs about
        hidden states expected under the policy at time ``t``
    Description: 
    Description Embedding: 
    Comments: ['initialise posterior predictive density as a list of beliefs over time, including current posterior beliefs about hidden states as the first element', 'get expected states over time', 'list of the hidden state factor indices that the dynamics of `qs[control_factor]` depend on']

  ../pymdp/control.py::get_num_controls_from_policies:
    name: get_num_controls_from_policies
    Module: None
    Signature: get_num_controls_from_policies(policies)
    function_exe_cmd: pymdp.control.get_num_controls_from_policies(policies)
    Parameters: ['policies']
    Returns: None
    Start Line: 995
    End Line: 1014
    Complexity: 1
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Calculates the ``list`` of dimensionalities of control factors (``num_controls``)
    from the ``list`` or array of policies. This assumes a policy space such that for each control factor, there is at least
    one policy that entails taking the action with the maximum index along that control factor.

    Parameters
    ----------
    policies: ``list`` of 2D ``numpy.ndarray``
        ``list`` that stores each policy as a 2D array in ``policies[p_idx]``. Shape of ``policies[p_idx]`` 
        is ``(num_timesteps, num_factors)`` where ``num_timesteps`` is the temporal
        depth of the policy and ``num_factors`` is the number of control factors.

    Returns
    ----------
    num_controls: ``list`` of ``int``
        ``list`` of the dimensionalities of each control state factor, computed here automatically from a ``list`` of policies.
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/control.py::sample_action:
    name: sample_action
    Module: None
    Signature: sample_action(q_pi, policies, num_controls, action_selection, alpha)
    function_exe_cmd: pymdp.control.sample_action(q_pi, policies, num_controls, action_selection, alpha)
    Parameters: ['q_pi', 'policies', 'num_controls', 'action_selection', 'alpha']
    Returns: None
    Start Line: 1017
    End Line: 1066
    Complexity: 6
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Computes the marginal posterior over actions and then samples an action from it, one action per control factor.

    Parameters
    ----------
    q_pi: 1D ``numpy.ndarray``
        Posterior beliefs over policies, i.e. a vector containing one posterior probability per policy.
    policies: ``list`` of 2D ``numpy.ndarray``
        ``list`` that stores each policy as a 2D array in ``policies[p_idx]``. Shape of ``policies[p_idx]`` 
        is ``(num_timesteps, num_factors)`` where ``num_timesteps`` is the temporal
        depth of the policy and ``num_factors`` is the number of control factors.
    num_controls: ``list`` of ``int``
        ``list`` of the dimensionalities of each control state factor.
    action_selection: ``str``, default "deterministic"
        String indicating whether whether the selected action is chosen as the maximum of the posterior over actions,
        or whether it's sampled from the posterior marginal over actions
    alpha: ``float``, default 16.0
        Action selection precision -- the inverse temperature of the softmax that is used to scale the 
        action marginals before sampling. This is only used if ``action_selection`` argument is "stochastic"

    Returns
    ----------
    selected_policy: 1D ``numpy.ndarray``
        Vector containing the indices of the actions for each control factor
    Description: 
    Description Embedding: 
    Comments: ['weight each action according to its integrated posterior probability under all policies at the current timestep', 'Either you do this:']

  ../pymdp/control.py::sample_policy:
    name: sample_policy
    Module: None
    Signature: sample_policy(q_pi, policies, num_controls, action_selection, alpha)
    function_exe_cmd: pymdp.control.sample_policy(q_pi, policies, num_controls, action_selection, alpha)
    Parameters: ['q_pi', 'policies', 'num_controls', 'action_selection', 'alpha']
    Returns: None
    Start Line: 1126
    End Line: 1166
    Complexity: 4
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Samples a policy from the posterior over policies, taking the action (per control factor) entailed by the first timestep of the selected policy.

    Parameters
    ----------
    q_pi: 1D ``numpy.ndarray``
        Posterior beliefs over policies, i.e. a vector containing one posterior probability per policy.
    policies: ``list`` of 2D ``numpy.ndarray``
        ``list`` that stores each policy as a 2D array in ``policies[p_idx]``. Shape of ``policies[p_idx]`` 
        is ``(num_timesteps, num_factors)`` where ``num_timesteps`` is the temporal
        depth of the policy and ``num_factors`` is the number of control factors.
    num_controls: ``list`` of ``int``
        ``list`` of the dimensionalities of each control state factor.
    action_selection: string, default "deterministic"
        String indicating whether whether the selected policy is chosen as the maximum of the posterior over policies,
        or whether it's sampled from the posterior over policies.
    alpha: float, default 16.0
        Action selection precision -- the inverse temperature of the softmax that is used to scale the 
        policy posterior before sampling. This is only used if ``action_selection`` argument is "stochastic"

    Returns
    ----------
    selected_policy: 1D ``numpy.ndarray``
        Vector containing the indices of the actions for each control factor
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/control.py::select_highest:
    name: select_highest
    Module: None
    Signature: select_highest(options_array)
    function_exe_cmd: pymdp.control.select_highest(options_array)
    Parameters: ['options_array']
    Returns: None
    Start Line: 1215
    End Line: 1234
    Complexity: 2
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Selects the highest value among the provided ones. If the higher value is more than once and they're closer than 1e-5, a random choice is made.
    Parameters
    ----------
    options_array: ``numpy.ndarray``
        The array to examine

    Returns
    -------
    The highest value in the given list
    Description: 
    Description Embedding: 
    Comments: ['If some of the most likely actions have nearly equal probability, sample from this subset of actions, instead of using argmax']

  ../pymdp/control.py::sophisticated_inference_search:
    name: sophisticated_inference_search
    Module: None
    Signature: sophisticated_inference_search(qs, policies, A, B, C, A_factor_list, B_factor_list, I, horizon, policy_prune_threshold, state_prune_threshold, prune_penalty, gamma, inference_params, n)
    function_exe_cmd: pymdp.control.sophisticated_inference_search(qs, policies, A, B, C, A_factor_list, B_factor_list, I, horizon, policy_prune_threshold, state_prune_threshold, prune_penalty, gamma, inference_params, n)
    Parameters: ['qs', 'policies', 'A', 'B', 'C', 'A_factor_list', 'B_factor_list', 'I', 'horizon', 'policy_prune_threshold', 'state_prune_threshold', 'prune_penalty', 'gamma', 'inference_params', 'n']
    Returns: None
    Start Line: 1353
    End Line: 1466
    Complexity: 11
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Performs sophisticated inference to find the optimal policy for a given generative model and prior preferences.

    Parameters
    ----------
    qs: ``numpy.ndarray`` of dtype object
        Marginal posterior beliefs over hidden states at a given timepoint.
    policies: ``list`` of 1D ``numpy.ndarray``                    inference_params = {"num_iter": 10, "dF": 1.0, "dF_tol": 0.001, "compute_vfe": False}

        ``list`` that stores each policy as a 1D array in ``policies[p_idx]``. Shape of ``policies[p_idx]`` 
        is ``(num_factors)`` where ``num_factors`` is the number of control factors.        
    A: ``numpy.ndarray`` of dtype object
        Sensory likelihood mapping or 'observation model', mapping from hidden states to observations. Each element ``A[m]`` of
        stores an ``numpy.ndarray`` multidimensional array for observation modality ``m``, whose entries ``A[m][i, j, k, ...]`` store 
        the probability of observation level ``i`` given hidden state levels ``j, k, ...``
    B: ``numpy.ndarray`` of dtype object
        Dynamics likelihood mapping or 'transition model', mapping from hidden states at ``t`` to hidden states at ``t+1``, given some control state ``u``.
        Each element ``B[f]`` of this object array stores a 3-D tensor for hidden state factor ``f``, whose entries ``B[f][s, v, u]`` store the probability
        of hidden state level ``s`` at the current time, given hidden state level ``v`` and action ``u`` at the previous time.
    C: ``numpy.ndarray`` of dtype object
       Prior over observations or 'prior preferences', storing the "value" of each outcome in terms of relative log probabilities. 
       This is softmaxed to form a proper probability distribution before being used to compute the expected utility term of the expected free energy.
    A_factor_list: ``list`` of ``list`` of ``int``
        List of lists, where ``A_factor_list[m]`` is a list of the hidden state factor indices that observation modality with the index ``m`` depends on
    B_factor_list: ``list`` of ``list`` of ``int``
        List of lists of hidden state factors each hidden state factor depends on. Each element ``B_factor_list[i]`` is a list of the factor indices that factor i's dynamics depend on.
    I: ``numpy.ndarray`` of dtype object
        For each state factor, contains a 2D ``numpy.ndarray`` whose element i,j yields the probability 
        of reaching the goal state backwards from state j after i steps.
    horizon: ``int``
        The temporal depth of the policy
    policy_prune_threshold: ``float``
        The threshold for pruning policies that are below a certain probability
    state_prune_threshold: ``float``
        The threshold for pruning states in the expectation that are below a certain probability
    prune_penalty: ``float``
        Penalty to add to the EFE when a policy is pruned
    gamma: ``float``, default 16.0
        Prior precision over policies, scales the contribution of the expected free energy to the posterior over policies
    n: ``int``
        timestep in the future we are calculating

    Returns
    ----------
    q_pi: 1D ``numpy.ndarray``
        Posterior beliefs over policies, i.e. a vector containing one posterior probability per policy.

    G: 1D ``numpy.ndarray``
        Negative expected free energies of each policy, i.e. a vector containing one negative expected free energy per policy.
    Description: 
    Description Embedding: 
    Comments: ['ignore low probability actions in the search tree', 'TODO shouldnt we have to add extra penalty for branches no longer considered?', 'or assume these are already low EFE (high NEFE) anyway?', 'average over outcomes', 'ignore low probability states in the search tree']

  ../pymdp/control.py::update_posterior_policies:
    name: update_posterior_policies
    Module: None
    Signature: update_posterior_policies(qs, A, B, C, policies, use_utility, use_states_info_gain, use_param_info_gain, pA, pB, E, I, gamma)
    function_exe_cmd: pymdp.control.update_posterior_policies(qs, A, B, C, policies, use_utility, use_states_info_gain, use_param_info_gain, pA, pB, E, I, gamma)
    Parameters: ['qs', 'A', 'B', 'C', 'policies', 'use_utility', 'use_states_info_gain', 'use_param_info_gain', 'pA', 'pB', 'E', 'I', 'gamma']
    Returns: None
    Start Line: 266
    End Line: 362
    Complexity: 9
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Update posterior beliefs about policies by computing expected free energy of each policy and integrating that
    with the prior over policies ``E``. This is intended to be used in conjunction
    with the ``update_posterior_states`` method of the ``inference`` module, since only the posterior about the hidden states at the current timestep
    ``qs`` is assumed to be provided, unconditional on policies. The predictive posterior over hidden states under all policies Q(s, pi) is computed 
    using the starting posterior about states at the current timestep ``qs`` and the generative model (e.g. ``A``, ``B``, ``C``)

    Parameters
    ----------
    qs: ``numpy.ndarray`` of dtype object
        Marginal posterior beliefs over hidden states at current timepoint (unconditioned on policies)
    A: ``numpy.ndarray`` of dtype object
        Sensory likelihood mapping or 'observation model', mapping from hidden states to observations. Each element ``A[m]`` of
        stores an ``numpy.ndarray`` multidimensional array for observation modality ``m``, whose entries ``A[m][i, j, k, ...]`` store 
        the probability of observation level ``i`` given hidden state levels ``j, k, ...``
    B: ``numpy.ndarray`` of dtype object
        Dynamics likelihood mapping or 'transition model', mapping from hidden states at ``t`` to hidden states at ``t+1``, given some control state ``u``.
        Each element ``B[f]`` of this object array stores a 3-D tensor for hidden state factor ``f``, whose entries ``B[f][s, v, u]`` store the probability
        of hidden state level ``s`` at the current time, given hidden state level ``v`` and action ``u`` at the previous time.
    C: ``numpy.ndarray`` of dtype object
       Prior over observations or 'prior preferences', storing the "value" of each outcome in terms of relative log probabilities. 
       This is softmaxed to form a proper probability distribution before being used to compute the expected utility term of the expected free energy.
    policies: ``list`` of 2D ``numpy.ndarray``
        ``list`` that stores each policy in ``policies[p_idx]``. Shape of ``policies[p_idx]`` is ``(num_timesteps, num_factors)`` where `num_timesteps` is the temporal
        depth of the policy and ``num_factors`` is the number of control factors.
    use_utility: ``Bool``, default ``True``
        Boolean flag that determines whether expected utility should be incorporated into computation of EFE.
    use_states_info_gain: ``Bool``, default ``True``
        Boolean flag that determines whether state epistemic value (info gain about hidden states) should be incorporated into computation of EFE.
    use_param_info_gain: ``Bool``, default ``False`` 
        Boolean flag that determines whether parameter epistemic value (info gain about generative model parameters) should be incorporated into computation of EFE.
    pA: ``numpy.ndarray`` of dtype object, optional
        Dirichlet parameters over observation model (same shape as ``A``)
    pB: ``numpy.ndarray`` of dtype object, optional
        Dirichlet parameters over transition model (same shape as ``B``)
    E: 1D ``numpy.ndarray``, optional
        Vector of prior probabilities of each policy (what's referred to in the active inference literature as "habits")
    I: ``numpy.ndarray`` of dtype object
        For each state factor, contains a 2D ``numpy.ndarray`` whose element i,j yields the probability 
        of reaching the goal state backwards from state j after i steps.
    gamma: float, default 16.0
        Prior precision over policies, scales the contribution of the expected free energy to the posterior over policies

    Returns
    ----------
    q_pi: 1D ``numpy.ndarray``
        Posterior beliefs over policies, i.e. a vector containing one posterior probability per policy.
    G: 1D ``numpy.ndarray``
        Negative expected free energies of each policy, i.e. a vector containing one negative expected free energy per policy.
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/control.py::update_posterior_policies_factorized:
    name: update_posterior_policies_factorized
    Module: None
    Signature: update_posterior_policies_factorized(qs, A, B, C, A_factor_list, B_factor_list, policies, use_utility, use_states_info_gain, use_param_info_gain, pA, pB, E, I, gamma)
    function_exe_cmd: pymdp.control.update_posterior_policies_factorized(qs, A, B, C, A_factor_list, B_factor_list, policies, use_utility, use_states_info_gain, use_param_info_gain, pA, pB, E, I, gamma)
    Parameters: ['qs', 'A', 'B', 'C', 'A_factor_list', 'B_factor_list', 'policies', 'use_utility', 'use_states_info_gain', 'use_param_info_gain', 'pA', 'pB', 'E', 'I', 'gamma']
    Returns: None
    Start Line: 364
    End Line: 468
    Complexity: 9
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Update posterior beliefs about policies by computing expected free energy of each policy and integrating that
    with the prior over policies ``E``. This is intended to be used in conjunction
    with the ``update_posterior_states`` method of the ``inference`` module, since only the posterior about the hidden states at the current timestep
    ``qs`` is assumed to be provided, unconditional on policies. The predictive posterior over hidden states under all policies Q(s, pi) is computed 
    using the starting posterior about states at the current timestep ``qs`` and the generative model (e.g. ``A``, ``B``, ``C``)

    Parameters
    ----------
    qs: ``numpy.ndarray`` of dtype object
        Marginal posterior beliefs over hidden states at current timepoint (unconditioned on policies)
    A: ``numpy.ndarray`` of dtype object
        Sensory likelihood mapping or 'observation model', mapping from hidden states to observations. Each element ``A[m]`` of
        stores an ``numpy.ndarray`` multidimensional array for observation modality ``m``, whose entries ``A[m][i, j, k, ...]`` store 
        the probability of observation level ``i`` given hidden state levels ``j, k, ...``
    B: ``numpy.ndarray`` of dtype object
        Dynamics likelihood mapping or 'transition model', mapping from hidden states at ``t`` to hidden states at ``t+1``, given some control state ``u``.
        Each element ``B[f]`` of this object array stores a 3-D tensor for hidden state factor ``f``, whose entries ``B[f][s, v, u]`` store the probability
        of hidden state level ``s`` at the current time, given hidden state level ``v`` and action ``u`` at the previous time.
    C: ``numpy.ndarray`` of dtype object
       Prior over observations or 'prior preferences', storing the "value" of each outcome in terms of relative log probabilities. 
       This is softmaxed to form a proper probability distribution before being used to compute the expected utility term of the expected free energy.
    A_factor_list: ``list`` of ``list``s of ``int``
        ``list`` that stores the indices of the hidden state factor indices that each observation modality depends on. For example, if ``A_factor_list[m] = [0, 1]``, then
        observation modality ``m`` depends on hidden state factors 0 and 1.
    B_factor_list: ``list`` of ``list``s of ``int``
        ``list`` that stores the indices of the hidden state factor indices that each hidden state factor depends on. For example, if ``B_factor_list[f] = [0, 1]``, then
        the transitions in hidden state factor ``f`` depend on hidden state factors 0 and 1.
    policies: ``list`` of 2D ``numpy.ndarray``
        ``list`` that stores each policy in ``policies[p_idx]``. Shape of ``policies[p_idx]`` is ``(num_timesteps, num_factors)`` where `num_timesteps` is the temporal
        depth of the policy and ``num_factors`` is the number of control factors.
    use_utility: ``Bool``, default ``True``
        Boolean flag that determines whether expected utility should be incorporated into computation of EFE.
    use_states_info_gain: ``Bool``, default ``True``
        Boolean flag that determines whether state epistemic value (info gain about hidden states) should be incorporated into computation of EFE.
    use_param_info_gain: ``Bool``, default ``False`` 
        Boolean flag that determines whether parameter epistemic value (info gain about generative model parameters) should be incorporated into computation of EFE.
    pA: ``numpy.ndarray`` of dtype object, optional
        Dirichlet parameters over observation model (same shape as ``A``)
    pB: ``numpy.ndarray`` of dtype object, optional
        Dirichlet parameters over transition model (same shape as ``B``)
    E: 1D ``numpy.ndarray``, optional
        Vector of prior probabilities of each policy (what's referred to in the active inference literature as "habits")
    I: ``numpy.ndarray`` of dtype object
        For each state factor, contains a 2D ``numpy.ndarray`` whose element i,j yields the probability 
        of reaching the goal state backwards from state j after i steps.
    gamma: float, default 16.0
        Prior precision over policies, scales the contribution of the expected free energy to the posterior over policies

    Returns
    ----------
    q_pi: 1D ``numpy.ndarray``
        Posterior beliefs over policies, i.e. a vector containing one posterior probability per policy.
    G: 1D ``numpy.ndarray``
        Negative expected free energies of each policy, i.e. a vector containing one negative expected free energy per policy.
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/control.py::update_posterior_policies_full:
    name: update_posterior_policies_full
    Module: None
    Signature: update_posterior_policies_full(qs_seq_pi, A, B, C, policies, use_utility, use_states_info_gain, use_param_info_gain, prior, pA, pB, F, E, I, gamma)
    function_exe_cmd: pymdp.control.update_posterior_policies_full(qs_seq_pi, A, B, C, policies, use_utility, use_states_info_gain, use_param_info_gain, prior, pA, pB, F, E, I, gamma)
    Parameters: ['qs_seq_pi', 'A', 'B', 'C', 'policies', 'use_utility', 'use_states_info_gain', 'use_param_info_gain', 'prior', 'pA', 'pB', 'F', 'E', 'I', 'gamma']
    Returns: None
    Start Line: 13
    End Line: 133
    Complexity: 12
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Update posterior beliefs about policies by computing expected free energy of each policy and integrating that
    with the variational free energy of policies ``F`` and prior over policies ``E``. This is intended to be used in conjunction
    with the ``update_posterior_states_full`` method of ``inference.py``, since the full posterior over future timesteps, under all policies, is
    assumed to be provided in the input array ``qs_seq_pi``.

    Parameters
    ----------
    qs_seq_pi: ``numpy.ndarray`` of dtype object
        Posterior beliefs over hidden states for each policy. Nesting structure is policies, timepoints, factors,
        where e.g. ``qs_seq_pi[p][t][f]`` stores the marginal belief about factor ``f`` at timepoint ``t`` under policy ``p``.
    A: ``numpy.ndarray`` of dtype object
        Sensory likelihood mapping or 'observation model', mapping from hidden states to observations. Each element ``A[m]`` of
        stores an ``numpy.ndarray`` multidimensional array for observation modality ``m``, whose entries ``A[m][i, j, k, ...]`` store 
        the probability of observation level ``i`` given hidden state levels ``j, k, ...``
    B: ``numpy.ndarray`` of dtype object
        Dynamics likelihood mapping or 'transition model', mapping from hidden states at ``t`` to hidden states at ``t+1``, given some control state ``u``.
        Each element ``B[f]`` of this object array stores a 3-D tensor for hidden state factor ``f``, whose entries ``B[f][s, v, u]`` store the probability
        of hidden state level ``s`` at the current time, given hidden state level ``v`` and action ``u`` at the previous time.
    C: ``numpy.ndarray`` of dtype object
       Prior over observations or 'prior preferences', storing the "value" of each outcome in terms of relative log probabilities. 
       This is softmaxed to form a proper probability distribution before being used to compute the expected utility term of the expected free energy.
    policies: ``list`` of 2D ``numpy.ndarray``
        ``list`` that stores each policy in ``policies[p_idx]``. Shape of ``policies[p_idx]`` is ``(num_timesteps, num_factors)`` where `num_timesteps` is the temporal
        depth of the policy and ``num_factors`` is the number of control factors.
    use_utility: ``Bool``, default ``True``
        Boolean flag that determines whether expected utility should be incorporated into computation of EFE.
    use_states_info_gain: ``Bool``, default ``True``
        Boolean flag that determines whether state epistemic value (info gain about hidden states) should be incorporated into computation of EFE.
    use_param_info_gain: ``Bool``, default ``False`` 
        Boolean flag that determines whether parameter epistemic value (info gain about generative model parameters) should be incorporated into computation of EFE. 
    prior: ``numpy.ndarray`` of dtype object, default ``None``
        If provided, this is a ``numpy`` object array with one sub-array per hidden state factor, that stores the prior beliefs about initial states. 
        If ``None``, this defaults to a flat (uninformative) prior over hidden states.
    pA: ``numpy.ndarray`` of dtype object, default ``None``
        Dirichlet parameters over observation model (same shape as ``A``)
    pB: ``numpy.ndarray`` of dtype object, default ``None``
        Dirichlet parameters over transition model (same shape as ``B``)
    F: 1D ``numpy.ndarray``, default ``None``
        Vector of variational free energies for each policy
    E: 1D ``numpy.ndarray``, default ``None``
        Vector of prior probabilities of each policy (what's referred to in the active inference literature as "habits"). If ``None``, this defaults to a flat (uninformative) prior over policies.
    I: ``numpy.ndarray`` of dtype object
        For each state factor, contains a 2D ``numpy.ndarray`` whose element i,j yields the probability 
        of reaching the goal state backwards from state j after i steps.
    gamma: ``float``, default 16.0
        Prior precision over policies, scales the contribution of the expected free energy to the posterior over policies

    Returns
    ----------
    q_pi: 1D ``numpy.ndarray``
        Posterior beliefs over policies, i.e. a vector containing one posterior probability per policy.
    G: 1D ``numpy.ndarray``
        Negative expected free energies of each policy, i.e. a vector containing one negative expected free energy per policy.
    Description: 
    Description Embedding: 
    Comments: ['initialise expected observations', 'initialize (negative) expected free energies for all policies']

  ../pymdp/control.py::update_posterior_policies_full_factorized:
    name: update_posterior_policies_full_factorized
    Module: None
    Signature: update_posterior_policies_full_factorized(qs_seq_pi, A, B, C, A_factor_list, B_factor_list, policies, use_utility, use_states_info_gain, use_param_info_gain, prior, pA, pB, F, E, I, gamma)
    function_exe_cmd: pymdp.control.update_posterior_policies_full_factorized(qs_seq_pi, A, B, C, A_factor_list, B_factor_list, policies, use_utility, use_states_info_gain, use_param_info_gain, prior, pA, pB, F, E, I, gamma)
    Parameters: ['qs_seq_pi', 'A', 'B', 'C', 'A_factor_list', 'B_factor_list', 'policies', 'use_utility', 'use_states_info_gain', 'use_param_info_gain', 'prior', 'pA', 'pB', 'F', 'E', 'I', 'gamma']
    Returns: None
    Start Line: 135
    End Line: 263
    Complexity: 12
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Update posterior beliefs about policies by computing expected free energy of each policy and integrating that
    with the variational free energy of policies ``F`` and prior over policies ``E``. This is intended to be used in conjunction
    with the ``update_posterior_states_full`` method of ``inference.py``, since the full posterior over future timesteps, under all policies, is
    assumed to be provided in the input array ``qs_seq_pi``.

    Parameters
    ----------
    qs_seq_pi: ``numpy.ndarray`` of dtype object
        Posterior beliefs over hidden states for each policy. Nesting structure is policies, timepoints, factors,
        where e.g. ``qs_seq_pi[p][t][f]`` stores the marginal belief about factor ``f`` at timepoint ``t`` under policy ``p``.
    A: ``numpy.ndarray`` of dtype object
        Sensory likelihood mapping or 'observation model', mapping from hidden states to observations. Each element ``A[m]`` of
        stores an ``numpy.ndarray`` multidimensional array for observation modality ``m``, whose entries ``A[m][i, j, k, ...]`` store 
        the probability of observation level ``i`` given hidden state levels ``j, k, ...``
    B: ``numpy.ndarray`` of dtype object
        Dynamics likelihood mapping or 'transition model', mapping from hidden states at ``t`` to hidden states at ``t+1``, given some control state ``u``.
        Each element ``B[f]`` of this object array stores a 3-D tensor for hidden state factor ``f``, whose entries ``B[f][s, v, u]`` store the probability
        of hidden state level ``s`` at the current time, given hidden state level ``v`` and action ``u`` at the previous time.
    C: ``numpy.ndarray`` of dtype object
       Prior over observations or 'prior preferences', storing the "value" of each outcome in terms of relative log probabilities. 
       This is softmaxed to form a proper probability distribution before being used to compute the expected utility term of the expected free energy.
    A_factor_list: ``list`` of ``list``s of ``int``
        ``list`` that stores the indices of the hidden state factor indices that each observation modality depends on. For example, if ``A_factor_list[m] = [0, 1]``, then
        observation modality ``m`` depends on hidden state factors 0 and 1.
    B_factor_list: ``list`` of ``list``s of ``int``
        ``list`` that stores the indices of the hidden state factor indices that each hidden state factor depends on. For example, if ``B_factor_list[f] = [0, 1]``, then
        the transitions in hidden state factor ``f`` depend on hidden state factors 0 and 1.
    policies: ``list`` of 2D ``numpy.ndarray``
        ``list`` that stores each policy in ``policies[p_idx]``. Shape of ``policies[p_idx]`` is ``(num_timesteps, num_factors)`` where `num_timesteps` is the temporal
        depth of the policy and ``num_factors`` is the number of control factors.
    use_utility: ``Bool``, default ``True``
        Boolean flag that determines whether expected utility should be incorporated into computation of EFE.
    use_states_info_gain: ``Bool``, default ``True``
        Boolean flag that determines whether state epistemic value (info gain about hidden states) should be incorporated into computation of EFE.
    use_param_info_gain: ``Bool``, default ``False`` 
        Boolean flag that determines whether parameter epistemic value (info gain about generative model parameters) should be incorporated into computation of EFE. 
    prior: ``numpy.ndarray`` of dtype object, default ``None``
        If provided, this is a ``numpy`` object array with one sub-array per hidden state factor, that stores the prior beliefs about initial states. 
        If ``None``, this defaults to a flat (uninformative) prior over hidden states.
    pA: ``numpy.ndarray`` of dtype object, default ``None``
        Dirichlet parameters over observation model (same shape as ``A``)
    pB: ``numpy.ndarray`` of dtype object, default ``None``
        Dirichlet parameters over transition model (same shape as ``B``)
    F: 1D ``numpy.ndarray``, default ``None``
        Vector of variational free energies for each policy
    E: 1D ``numpy.ndarray``, default ``None``
        Vector of prior probabilities of each policy (what's referred to in the active inference literature as "habits"). If ``None``, this defaults to a flat (uninformative) prior over policies.
    I: ``numpy.ndarray`` of dtype object
        For each state factor, contains a 2D ``numpy.ndarray`` whose element i,j yields the probability 
        of reaching the goal state backwards from state j after i steps.
    gamma: ``float``, default 16.0
        Prior precision over policies, scales the contribution of the expected free energy to the posterior over policies

    Returns
    ----------
    q_pi: 1D ``numpy.ndarray``
        Posterior beliefs over policies, i.e. a vector containing one posterior probability per policy.
    G: 1D ``numpy.ndarray``
        Negative expected free energies of each policy, i.e. a vector containing one negative expected free energy per policy.
    Description: 
    Description Embedding: 
    Comments: ['initialise expected observations', 'initialize (negative) expected free energies for all policies']

  ../pymdp/envs/env.py::get_likelihood_dist:
    name: get_likelihood_dist
    Module: None
    Signature: get_likelihood_dist(self)
    function_exe_cmd: pymdp.envs.env.Env.get_likelihood_dist(self)
    Parameters: ['self']
    Returns: None
    Start Line: 61
    End Line: 64
    Complexity: 1
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: None
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/envs/env.py::get_rand_likelihood_dist:
    name: get_rand_likelihood_dist
    Module: None
    Signature: get_rand_likelihood_dist(self)
    function_exe_cmd: pymdp.envs.env.Env.get_rand_likelihood_dist(self)
    Parameters: ['self']
    Returns: None
    Start Line: 76
    End Line: 79
    Complexity: 1
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: None
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/envs/env.py::get_rand_transition_dist:
    name: get_rand_transition_dist
    Module: None
    Signature: get_rand_transition_dist(self)
    function_exe_cmd: pymdp.envs.env.Env.get_rand_transition_dist(self)
    Parameters: ['self']
    Returns: None
    Start Line: 81
    End Line: 84
    Complexity: 1
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: None
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/envs/env.py::get_transition_dist:
    name: get_transition_dist
    Module: None
    Signature: get_transition_dist(self)
    function_exe_cmd: pymdp.envs.env.Env.get_transition_dist(self)
    Parameters: ['self']
    Returns: None
    Start Line: 66
    End Line: 69
    Complexity: 1
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: None
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/envs/env.py::get_uniform_posterior:
    name: get_uniform_posterior
    Module: None
    Signature: get_uniform_posterior(self)
    function_exe_cmd: pymdp.envs.env.Env.get_uniform_posterior(self)
    Parameters: ['self']
    Returns: None
    Start Line: 71
    End Line: 74
    Complexity: 1
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: None
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/envs/env.py::render:
    name: render
    Module: None
    Signature: render(self)
    function_exe_cmd: pymdp.envs.env.Env.render(self)
    Parameters: ['self']
    Returns: None
    Start Line: 52
    End Line: 56
    Complexity: 1
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Rendering function, that typically creates a visual representation of the state of the environment at the current timestep.
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/envs/env.py::reset:
    name: reset
    Module: None
    Signature: reset(self, state)
    function_exe_cmd: pymdp.envs.env.Env.reset(self, state)
    Parameters: ['self', 'state']
    Returns: None
    Start Line: 30
    End Line: 34
    Complexity: 1
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Resets the initial state of the environment. Depending on case, it may be common to return an initial observation as well.
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/envs/env.py::sample_action:
    name: sample_action
    Module: None
    Signature: sample_action(self)
    function_exe_cmd: pymdp.envs.env.Env.sample_action(self)
    Parameters: ['self']
    Returns: None
    Start Line: 58
    End Line: 59
    Complexity: 1
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: None
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/envs/env.py::step:
    name: step
    Module: None
    Signature: step(self, action)
    function_exe_cmd: pymdp.envs.env.Env.step(self, action)
    Parameters: ['self', 'action']
    Returns: None
    Start Line: 36
    End Line: 50
    Complexity: 1
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Steps the environment forward using an action.

        Parameters
        ----------
        action
            The action, the type/format of which depends on the implementation.

        Returns
        ---------
        observation
            Sensory observations for an agent, the type/format of which depends on the implementation of ``step`` and the observation space of the agent.
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/envs/tmaze.py::get_likelihood_dist:
    name: get_likelihood_dist
    Module: None
    Signature: get_likelihood_dist(self)
    function_exe_cmd: pymdp.envs.tmaze.TMazeEnvNullOutcome.get_likelihood_dist(self)
    Parameters: ['self']
    Returns: None
    Start Line: 252
    End Line: 253
    Complexity: 1
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: None
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/envs/tmaze.py::get_rand_likelihood_dist:
    name: get_rand_likelihood_dist
    Module: None
    Signature: get_rand_likelihood_dist(self)
    function_exe_cmd: pymdp.envs.tmaze.TMazeEnv.get_rand_likelihood_dist(self)
    Parameters: ['self']
    Returns: None
    Start Line: 92
    End Line: 93
    Complexity: 1
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: None
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/envs/tmaze.py::get_rand_transition_dist:
    name: get_rand_transition_dist
    Module: None
    Signature: get_rand_transition_dist(self)
    function_exe_cmd: pymdp.envs.tmaze.TMazeEnv.get_rand_transition_dist(self)
    Parameters: ['self']
    Returns: None
    Start Line: 95
    End Line: 96
    Complexity: 1
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: None
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/envs/tmaze.py::get_transition_dist:
    name: get_transition_dist
    Module: None
    Signature: get_transition_dist(self)
    function_exe_cmd: pymdp.envs.tmaze.TMazeEnvNullOutcome.get_transition_dist(self)
    Parameters: ['self']
    Returns: None
    Start Line: 255
    End Line: 256
    Complexity: 1
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: None
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/envs/tmaze.py::render:
    name: render
    Module: None
    Signature: render(self)
    function_exe_cmd: pymdp.envs.tmaze.TMazeEnv.render(self)
    Parameters: ['self']
    Returns: None
    Start Line: 79
    End Line: 80
    Complexity: 1
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: None
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/envs/tmaze.py::reset:
    name: reset
    Module: None
    Signature: reset(self, state)
    function_exe_cmd: pymdp.envs.tmaze.TMazeEnvNullOutcome.reset(self, state)
    Parameters: ['self', 'state']
    Returns: None
    Start Line: 225
    End Line: 238
    Complexity: 2
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: None
    Description: 
    Description Embedding: 
    Comments: ['randomly select a reward condition']

  ../pymdp/envs/tmaze.py::reward_condition:
    name: reward_condition
    Module: None
    Signature: reward_condition(self)
    function_exe_cmd: pymdp.envs.tmaze.TMazeEnvNullOutcome.reward_condition(self)
    Parameters: ['self']
    Returns: None
    Start Line: 344
    End Line: 346
    Complexity: 1
    Is Method: True
    Is Async: False
    Decorators: property
    Docstring: None
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/envs/tmaze.py::sample_action:
    name: sample_action
    Module: None
    Signature: sample_action(self)
    function_exe_cmd: pymdp.envs.tmaze.TMazeEnvNullOutcome.sample_action(self)
    Parameters: ['self']
    Returns: None
    Start Line: 249
    End Line: 250
    Complexity: 1
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: None
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/envs/tmaze.py::state:
    name: state
    Module: None
    Signature: state(self)
    function_exe_cmd: pymdp.envs.tmaze.TMazeEnvNullOutcome.state(self)
    Parameters: ['self']
    Returns: None
    Start Line: 340
    End Line: 342
    Complexity: 1
    Is Method: True
    Is Async: False
    Decorators: property
    Docstring: None
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/envs/tmaze.py::step:
    name: step
    Module: None
    Signature: step(self, actions)
    function_exe_cmd: pymdp.envs.tmaze.TMazeEnvNullOutcome.step(self, actions)
    Parameters: ['self', 'actions']
    Returns: None
    Start Line: 240
    End Line: 246
    Complexity: 2
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: None
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/inference.py::average_states_over_policies:
    name: average_states_over_policies
    Module: None
    Signature: average_states_over_policies(qs_pi, q_pi)
    function_exe_cmd: pymdp.inference.average_states_over_policies(qs_pi, q_pi)
    Parameters: ['qs_pi', 'q_pi']
    Returns: None
    Start Line: 247
    End Line: 280
    Complexity: 4
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: This function computes a expected posterior over hidden states with respect to the posterior over policies, 
    also known as the 'Bayesian model average of states with respect to policies'.

    Parameters
    ----------
    qs_pi: ``numpy.ndarray`` of dtype object
        Posterior beliefs over hidden states for each policy. Nesting structure is policies, factors,
        where e.g. ``qs_pi[p][f]`` stores the marginal belief about factor ``f`` under policy ``p``.
    q_pi: ``numpy.ndarray`` of dtype object
        Posterior beliefs about policies where ``len(q_pi) = num_policies``

    Returns
    ---------
    qs_bma: ``numpy.ndarray`` of dtype object
        Marginal posterior over hidden states for the current timepoint, 
        averaged across policies according to their posterior probability given by ``q_pi``
    Description: 
    Description Embedding: 
    Comments: ['get the number of hidden state factors using the shape of the first-policy-conditioned posterior', 'get the dimensionalities of each hidden state factor']

  ../pymdp/inference.py::update_posterior_states:
    name: update_posterior_states
    Module: None
    Signature: update_posterior_states(A, obs, prior)
    function_exe_cmd: pymdp.inference.update_posterior_states(A, obs, prior)
    Parameters: ['A', 'obs', 'prior']
    Returns: None
    Start Line: 282
    End Line: 322
    Complexity: 2
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Update marginal posterior over hidden states using mean-field fixed point iteration 
    FPI or Fixed point iteration. 

    See the following links for details:
    http://www.cs.cmu.edu/~guestrin/Class/10708/recitations/r9/VI-view.pdf, slides 13- 18, and http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.137.221&rep=rep1&type=pdf, slides 24 - 38.

    Parameters
    ----------
    A: ``numpy.ndarray`` of dtype object
        Sensory likelihood mapping or 'observation model', mapping from hidden states to observations. Each element ``A[m]`` of
        stores an ``np.ndarray`` multidimensional array for observation modality ``m``, whose entries ``A[m][i, j, k, ...]`` store 
        the probability of observation level ``i`` given hidden state levels ``j, k, ...``
    obs: 1D ``numpy.ndarray``, ``numpy.ndarray`` of dtype object, int or tuple
        The observation (generated by the environment). If single modality, this can be a 1D ``np.ndarray``
        (one-hot vector representation) or an ``int`` (observation index)
        If multi-modality, this can be ``np.ndarray`` of dtype object whose entries are 1D one-hot vectors,
        or a tuple (of ``int``)
    prior: 1D ``numpy.ndarray`` or ``numpy.ndarray`` of dtype object, default None
        Prior beliefs about hidden states, to be integrated with the marginal likelihood to obtain
        a posterior distribution. If not provided, prior is set to be equal to a flat categorical distribution (at the level of
        the individual inference functions).
    **kwargs: keyword arguments 
        List of keyword/parameter arguments corresponding to parameter values for the fixed-point iteration
        algorithm ``algos.fpi.run_vanilla_fpi.py``

    Returns
    ----------
    qs: 1D ``numpy.ndarray`` or ``numpy.ndarray`` of dtype object
        Marginal posterior beliefs over hidden states at current timepoint
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/inference.py::update_posterior_states_factorized:
    name: update_posterior_states_factorized
    Module: None
    Signature: update_posterior_states_factorized(A, obs, num_obs, num_states, mb_dict, prior)
    function_exe_cmd: pymdp.inference.update_posterior_states_factorized(A, obs, num_obs, num_states, mb_dict, prior)
    Parameters: ['A', 'obs', 'num_obs', 'num_states', 'mb_dict', 'prior']
    Returns: None
    Start Line: 324
    End Line: 371
    Complexity: 2
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Update marginal posterior over hidden states using mean-field fixed point iteration 
    FPI or Fixed point iteration. This version identifies the Markov blanket of each factor using `A_factor_list`

    See the following links for details:
    http://www.cs.cmu.edu/~guestrin/Class/10708/recitations/r9/VI-view.pdf, slides 13- 18, and http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.137.221&rep=rep1&type=pdf, slides 24 - 38.

    Parameters
    ----------
    A: ``numpy.ndarray`` of dtype object
        Sensory likelihood mapping or 'observation model', mapping from hidden states to observations. Each element ``A[m]`` of
        stores an ``np.ndarray`` multidimensional array for observation modality ``m``, whose entries ``A[m][i, j, k, ...]`` store 
        the probability of observation level ``i`` given hidden state levels ``j, k, ...``
    obs: 1D ``numpy.ndarray``, ``numpy.ndarray`` of dtype object, int or tuple
        The observation (generated by the environment). If single modality, this can be a 1D ``np.ndarray``
        (one-hot vector representation) or an ``int`` (observation index)
        If multi-modality, this can be ``np.ndarray`` of dtype object whose entries are 1D one-hot vectors,
        or a tuple (of ``int``)
    num_obs: ``list`` of ``int``
        List of dimensionalities of each observation modality
    num_states: ``list`` of ``int``
        List of dimensionalities of each hidden state factor
    mb_dict: ``Dict``
        Dictionary with two keys (``A_factor_list`` and ``A_modality_list``), that stores the factor indices that influence each modality (``A_factor_list``)
        and the modality indices influenced by each factor (``A_modality_list``).
    prior: 1D ``numpy.ndarray`` or ``numpy.ndarray`` of dtype object, default None
        Prior beliefs about hidden states, to be integrated with the marginal likelihood to obtain
        a posterior distribution. If not provided, prior is set to be equal to a flat categorical distribution (at the level of
        the individual inference functions).
    **kwargs: keyword arguments 
        List of keyword/parameter arguments corresponding to parameter values for the fixed-point iteration
        algorithm ``algos.fpi.run_vanilla_fpi.py``

    Returns
    ----------
    qs: 1D ``numpy.ndarray`` or ``numpy.ndarray`` of dtype object
        Marginal posterior beliefs over hidden states at current timepoint
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/inference.py::update_posterior_states_full:
    name: update_posterior_states_full
    Module: None
    Signature: update_posterior_states_full(A, B, prev_obs, policies, prev_actions, prior, policy_sep_prior)
    function_exe_cmd: pymdp.inference.update_posterior_states_full(A, B, prev_obs, policies, prev_actions, prior, policy_sep_prior)
    Parameters: ['A', 'B', 'prev_obs', 'policies', 'prev_actions', 'prior', 'policy_sep_prior']
    Returns: None
    Start Line: 18
    End Line: 87
    Complexity: 3
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Update posterior over hidden states using marginal message passing

    Parameters
    ----------
    A: ``numpy.ndarray`` of dtype object
        Sensory likelihood mapping or 'observation model', mapping from hidden states to observations. Each element ``A[m]`` of
        stores an ``numpy.ndarray`` multidimensional array for observation modality ``m``, whose entries ``A[m][i, j, k, ...]`` store 
        the probability of observation level ``i`` given hidden state levels ``j, k, ...``
    B: ``numpy.ndarray`` of dtype object
        Dynamics likelihood mapping or 'transition model', mapping from hidden states at ``t`` to hidden states at ``t+1``, given some control state ``u``.
        Each element ``B[f]`` of this object array stores a 3-D tensor for hidden state factor ``f``, whose entries ``B[f][s, v, u]`` store the probability
        of hidden state level ``s`` at the current time, given hidden state level ``v`` and action ``u`` at the previous time.
    prev_obs: ``list``
        List of observations over time. Each observation in the list can be an ``int``, a ``list`` of ints, a ``tuple`` of ints, a one-hot vector or an object array of one-hot vectors.
    policies: ``list`` of 2D ``numpy.ndarray``
        List that stores each policy in ``policies[p_idx]``. Shape of ``policies[p_idx]`` is ``(num_timesteps, num_factors)`` where `num_timesteps` is the temporal
        depth of the policy and ``num_factors`` is the number of control factors.
    prior: ``numpy.ndarray`` of dtype object, default ``None``
        If provided, this a ``numpy.ndarray`` of dtype object, with one sub-array per hidden state factor, that stores the prior beliefs about initial states. 
        If ``None``, this defaults to a flat (uninformative) prior over hidden states.
    policy_sep_prior: ``Bool``, default ``True``
        Flag determining whether the prior beliefs from the past are unconditioned on policy, or separated by /conditioned on the policy variable.
    **kwargs: keyword arguments
        Optional keyword arguments for the function ``algos.mmp.run_mmp``

    Returns
    ---------
    qs_seq_pi: ``numpy.ndarray`` of dtype object
        Posterior beliefs over hidden states for each policy. Nesting structure is policies, timepoints, factors,
        where e.g. ``qs_seq_pi[p][t][f]`` stores the marginal belief about factor ``f`` at timepoint ``t`` under policy ``p``.
    F: 1D ``numpy.ndarray``
        Vector of variational free energies for each policy
    Description: 
    Description Embedding: 
    Comments: ['variational free energy of policies', 'get sequence and the free energy for policy']

  ../pymdp/inference.py::update_posterior_states_full_factorized:
    name: update_posterior_states_full_factorized
    Module: None
    Signature: update_posterior_states_full_factorized(A, mb_dict, B, B_factor_list, prev_obs, policies, prev_actions, prior, policy_sep_prior)
    function_exe_cmd: pymdp.inference.update_posterior_states_full_factorized(A, mb_dict, B, B_factor_list, prev_obs, policies, prev_actions, prior, policy_sep_prior)
    Parameters: ['A', 'mb_dict', 'B', 'B_factor_list', 'prev_obs', 'policies', 'prev_actions', 'prior', 'policy_sep_prior']
    Returns: None
    Start Line: 89
    End Line: 167
    Complexity: 3
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Update posterior over hidden states using marginal message passing

    Parameters
    ----------
    A: ``numpy.ndarray`` of dtype object
        Sensory likelihood mapping or 'observation model', mapping from hidden states to observations. Each element ``A[m]`` of
        stores an ``numpy.ndarray`` multidimensional array for observation modality ``m``, whose entries ``A[m][i, j, k, ...]`` store 
        the probability of observation level ``i`` given hidden state levels ``j, k, ...``
    mb_dict: ``Dict``
        Dictionary with two keys (``A_factor_list`` and ``A_modality_list``), that stores the factor indices that influence each modality (``A_factor_list``)
        and the modality indices influenced by each factor (``A_modality_list``).
    B: ``numpy.ndarray`` of dtype object
        Dynamics likelihood mapping or 'transition model', mapping from hidden states at ``t`` to hidden states at ``t+1``, given some control state ``u``.
        Each element ``B[f]`` of this object array stores a 3-D tensor for hidden state factor ``f``, whose entries ``B[f][s, v, u]`` store the probability
        of hidden state level ``s`` at the current time, given hidden state level ``v`` and action ``u`` at the previous time.
    B_factor_list: ``list`` of ``list`` of ``int``
        List of lists of hidden state factors each hidden state factor depends on. Each element ``B_factor_list[i]`` is a list of the factor indices that factor i's dynamics depend on.
    prev_obs: ``list``
        List of observations over time. Each observation in the list can be an ``int``, a ``list`` of ints, a ``tuple`` of ints, a one-hot vector or an object array of one-hot vectors.
    policies: ``list`` of 2D ``numpy.ndarray``
        List that stores each policy in ``policies[p_idx]``. Shape of ``policies[p_idx]`` is ``(num_timesteps, num_factors)`` where `num_timesteps` is the temporal
        depth of the policy and ``num_factors`` is the number of control factors.
    prior: ``numpy.ndarray`` of dtype object, default ``None``
        If provided, this a ``numpy.ndarray`` of dtype object, with one sub-array per hidden state factor, that stores the prior beliefs about initial states. 
        If ``None``, this defaults to a flat (uninformative) prior over hidden states.
    policy_sep_prior: ``Bool``, default ``True``
        Flag determining whether the prior beliefs from the past are unconditioned on policy, or separated by /conditioned on the policy variable.
    **kwargs: keyword arguments
        Optional keyword arguments for the function ``algos.mmp.run_mmp``

    Returns
    ---------
    qs_seq_pi: ``numpy.ndarray`` of dtype object
        Posterior beliefs over hidden states for each policy. Nesting structure is policies, timepoints, factors,
        where e.g. ``qs_seq_pi[p][t][f]`` stores the marginal belief about factor ``f`` at timepoint ``t`` under policy ``p``.
    F: 1D ``numpy.ndarray``
        Vector of variational free energies for each policy
    Description: 
    Description Embedding: 
    Comments: ['variational free energy of policies', 'get sequence and the free energy for policy']

  ../pymdp/jax/agent.py::infer_parameters:
    name: infer_parameters
    Module: None
    Signature: infer_parameters(self, beliefs_A, outcomes, actions, beliefs_B, lr_pA, lr_pB)
    function_exe_cmd: pymdp.jax.agent.Agent.infer_parameters(self, beliefs_A, outcomes, actions, beliefs_B, lr_pA, lr_pB)
    Parameters: ['self', 'beliefs_A', 'outcomes', 'actions', 'beliefs_B', 'lr_pA', 'lr_pB']
    Returns: None
    Start Line: 258
    End Line: 315
    Complexity: 8
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: None
    Description: 
    Description Embedding: 
    Comments: ['if you have updated your beliefs about transitions, you need to re-compute the I matrix used for inductive inferenece']

  ../pymdp/jax/agent.py::infer_policies:
    name: infer_policies
    Module: None
    Signature: infer_policies(self, qs)
    function_exe_cmd: pymdp.jax.agent.Agent.infer_policies(self, qs)
    Parameters: ['self', 'qs']
    Returns: None
    Start Line: 383
    End Line: 423
    Complexity: 1
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Perform policy inference by optimizing a posterior (categorical) distribution over policies.
        This distribution is computed as the softmax of ``G * gamma + lnE`` where ``G`` is the negative expected
        free energy of policies, ``gamma`` is a policy precision and ``lnE`` is the (log) prior probability of policies.
        This function returns the posterior over policies as well as the negative expected free energy of each policy.

        Returns
        ----------
        q_pi: 1D ``numpy.ndarray``
            Posterior beliefs over policies, i.e. a vector containing one posterior probability per policy.
        G: 1D ``numpy.ndarray``
            Negative expected free energies of each policy, i.e. a vector containing one negative expected free energy per policy.
    Description: 
    Description Embedding: 
    Comments: ['only get the posterior belief held at the current timepoint']

  ../pymdp/jax/agent.py::infer_states:
    name: infer_states
    Module: None
    Signature: infer_states(self, observations, empirical_prior)
    function_exe_cmd: pymdp.jax.agent.Agent.infer_states(self, observations, empirical_prior)
    Parameters: ['self', 'observations', 'empirical_prior']
    Returns: None
    Start Line: 317
    End Line: 367
    Complexity: 4
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Update approximate posterior over hidden states by solving variational inference problem, given an observation.

        Parameters
        ----------
        observations: ``list`` or ``tuple`` of ints
            The observation input. Each entry ``observation[m]`` stores one-hot vectors representing the observations for modality ``m``.
        past_actions: ``list`` or ``tuple`` of ints
            The action input. Each entry ``past_actions[f]`` stores indices (or one-hots?) representing the actions for control factor ``f``.
        empirical_prior: ``list`` or ``tuple`` of ``jax.numpy.ndarray`` of dtype object
            Empirical prior beliefs over hidden states. Depending on the inference algorithm chosen, the resulting ``empirical_prior`` variable may be a matrix (or list of matrices) 
            of additional dimensions to encode extra conditioning variables like timepoint and policy.
        Returns
        ---------
        qs: ``numpy.ndarray`` of dtype object
            Posterior beliefs over hidden states. Depending on the inference algorithm chosen, the resulting ``qs`` variable will have additional sub-structure to reflect whether
            beliefs are additionally conditioned on timepoint and policy.
            For example, in case the ``self.inference_algo == 'MMP' `` indexing structure is policy->timepoint-->factor, so that 
            ``qs[p_idx][t_idx][f_idx]`` refers to beliefs about marginal factor ``f_idx`` expected under policy ``p_idx`` 
            at timepoint ``t_idx``.
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/jax/agent.py::multiaction_probabilities:
    name: multiaction_probabilities
    Module: None
    Signature: multiaction_probabilities(self, q_pi)
    function_exe_cmd: pymdp.jax.agent.Agent.multiaction_probabilities(self, q_pi)
    Parameters: ['self', 'q_pi']
    Returns: None
    Start Line: 425
    End Line: 454
    Complexity: 3
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Compute probabilities of unique multi-actions from the posterior over policies.

        Parameters
        ----------
        q_pi: 1D ``numpy.ndarray``
        Posterior beliefs over policies, i.e. a vector containing one posterior probability per policy.

        Returns
        ----------
        multi-action: 1D ``jax.numpy.ndarray``
            Vector containing probabilities of possible multi-actions for different factors
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/jax/agent.py::sample_action:
    name: sample_action
    Module: None
    Signature: sample_action(self, q_pi, rng_key)
    function_exe_cmd: pymdp.jax.agent.Agent.sample_action(self, q_pi, rng_key)
    Parameters: ['self', 'q_pi', 'rng_key']
    Returns: None
    Start Line: 456
    End Line: 478
    Complexity: 6
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Sample or select a discrete action from the posterior over control states.

        Returns
        ----------
        action: 1D ``jax.numpy.ndarray``
            Vector containing the indices of the actions for each control factor
        action_probs: 2D ``jax.numpy.ndarray``
            Array of action probabilities
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/jax/agent.py::unique_multiactions:
    name: unique_multiactions
    Module: None
    Signature: unique_multiactions(self)
    function_exe_cmd: pymdp.jax.agent.Agent.unique_multiactions(self)
    Parameters: ['self']
    Returns: None
    Start Line: 253
    End Line: 256
    Complexity: 1
    Is Method: True
    Is Async: False
    Decorators: property
    Docstring: None
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/jax/agent.py::update_empirical_prior:
    name: update_empirical_prior
    Module: None
    Signature: update_empirical_prior(self, action, qs)
    function_exe_cmd: pymdp.jax.agent.Agent.update_empirical_prior(self, action, qs)
    Parameters: ['self', 'action', 'qs']
    Returns: None
    Start Line: 369
    End Line: 381
    Complexity: 2
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: None
    Description: 
    Description Embedding: 
    Comments: ['return empirical_prior, and the history of posterior beliefs (filtering distributions) held about hidden states at times 1, 2 ... t', 'this computation of the predictive prior is correct only for fully factorised Bs.', "in the case of the 'mmp' or 'vmp' we have to use D as prior parameter for infer states"]

  ../pymdp/jax/algos.py::add:
    name: add
    Module: None
    Signature: add(x, y)
    function_exe_cmd: pymdp.jax.algos.add(x, y)
    Parameters: ['x', 'y']
    Returns: None
    Start Line: 11
    End Line: 12
    Complexity: 1
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: None
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/jax/algos.py::all_marginal_log_likelihood:
    name: all_marginal_log_likelihood
    Module: None
    Signature: all_marginal_log_likelihood(qs, log_likelihoods, all_factor_lists)
    function_exe_cmd: pymdp.jax.algos.all_marginal_log_likelihood(qs, log_likelihoods, all_factor_lists)
    Parameters: ['qs', 'log_likelihoods', 'all_factor_lists']
    Returns: None
    Start Line: 18
    End Line: 30
    Complexity: 3
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: None
    Description: 
    Description Embedding: 
    Comments: ['insted of a double loop we could have a list defining m to f mapping', 'which could be resolved with a single tree_map cast']

  ../pymdp/jax/algos.py::get_mmp_messages:
    name: get_mmp_messages
    Module: None
    Signature: get_mmp_messages(ln_B, B, qs, ln_prior, B_deps)
    function_exe_cmd: pymdp.jax.algos.get_mmp_messages(ln_B, B, qs, ln_prior, B_deps)
    Parameters: ['ln_B', 'B', 'qs', 'ln_prior', 'B_deps']
    Returns: None
    Start Line: 275
    End Line: 329
    Complexity: 7
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: None
    Description: 
    Description Embedding: 
    Comments: ['append log_prior as a first message', 'mutliply with 1/2 all but the last msg']

  ../pymdp/jax/algos.py::get_vmp_messages:
    name: get_vmp_messages
    Module: None
    Signature: get_vmp_messages(ln_B, B, qs, ln_prior, B_dependencies)
    function_exe_cmd: pymdp.jax.algos.get_vmp_messages(ln_B, B, qs, ln_prior, B_dependencies)
    Parameters: ['ln_B', 'B', 'qs', 'ln_prior', 'B_dependencies']
    Returns: None
    Start Line: 212
    End Line: 255
    Complexity: 3
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: None
    Description: 
    Description Embedding: 
    Comments: ['function that effectively "slices" a list with a set of indices `f_idx`', 'make a list of lists, where each list contains all dependencies of a factor except itself', 'make list of integers, where each integer is the position of the self-factor in its dependencies list', 'this is a list of matrices, where each matrix is the marginal transition tensor for factor f', 'shape = (T, states_f_{t+1}, states_f_{t})', 'ln_b has shape (num_states, num_states) qs[:-1] has shape (T-1, num_states)', 'q_i B_ij']

  ../pymdp/jax/algos.py::marginal_log_likelihood:
    name: marginal_log_likelihood
    Module: None
    Signature: marginal_log_likelihood(qs, log_likelihood, i)
    function_exe_cmd: pymdp.jax.algos.marginal_log_likelihood(qs, log_likelihood, i)
    Parameters: ['qs', 'log_likelihood', 'i']
    Returns: None
    Start Line: 14
    End Line: 16
    Complexity: 1
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: None
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/jax/algos.py::mirror_gradient_descent_step:
    name: mirror_gradient_descent_step
    Module: None
    Signature: mirror_gradient_descent_step(tau, ln_A, lnB_past, lnB_future, ln_qs)
    function_exe_cmd: pymdp.jax.algos.mirror_gradient_descent_step(tau, ln_A, lnB_past, lnB_future, ln_qs)
    Parameters: ['tau', 'ln_A', 'lnB_past', 'lnB_future', 'ln_qs']
    Returns: None
    Start Line: 95
    End Line: 104
    Complexity: 1
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: u_{k+1} = u_{k} - 
abla_p F_k
    p_k = softmax(u_k)
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/jax/algos.py::mll_factors:
    name: mll_factors
    Module: None
    Signature: mll_factors(qs, ll_m, factor_list_m)
    function_exe_cmd: pymdp.jax.algos.mll_factors(qs, ll_m, factor_list_m)
    Parameters: ['qs', 'll_m', 'factor_list_m']
    Returns: List
    Start Line: 32
    End Line: 37
    Complexity: 1
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: None
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/jax/algos.py::run_factorized_fpi:
    name: run_factorized_fpi
    Module: None
    Signature: run_factorized_fpi(A, obs, prior, A_dependencies, num_iter)
    function_exe_cmd: pymdp.jax.algos.run_factorized_fpi(A, obs, prior, A_dependencies, num_iter)
    Parameters: ['A', 'obs', 'prior', 'A_dependencies', 'num_iter']
    Returns: None
    Start Line: 68
    End Line: 93
    Complexity: 1
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Run the fixed point iteration algorithm with sparse dependencies between factors and outcomes (stored in `A_dependencies`)
    Description: 
    Description Embedding: 
    Comments: ['Step 1: Compute log likelihoods for each factor', 'Step 2: Map prior to log space and create initial log-posterior', 'Step 3: Iterate until convergence', 'Step 4: Map result to factorised posterior']

  ../pymdp/jax/algos.py::run_mmp:
    name: run_mmp
    Module: None
    Signature: run_mmp(A, B, obs, prior, A_dependencies, B_dependencies, num_iter, tau)
    function_exe_cmd: pymdp.jax.algos.run_mmp(A, B, obs, prior, A_dependencies, B_dependencies, num_iter, tau)
    Parameters: ['A', 'B', 'obs', 'prior', 'A_dependencies', 'B_dependencies', 'num_iter', 'tau']
    Returns: None
    Start Line: 331
    End Line: 343
    Complexity: 1
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: None
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/jax/algos.py::run_online_filtering:
    name: run_online_filtering
    Module: None
    Signature: run_online_filtering(A, B, obs, prior, A_dependencies, num_iter, tau)
    function_exe_cmd: pymdp.jax.algos.run_online_filtering(A, B, obs, prior, A_dependencies, num_iter, tau)
    Parameters: ['A', 'B', 'obs', 'prior', 'A_dependencies', 'num_iter', 'tau']
    Returns: None
    Start Line: 345
    End Line: 348
    Complexity: 1
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Runs online filtering (HAVE TO REPLACE WITH OVF CODE)
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/jax/algos.py::run_vanilla_fpi:
    name: run_vanilla_fpi
    Module: None
    Signature: run_vanilla_fpi(A, obs, prior, num_iter, distr_obs)
    function_exe_cmd: pymdp.jax.algos.run_vanilla_fpi(A, obs, prior, num_iter, distr_obs)
    Parameters: ['A', 'obs', 'prior', 'num_iter', 'distr_obs']
    Returns: None
    Start Line: 39
    End Line: 66
    Complexity: 1
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Vanilla fixed point iteration (jaxified)
    Description: 
    Description Embedding: 
    Comments: ['Step 1: Compute log likelihoods for each factor', 'log_likelihoods = [ll] * nf', 'Step 2: Map prior to log space and create initial log-posterior', 'Step 3: Iterate until convergence', 'Step 4: Map result to factorised posterior']

  ../pymdp/jax/algos.py::run_vmp:
    name: run_vmp
    Module: None
    Signature: run_vmp(A, B, obs, prior, A_dependencies, B_dependencies, num_iter, tau)
    function_exe_cmd: pymdp.jax.algos.run_vmp(A, B, obs, prior, A_dependencies, B_dependencies, num_iter, tau)
    Parameters: ['A', 'B', 'obs', 'prior', 'A_dependencies', 'B_dependencies', 'num_iter', 'tau']
    Returns: None
    Start Line: 257
    End Line: 273
    Complexity: 1
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Run variational message passing (VMP) on a sequence of observations
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/jax/algos.py::update_marginals:
    name: update_marginals
    Module: None
    Signature: update_marginals(get_messages, obs, A, B, prior, A_dependencies, B_dependencies, num_iter, tau)
    function_exe_cmd: pymdp.jax.algos.update_marginals(get_messages, obs, A, B, prior, A_dependencies, B_dependencies, num_iter, tau)
    Parameters: ['get_messages', 'obs', 'A', 'B', 'prior', 'A_dependencies', 'B_dependencies', 'num_iter', 'tau']
    Returns: None
    Start Line: 106
    End Line: 148
    Complexity: 1
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: " Version of marginal update that uses a sparse dependency matrix for A
    Description: 
    Description Embedding: 
    Comments: ['log likelihoods -> $\\ln(A)$ for all time steps', 'for $k > t$ we have $\\ln(A) = 0$', '# mapping over batch dimension', 'return vmap(compute_log_likelihood_per_modality)(obs_t, A)', 'mapping over time dimension of obs array', 'this gives a sequence of log-likelihoods (one for each `t`)', 'log marginals -> $\\ln(q(s_t))$ for all time steps and factors', 'log prior -> $\\ln(p(s_t))$ for all factors', 'messages from future $m_+(s_t)$ and past $m_-(s_t)$ for all time steps and factors. For t = T we have that $m_+(s_T) = 0$']

  ../pymdp/jax/algos.py::update_variational_filtering:
    name: update_variational_filtering
    Module: None
    Signature: update_variational_filtering(obs, A, B, prior, A_dependencies)
    function_exe_cmd: pymdp.jax.algos.update_variational_filtering(obs, A, B, prior, A_dependencies)
    Parameters: ['obs', 'A', 'B', 'prior', 'A_dependencies']
    Returns: None
    Start Line: 179
    End Line: 210
    Complexity: 1
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Online variational filtering belief update that uses a sparse dependency matrix for A
    Description: 
    Description Embedding: 
    Comments: ['mapping over batch dimension', 'mapping over time dimension of obs array', 'this gives a sequence of log-likelihoods (one for each `t`)', 'get q_T(s_t), p_T(s_{t+1}) and the history q_{T}(s_{t}|s_{t+1})q_{T-1}(s_{t-1}|s_{t}) ...']

  ../pymdp/jax/algos.py::variational_filtering_step:
    name: variational_filtering_step
    Module: None
    Signature: variational_filtering_step(prior, Bs, ln_As, A_dependencies)
    function_exe_cmd: pymdp.jax.algos.variational_filtering_step(prior, Bs, ln_As, A_dependencies)
    Parameters: ['prior', 'Bs', 'ln_As', 'A_dependencies']
    Returns: None
    Start Line: 150
    End Line: 177
    Complexity: 1
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: None
    Description: 
    Description Embedding: 
    Comments: ['TODO: put this inside scan', '###', 'compute posterior q(z_t) -> n x 1 x d', '###', 'compute prediction p(z_{t+1}) = \\int p(z_{t+1}|z_t) q(z_t) -> n x d x 1', 'compute reverse conditional distribution q(z_t|z_{t+1})']

  ../pymdp/jax/control.py::calc_inductive_value_t:
    name: calc_inductive_value_t
    Module: None
    Signature: calc_inductive_value_t(qs, qs_next, I, epsilon)
    function_exe_cmd: pymdp.jax.control.calc_inductive_value_t(qs, qs_next, I, epsilon)
    Parameters: ['qs', 'qs_next', 'I', 'epsilon']
    Returns: None
    Start Line: 419
    End Line: 457
    Complexity: 2
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Computes the inductive value of a state at a particular time (translation of @tverbele's `numpy` implementation of inductive planning, formerly
    called `calc_inductive_cost`).

    Parameters
    ----------
    qs: ``list`` of ``jax.numpy.ndarray`` 
        Marginal posterior beliefs over hidden states at a given timepoint.
    qs_next: ```list`` of ``jax.numpy.ndarray`` 
        Predictive posterior beliefs over hidden states expected under the policy.
    I: ``numpy.ndarray`` of dtype object
        For each state factor, contains a 2D ``numpy.ndarray`` whose element i,j yields the probability 
        of reaching the goal state backwards from state j after i steps.
    epsilon: ``float``
        Value that tunes the strength of the inductive value (how much it contributes to the expected free energy of policies)

    Returns
    -------
    inductive_val: float
        Value (negative inductive cost) of visiting this state using backwards induction under the policy in question
    Description: 
    Description Embedding: 
    Comments: ['initialise inductive value', 'we also assume precise beliefs here?!', 'm = arg max_n p_n < sup p', 'i.e. find first entry at which I_idx equals 1, and then m is the index before that', "if there are any 1's at all in that column of I, then this == 1, otherwise 0", 'scaling by path_available will nullify the addition of inductive value in the case we find no path to goal (i.e. when no goal specified)']

  ../pymdp/jax/control.py::calc_pA_info_gain:
    name: calc_pA_info_gain
    Module: None
    Signature: calc_pA_info_gain(pA, qo, qs, A_dependencies)
    function_exe_cmd: pymdp.jax.control.calc_pA_info_gain(pA, qo, qs, A_dependencies)
    Parameters: ['pA', 'qo', 'qs', 'A_dependencies']
    Returns: None
    Start Line: 229
    End Line: 260
    Complexity: 1
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Compute expected Dirichlet information gain about parameters ``pA`` for a given posterior predictive distribution over observations ``qo`` and states ``qs``.

    Parameters
    ----------
    pA: ``numpy.ndarray`` of dtype object
        Dirichlet parameters over observation model (same shape as ``A``)
    qo: ``list`` of ``numpy.ndarray`` of dtype object
        Predictive posterior beliefs over observations; stores the beliefs about
        observations expected under the policy at some arbitrary time ``t``
    qs: ``list`` of ``numpy.ndarray`` of dtype object
        Predictive posterior beliefs over hidden states, stores the beliefs about
        hidden states expected under the policy at some arbitrary time ``t``

    Returns
    -------
    infogain_pA: float
        Surprise (about Dirichlet parameters) expected for the pair of posterior predictive distributions ``qo`` and ``qs``
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/jax/control.py::calc_pB_info_gain:
    name: calc_pB_info_gain
    Module: None
    Signature: calc_pB_info_gain(pB, qs_t, qs_t_minus_1, B_dependencies, u_t_minus_1)
    function_exe_cmd: pymdp.jax.control.calc_pB_info_gain(pB, qs_t, qs_t_minus_1, B_dependencies, u_t_minus_1)
    Parameters: ['pB', 'qs_t', 'qs_t_minus_1', 'B_dependencies', 'u_t_minus_1']
    Returns: None
    Start Line: 262
    End Line: 288
    Complexity: 1
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Compute expected Dirichlet information gain about parameters ``pB`` under a given policy

    Parameters
    ----------
    pB: ``Array`` of dtype object
        Dirichlet parameters over transition model (same shape as ``B``)
    qs_t: ``list`` of ``Array`` of dtype object
        Predictive posterior beliefs over hidden states expected under the policy at time ``t``
    qs_t_minus_1: ``list`` of ``Array`` of dtype object
        Posterior over hidden states at time ``t-1`` (before receiving observations)
    u_t_minus_1: "Array"
        Actions in time step t-1 for each factor

    Returns
    -------
    infogain_pB: float
        Surprise (about Dirichlet parameters) expected under the policy in question
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/jax/control.py::compute_G_policy:
    name: compute_G_policy
    Module: None
    Signature: compute_G_policy(qs_init, A, B, C, pA, pB, A_dependencies, B_dependencies, policy_i, use_utility, use_states_info_gain, use_param_info_gain)
    function_exe_cmd: pymdp.jax.control.compute_G_policy(qs_init, A, B, C, pA, pB, A_dependencies, B_dependencies, policy_i, use_utility, use_states_info_gain, use_param_info_gain)
    Parameters: ['qs_init', 'A', 'B', 'C', 'pA', 'pB', 'A_dependencies', 'B_dependencies', 'policy_i', 'use_utility', 'use_states_info_gain', 'use_param_info_gain']
    Returns: None
    Start Line: 290
    End Line: 316
    Complexity: 1
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Write a version of compute_G_policy that does the same computations as `compute_G_policy` but using `lax.scan` instead of a for loop.
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/jax/control.py::compute_G_policy_inductive:
    name: compute_G_policy_inductive
    Module: None
    Signature: compute_G_policy_inductive(qs_init, A, B, C, pA, pB, A_dependencies, B_dependencies, I, policy_i, inductive_epsilon, use_utility, use_states_info_gain, use_param_info_gain, use_inductive)
    function_exe_cmd: pymdp.jax.control.compute_G_policy_inductive(qs_init, A, B, C, pA, pB, A_dependencies, B_dependencies, I, policy_i, inductive_epsilon, use_utility, use_states_info_gain, use_param_info_gain, use_inductive)
    Parameters: ['qs_init', 'A', 'B', 'C', 'pA', 'pB', 'A_dependencies', 'B_dependencies', 'I', 'policy_i', 'inductive_epsilon', 'use_utility', 'use_states_info_gain', 'use_param_info_gain', 'use_inductive']
    Returns: None
    Start Line: 318
    End Line: 352
    Complexity: 3
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Write a version of compute_G_policy that does the same computations as `compute_G_policy` but using `lax.scan` instead of a for loop.
    This one further adds computations used for inductive planning.
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/jax/control.py::compute_expected_obs:
    name: compute_expected_obs
    Module: None
    Signature: compute_expected_obs(qs, A, A_dependencies)
    function_exe_cmd: pymdp.jax.control.compute_expected_obs(qs, A, A_dependencies)
    Parameters: ['qs', 'A', 'A_dependencies']
    Returns: None
    Start Line: 188
    End Line: 199
    Complexity: 1
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: New version of expected observation (computation of Q(o|pi)) that takes into account sparse dependencies between observation
    modalities and hidden state factors
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/jax/control.py::compute_expected_state:
    name: compute_expected_state
    Module: None
    Signature: compute_expected_state(qs_prior, B, u_t, B_dependencies)
    function_exe_cmd: pymdp.jax.control.compute_expected_state(qs_prior, B, u_t, B_dependencies)
    Parameters: ['qs_prior', 'B', 'u_t', 'B_dependencies']
    Returns: None
    Start Line: 159
    End Line: 173
    Complexity: 2
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Compute posterior over next state, given belief about previous state, transition model and action...
    Description: 
    Description Embedding: 
    Comments: ['Note: this algorithm is only correct if each factor depends only on itself. For any interactions,', 'we will have empirical priors with codependent factors.', "P(s'|s, u) = \\sum_{s, u} P(s'|s) P(s|u) P(u|pi)P(pi) because u </-> pi"]

  ../pymdp/jax/control.py::compute_expected_state_and_Bs:
    name: compute_expected_state_and_Bs
    Module: None
    Signature: compute_expected_state_and_Bs(qs_prior, B, u_t)
    function_exe_cmd: pymdp.jax.control.compute_expected_state_and_Bs(qs_prior, B, u_t)
    Parameters: ['qs_prior', 'B', 'u_t']
    Returns: None
    Start Line: 175
    End Line: 186
    Complexity: 2
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Compute posterior over next state, given belief about previous state, transition model and action...
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/jax/control.py::compute_expected_utility:
    name: compute_expected_utility
    Module: None
    Signature: compute_expected_utility(t, qo, C)
    function_exe_cmd: pymdp.jax.control.compute_expected_utility(t, qo, C)
    Parameters: ['t', 'qo', 'C']
    Returns: None
    Start Line: 218
    End Line: 227
    Complexity: 3
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: None
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/jax/control.py::compute_info_gain:
    name: compute_info_gain
    Module: None
    Signature: compute_info_gain(qs, qo, A, A_dependencies)
    function_exe_cmd: pymdp.jax.control.compute_info_gain(qs, qo, A, A_dependencies)
    Parameters: ['qs', 'qo', 'A', 'A_dependencies']
    Returns: None
    Start Line: 201
    End Line: 216
    Complexity: 1
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: New version of expected information gain that takes into account sparse dependencies between observation modalities and hidden state factors.
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/jax/control.py::construct_policies:
    name: construct_policies
    Module: None
    Signature: construct_policies(num_states, num_controls, policy_len, control_fac_idx)
    function_exe_cmd: pymdp.jax.control.construct_policies(num_states, num_controls, policy_len, control_fac_idx)
    Parameters: ['num_states', 'num_controls', 'policy_len', 'control_fac_idx']
    Returns: None
    Start Line: 99
    End Line: 140
    Complexity: 5
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Generate a ``list`` of policies. The returned array ``policies`` is a ``list`` that stores one policy per entry.
    A particular policy (``policies[i]``) has shape ``(num_timesteps, num_factors)`` 
    where ``num_timesteps`` is the temporal depth of the policy and ``num_factors`` is the number of control factors.

    Parameters
    ----------
    num_states: ``list`` of ``int``
        ``list`` of the dimensionalities of each hidden state factor
    num_controls: ``list`` of ``int``, default ``None``
        ``list`` of the dimensionalities of each control state factor. If ``None``, then is automatically computed as the dimensionality of each hidden state factor that is controllable
    policy_len: ``int``, default 1
        temporal depth ("planning horizon") of policies
    control_fac_idx: ``list`` of ``int``
        ``list`` of indices of the hidden state factors that are controllable (i.e. those state factors ``i`` where ``num_controls[i] > 1``)

    Returns
    ----------
    policies: ``list`` of 2D ``numpy.ndarray``
        ``list`` that stores each policy as a 2D array in ``policies[p_idx]``. Shape of ``policies[p_idx]`` 
        is ``(num_timesteps, num_factors)`` where ``num_timesteps`` is the temporal
        depth of the policy and ``num_factors`` is the number of control factors.
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/jax/control.py::generate_I_matrix:
    name: generate_I_matrix
    Module: None
    Signature: generate_I_matrix(H, B, threshold, depth)
    function_exe_cmd: pymdp.jax.control.generate_I_matrix(H, B, threshold, depth)
    Parameters: ['H', 'B', 'threshold', 'depth']
    Returns: None
    Start Line: 370
    End Line: 417
    Complexity: 2
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Generates the `I` matrices used in inductive planning. These matrices stores the probability of reaching the goal state backwards from state j (columns) after i (rows) steps.
    Parameters
    ----------    
    H: ``list`` of ``jax.numpy.ndarray``
        Constraints over desired states (1 if you want to reach that state, 0 otherwise)
    B: ``list`` of ``jax.numpy.ndarray``
        Dynamics likelihood mapping or 'transition model', mapping from hidden states at ``t`` to hidden states at ``t+1``, given some control state ``u``.
        Each element ``B[f]`` of this object array stores a 3-D tensor for hidden state factor ``f``, whose entries ``B[f][s, v, u]`` store the probability
        of hidden state level ``s`` at the current time, given hidden state level ``v`` and action ``u`` at the previous time.
    threshold: ``float``
        The threshold for pruning transitions that are below a certain probability
    depth: ``int``
        The temporal depth of the backward induction

    Returns
    ----------
    I: ``numpy.ndarray`` of dtype object
        For each state factor, contains a 2D ``numpy.ndarray`` whose element i,j yields the probability 
        of reaching the goal state backwards from state j after i steps.
    Description: 
    Description Embedding: 
    Comments: ['If there exists an action that allows transitioning', 'from state to next_state, with probability larger than threshold', 'set b_reachable[current_state, previous_state] to 1', "clamp I_next to 1.0 if it's above 0.1, 0 otherwise"]

  ../pymdp/jax/control.py::get_marginals:
    name: get_marginals
    Module: None
    Signature: get_marginals(q_pi, policies, num_controls)
    function_exe_cmd: pymdp.jax.control.get_marginals(q_pi, policies, num_controls)
    Parameters: ['q_pi', 'policies', 'num_controls']
    Returns: None
    Start Line: 20
    End Line: 47
    Complexity: 2
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Computes the marginal posterior(s) over actions by integrating their posterior probability under the policies that they appear within.

    Parameters
    ----------
    q_pi: 1D ``numpy.ndarray``
        Posterior beliefs over policies, i.e. a vector containing one posterior probability per policy.
    policies: ``list`` of 2D ``numpy.ndarray``
        ``list`` that stores each policy as a 2D array in ``policies[p_idx]``. Shape of ``policies[p_idx]`` 
        is ``(num_timesteps, num_factors)`` where ``num_timesteps`` is the temporal
        depth of the policy and ``num_factors`` is the number of control factors.
    num_controls: ``list`` of ``int``
        ``list`` of the dimensionalities of each control state factor.

    Returns
    ----------
    action_marginals: ``list`` of ``jax.numpy.ndarrays``
       List of arrays corresponding to marginal probability of each action possible action
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/jax/control.py::sample_action:
    name: sample_action
    Module: None
    Signature: sample_action(policies, num_controls, q_pi, action_selection, alpha, rng_key)
    function_exe_cmd: pymdp.jax.control.sample_action(policies, num_controls, q_pi, action_selection, alpha, rng_key)
    Parameters: ['policies', 'num_controls', 'q_pi', 'action_selection', 'alpha', 'rng_key']
    Returns: None
    Start Line: 49
    End Line: 86
    Complexity: 3
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Samples an action from posterior marginals, one action per control factor.

    Parameters
    ----------
    q_pi: 1D ``numpy.ndarray``
        Posterior beliefs over policies, i.e. a vector containing one posterior probability per policy.
    policies: ``list`` of 2D ``numpy.ndarray``
        ``list`` that stores each policy as a 2D array in ``policies[p_idx]``. Shape of ``policies[p_idx]`` 
        is ``(num_timesteps, num_factors)`` where ``num_timesteps`` is the temporal
        depth of the policy and ``num_factors`` is the number of control factors.
    num_controls: ``list`` of ``int``
        ``list`` of the dimensionalities of each control state factor.
    action_selection: string, default "deterministic"
        String indicating whether whether the selected action is chosen as the maximum of the posterior over actions,
        or whether it's sampled from the posterior marginal over actions
    alpha: float, default 16.0
        Action selection precision -- the inverse temperature of the softmax that is used to scale the 
        action marginals before sampling. This is only used if ``action_selection`` argument is "stochastic"

    Returns
    ----------
    selected_policy: 1D ``numpy.ndarray``
        Vector containing the indices of the actions for each control factor
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/jax/control.py::sample_policy:
    name: sample_policy
    Module: None
    Signature: sample_policy(policies, q_pi, action_selection, alpha, rng_key)
    function_exe_cmd: pymdp.jax.control.sample_policy(policies, q_pi, action_selection, alpha, rng_key)
    Parameters: ['policies', 'q_pi', 'action_selection', 'alpha', 'rng_key']
    Returns: None
    Start Line: 88
    End Line: 97
    Complexity: 3
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: None
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/jax/control.py::update_posterior_policies:
    name: update_posterior_policies
    Module: None
    Signature: update_posterior_policies(policy_matrix, qs_init, A, B, C, E, pA, pB, A_dependencies, B_dependencies, gamma, use_utility, use_states_info_gain, use_param_info_gain)
    function_exe_cmd: pymdp.jax.control.update_posterior_policies(policy_matrix, qs_init, A, B, C, E, pA, pB, A_dependencies, B_dependencies, gamma, use_utility, use_states_info_gain, use_param_info_gain)
    Parameters: ['policy_matrix', 'qs_init', 'A', 'B', 'C', 'E', 'pA', 'pB', 'A_dependencies', 'B_dependencies', 'gamma', 'use_utility', 'use_states_info_gain', 'use_param_info_gain']
    Returns: None
    Start Line: 143
    End Line: 157
    Complexity: 1
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: None
    Description: 
    Description Embedding: 
    Comments: ['policy --> n_levels_factor_f x 1', 'factor --> n_levels_factor_f x n_policies', '# vmap across policies', 'only in the case of policy-dependent qs_inits', 'in_axes_list = (1,) * n_factors', 'all_efe_of_policies = vmap(compute_G_policy, in_axes=(in_axes_list, 0))(qs_init_pi, policy_matrix)', 'policies needs to be an NDarray of shape (n_policies, n_timepoints, n_control_factors)']

  ../pymdp/jax/control.py::update_posterior_policies_inductive:
    name: update_posterior_policies_inductive
    Module: None
    Signature: update_posterior_policies_inductive(policy_matrix, qs_init, A, B, C, E, pA, pB, A_dependencies, B_dependencies, I, gamma, inductive_epsilon, use_utility, use_states_info_gain, use_param_info_gain, use_inductive)
    function_exe_cmd: pymdp.jax.control.update_posterior_policies_inductive(policy_matrix, qs_init, A, B, C, E, pA, pB, A_dependencies, B_dependencies, I, gamma, inductive_epsilon, use_utility, use_states_info_gain, use_param_info_gain, use_inductive)
    Parameters: ['policy_matrix', 'qs_init', 'A', 'B', 'C', 'E', 'pA', 'pB', 'A_dependencies', 'B_dependencies', 'I', 'gamma', 'inductive_epsilon', 'use_utility', 'use_states_info_gain', 'use_param_info_gain', 'use_inductive']
    Returns: None
    Start Line: 354
    End Line: 368
    Complexity: 1
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: None
    Description: 
    Description Embedding: 
    Comments: ['policy --> n_levels_factor_f x 1', 'factor --> n_levels_factor_f x n_policies', '# vmap across policies', 'only in the case of policy-dependent qs_inits', 'in_axes_list = (1,) * n_factors', 'all_efe_of_policies = vmap(compute_G_policy, in_axes=(in_axes_list, 0))(qs_init_pi, policy_matrix)', 'policies needs to be an NDarray of shape (n_policies, n_timepoints, n_control_factors)']

  ../pymdp/jax/inference.py::joint_dist_factor:
    name: joint_dist_factor
    Module: None
    Signature: joint_dist_factor(b, filtered_qs, actions)
    function_exe_cmd: pymdp.jax.inference.joint_dist_factor(b, filtered_qs, actions)
    Parameters: ['b', 'filtered_qs', 'actions']
    Returns: None
    Start Line: 64
    End Line: 100
    Complexity: 4
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: None
    Description: 
    Description Embedding: 
    Comments: ['returns q(s_t), (q(s_t), q(s_t, s_t+1))', 'seq_qs will contain a sequence of smoothed marginals and joints', 'we add the last filtered belief to smoothed beliefs']

  ../pymdp/jax/inference.py::smoothing_ovf:
    name: smoothing_ovf
    Module: None
    Signature: smoothing_ovf(filtered_post, B, past_actions)
    function_exe_cmd: pymdp.jax.inference.smoothing_ovf(filtered_post, B, past_actions)
    Parameters: ['filtered_post', 'B', 'past_actions']
    Returns: None
    Start Line: 103
    End Line: 115
    Complexity: 2
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: None
    Description: 
    Description Embedding: 
    Comments: ['number of factors']

  ../pymdp/jax/inference.py::update_posterior_states:
    name: update_posterior_states
    Module: None
    Signature: update_posterior_states(A, B, obs, past_actions, prior, qs_hist, A_dependencies, B_dependencies, num_iter, method)
    function_exe_cmd: pymdp.jax.inference.update_posterior_states(A, B, obs, past_actions, prior, qs_hist, A_dependencies, B_dependencies, num_iter, method)
    Parameters: ['A', 'B', 'obs', 'past_actions', 'prior', 'qs_hist', 'A_dependencies', 'B_dependencies', 'num_iter', 'method']
    Returns: None
    Start Line: 14
    End Line: 62
    Complexity: 14
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: None
    Description: 
    Description Embedding: 
    Comments: ['format obs to select only last observation', 'format B matrices using action sequences here', 'TODO: past_actions can be None', 'move time steps to the leading axis (leftmost)', 'this assumes that a policy is always specified as the rightmost axis of Bs', 'outputs of both VMP and MMP should be a list of hidden state factors, where each qs[f].shape = (T, batch_dim, num_states_f)', 'TODO: return entire history of beliefs']

  ../pymdp/jax/learning.py::update_obs_likelihood_dirichlet:
    name: update_obs_likelihood_dirichlet
    Module: None
    Signature: update_obs_likelihood_dirichlet(pA, A, obs, qs)
    function_exe_cmd: pymdp.jax.learning.update_obs_likelihood_dirichlet(pA, A, obs, qs)
    Parameters: ['pA', 'A', 'obs', 'qs']
    Returns: None
    Start Line: 33
    End Line: 57
    Complexity: 3
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: JAX version of ``pymdp.learning.update_obs_likelihood_dirichlet``
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/jax/learning.py::update_obs_likelihood_dirichlet_m:
    name: update_obs_likelihood_dirichlet_m
    Module: None
    Signature: update_obs_likelihood_dirichlet_m(pA_m, obs_m, qs, dependencies_m, lr)
    function_exe_cmd: pymdp.jax.learning.update_obs_likelihood_dirichlet_m(pA_m, obs_m, qs, dependencies_m, lr)
    Parameters: ['pA_m', 'obs_m', 'qs', 'dependencies_m', 'lr']
    Returns: None
    Start Line: 10
    End Line: 31
    Complexity: 1
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: JAX version of ``pymdp.learning.update_obs_likelihood_dirichlet_m``
    Description: 
    Description Embedding: 
    Comments: ['pA_m - parameters of the dirichlet from the prior', 'pA_m.shape = (no_m x num_states[k] x num_states[j] x ... x num_states[n]) where (k, j, n) are indices of the hidden state factors that are parents of modality m', '\\alpha^{*} = \\alpha_{0} + \\kappa * \\sum_{t=t_begin}^{t=T} o_{m,t} \\otimes \\mathbf{s}_{f \\in parents(m), t}', '\\alpha^{*} is the VFE-minimizing solution for the parameters of q(A)', '\\alpha_{0} are the Dirichlet parameters of p(A)', 'o_{m,t} = observation (one-hot vector) of modality m at time t', '\\mathbf{s}_{f \\in parents(m), t} = categorical parameters of marginal posteriors over hidden state factors that are parents of modality m, at time t', '\\otimes is a multidimensional outer product, not just a outer product of two vectors', '\\kappa is an optional learning rate']

  ../pymdp/jax/learning.py::update_state_transition_dirichlet:
    name: update_state_transition_dirichlet
    Module: None
    Signature: update_state_transition_dirichlet(pB, B, joint_beliefs, actions)
    function_exe_cmd: pymdp.jax.learning.update_state_transition_dirichlet(pB, B, joint_beliefs, actions)
    Parameters: ['pB', 'B', 'joint_beliefs', 'actions']
    Returns: None
    Start Line: 79
    End Line: 102
    Complexity: 3
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: None
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/jax/learning.py::update_state_transition_dirichlet_f:
    name: update_state_transition_dirichlet_f
    Module: None
    Signature: update_state_transition_dirichlet_f(pB_f, actions_f, joint_qs_f, lr)
    function_exe_cmd: pymdp.jax.learning.update_state_transition_dirichlet_f(pB_f, actions_f, joint_qs_f, lr)
    Parameters: ['pB_f', 'actions_f', 'joint_qs_f', 'lr']
    Returns: None
    Start Line: 59
    End Line: 77
    Complexity: 1
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: JAX version of ``pymdp.learning.update_state_likelihood_dirichlet_f``
    Description: 
    Description Embedding: 
    Comments: ['pB_f - parameters of the dirichlet from the prior', 'pB_f.shape = (num_states[f] x num_states[f] x num_actions[f]) where f is the index of the hidden state factor', '\\alpha^{*} = \\alpha_{0} + \\kappa * \\sum_{t=t_begin}^{t=T} \\mathbf{s}_{f, t} \\otimes \\mathbf{s}_{f, t-1} \\otimes \\mathbf{a}_{f, t-1}', '\\alpha^{*} is the VFE-minimizing solution for the parameters of q(B)', '\\alpha_{0} are the Dirichlet parameters of p(B)', '\\mathbf{s}_{f, t} = categorical parameters of marginal posteriors over hidden state factor f, at time t', '\\mathbf{a}_{f, t-1} = categorical parameters of marginal posteriors over control factor f, at time t-1', '\\otimes is a multidimensional outer product, not just a outer product of two vectors', '\\kappa is an optional learning rate']

  ../pymdp/jax/likelihoods.py::aif_likelihood:
    name: aif_likelihood
    Module: None
    Signature: aif_likelihood(Nb, Nt, Na, data, agent)
    function_exe_cmd: pymdp.jax.likelihoods.aif_likelihood(Nb, Nt, Na, data, agent)
    Parameters: ['Nb', 'Nt', 'Na', 'data', 'agent']
    Returns: None
    Start Line: 27
    End Line: 45
    Complexity: 3
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: None
    Description: 
    Description Embedding: 
    Comments: ['Na -> batch dimension - number of different subjects/agents', 'Nb -> number of experimental blocks', 'Nt -> number of trials within each block', 'TODO: See if some information has to be passed from one block to the next and change init and carry accordingly']

  ../pymdp/jax/likelihoods.py::evolve_trials:
    name: evolve_trials
    Module: None
    Signature: evolve_trials(agent, data)
    function_exe_cmd: pymdp.jax.likelihoods.evolve_trials(agent, data)
    Parameters: ['agent', 'data']
    Returns: None
    Start Line: 7
    End Line: 25
    Complexity: 1
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: None
    Description: 
    Description Embedding: 
    Comments: ['TODO: if outcomes and actions are None, generate samples']

  ../pymdp/jax/maths.py::compute_accuracy:
    name: compute_accuracy
    Module: None
    Signature: compute_accuracy(qs, obs, A)
    function_exe_cmd: pymdp.jax.maths.compute_accuracy(qs, obs, A)
    Parameters: ['qs', 'obs', 'A']
    Returns: None
    Start Line: 91
    End Line: 101
    Complexity: 2
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Compute the accuracy portion of the variational free energy (expected log likelihood under the variational posterior)
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/jax/maths.py::compute_free_energy:
    name: compute_free_energy
    Module: None
    Signature: compute_free_energy(qs, prior, obs, A)
    function_exe_cmd: pymdp.jax.maths.compute_free_energy(qs, prior, obs, A)
    Parameters: ['qs', 'prior', 'obs', 'A']
    Returns: None
    Start Line: 103
    End Line: 121
    Complexity: 2
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Calculate variational free energy by breaking its computation down into three steps:
    1. computation of the negative entropy of the posterior -H[Q(s)]
    2. computation of the cross entropy of the posterior with the prior H_{Q(s)}[P(s)]
    3. computation of the accuracy E_{Q(s)}[lnP(o|s)] 

    Then add them all together -- except subtract the accuracy
    Description: 
    Description Embedding: 
    Comments: ['initialize variational free energy']

  ../pymdp/jax/maths.py::compute_log_likelihood:
    name: compute_log_likelihood
    Module: None
    Signature: compute_log_likelihood(obs, A, distr_obs)
    function_exe_cmd: pymdp.jax.maths.compute_log_likelihood(obs, A, distr_obs)
    Parameters: ['obs', 'A', 'distr_obs']
    Returns: None
    Start Line: 78
    End Line: 83
    Complexity: 1
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Compute likelihood over hidden states across observations from different modalities
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/jax/maths.py::compute_log_likelihood_per_modality:
    name: compute_log_likelihood_per_modality
    Module: None
    Signature: compute_log_likelihood_per_modality(obs, A, distr_obs)
    function_exe_cmd: pymdp.jax.maths.compute_log_likelihood_per_modality(obs, A, distr_obs)
    Parameters: ['obs', 'A', 'distr_obs']
    Returns: None
    Start Line: 85
    End Line: 89
    Complexity: 1
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Compute likelihood over hidden states across observations from different modalities, and return them per modality
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/jax/maths.py::compute_log_likelihood_single_modality:
    name: compute_log_likelihood_single_modality
    Module: None
    Signature: compute_log_likelihood_single_modality(o_m, A_m, distr_obs)
    function_exe_cmd: pymdp.jax.maths.compute_log_likelihood_single_modality(o_m, A_m, distr_obs)
    Parameters: ['o_m', 'A_m', 'distr_obs']
    Returns: None
    Start Line: 74
    End Line: 76
    Complexity: 1
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Compute observation log-likelihood for a single modality
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/jax/maths.py::dirichlet_expected_value:
    name: dirichlet_expected_value
    Module: None
    Signature: dirichlet_expected_value(dir_arr)
    function_exe_cmd: pymdp.jax.maths.dirichlet_expected_value(dir_arr)
    Parameters: ['dir_arr']
    Returns: None
    Start Line: 142
    End Line: 149
    Complexity: 1
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Returns Expectation of Dirichlet parameters over a set of 
    Categorical distributions, stored in the columns of A.
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/jax/maths.py::factor_dot:
    name: factor_dot
    Module: None
    Signature: factor_dot(M, xs, keep_dims)
    function_exe_cmd: pymdp.jax.maths.factor_dot(M, xs, keep_dims)
    Parameters: ['M', 'xs', 'keep_dims']
    Returns: None
    Start Line: 23
    End Line: 39
    Complexity: 1
    Is Method: False
    Is Async: False
    Decorators: partial(jit, static_argnames=['keep_dims'])
    Docstring: Dot product of a multidimensional array with `x`.

    Parameters
    ----------
    - `qs` [list of 1D numpy.ndarray] - list of jnp.ndarrays

    Returns 
    -------
    - `Y` [1D numpy.ndarray] - the result of the dot product
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/jax/maths.py::factor_dot_flex:
    name: factor_dot_flex
    Module: None
    Signature: factor_dot_flex(M, xs, dims, keep_dims)
    function_exe_cmd: pymdp.jax.maths.factor_dot_flex(M, xs, dims, keep_dims)
    Parameters: ['M', 'xs', 'dims', 'keep_dims']
    Returns: None
    Start Line: 41
    End Line: 62
    Complexity: 2
    Is Method: False
    Is Async: False
    Decorators: partial(jit, static_argnames=['dims', 'keep_dims'])
    Docstring: Dot product of a multidimensional array with `x`.

    Parameters
    ----------
    - `M` [numpy.ndarray] - tensor
    - 'xs' [list of numpyr.ndarray] - list of tensors
    - 'dims' [list of tuples] - list of dimensions of xs tensors in tensor M
    - 'keep_dims' [tuple] - tuple of integers denoting dimesions to keep
    Returns 
    -------
    - `Y` [1D numpy.ndarray] - the result of the dot product
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/jax/maths.py::get_likelihood_single_modality:
    name: get_likelihood_single_modality
    Module: None
    Signature: get_likelihood_single_modality(o_m, A_m, distr_obs)
    function_exe_cmd: pymdp.jax.maths.get_likelihood_single_modality(o_m, A_m, distr_obs)
    Parameters: ['o_m', 'A_m', 'distr_obs']
    Returns: None
    Start Line: 64
    End Line: 72
    Complexity: 2
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Return observation likelihood for a single observation modality m
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/jax/maths.py::log_stable:
    name: log_stable
    Module: None
    Signature: log_stable(x)
    function_exe_cmd: pymdp.jax.maths.log_stable(x)
    Parameters: ['x']
    Returns: None
    Start Line: 20
    End Line: 21
    Complexity: 1
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: None
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/jax/maths.py::multidimensional_outer:
    name: multidimensional_outer
    Module: None
    Signature: multidimensional_outer(arrs)
    function_exe_cmd: pymdp.jax.maths.multidimensional_outer(arrs)
    Parameters: ['arrs']
    Returns: None
    Start Line: 123
    End Line: 130
    Complexity: 2
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Compute the outer product of a list of arrays by iteratively expanding the first array and multiplying it with the next array
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/jax/maths.py::spm_wnorm:
    name: spm_wnorm
    Module: None
    Signature: spm_wnorm(A)
    function_exe_cmd: pymdp.jax.maths.spm_wnorm(A)
    Parameters: ['A']
    Returns: None
    Start Line: 132
    End Line: 140
    Complexity: 1
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Returns Expectation of logarithm of Dirichlet parameters over a set of 
    Categorical distributions, stored in the columns of A.
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/jax/maths.py::stable_cross_entropy:
    name: stable_cross_entropy
    Module: None
    Signature: stable_cross_entropy(x, y)
    function_exe_cmd: pymdp.jax.maths.stable_cross_entropy(x, y)
    Parameters: ['x', 'y']
    Returns: None
    Start Line: 17
    End Line: 18
    Complexity: 1
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: None
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/jax/maths.py::stable_entropy:
    name: stable_entropy
    Module: None
    Signature: stable_entropy(x)
    function_exe_cmd: pymdp.jax.maths.stable_entropy(x)
    Parameters: ['x']
    Returns: None
    Start Line: 14
    End Line: 15
    Complexity: 1
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: None
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/jax/maths.py::stable_xlogx:
    name: stable_xlogx
    Module: None
    Signature: stable_xlogx(x)
    function_exe_cmd: pymdp.jax.maths.stable_xlogx(x)
    Parameters: ['x']
    Returns: None
    Start Line: 11
    End Line: 12
    Complexity: 1
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: None
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/jax/task.py::cat_sample:
    name: cat_sample
    Module: None
    Signature: cat_sample(key, p)
    function_exe_cmd: pymdp.jax.task.cat_sample(key, p)
    Parameters: ['key', 'p']
    Returns: None
    Start Line: 16
    End Line: 23
    Complexity: 2
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: None
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/jax/task.py::reset:
    name: reset
    Module: None
    Signature: reset(self, key)
    function_exe_cmd: pymdp.jax.task.PyMDPEnv.reset(self, key)
    Parameters: ['self', 'key']
    Returns: None
    Start Line: 41
    End Line: 49
    Complexity: 2
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: None
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/jax/task.py::select_probs:
    name: select_probs
    Module: None
    Signature: select_probs(positions, matrix, dependency_list, actions)
    function_exe_cmd: pymdp.jax.task.select_probs(positions, matrix, dependency_list, actions)
    Parameters: ['positions', 'matrix', 'dependency_list', 'actions']
    Returns: None
    Start Line: 10
    End Line: 14
    Complexity: 1
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: None
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/jax/task.py::step:
    name: step
    Module: None
    Signature: step(self, key, actions)
    function_exe_cmd: pymdp.jax.task.PyMDPEnv.step(self, key, actions)
    Parameters: ['self', 'key', 'actions']
    Returns: None
    Start Line: 51
    End Line: 76
    Complexity: 2
    Is Method: True
    Is Async: False
    Decorators: vmap
    Docstring: None
    Description: 
    Description Embedding: 
    Comments: ['return a list of random observations and states']

  ../pymdp/jax/utils.py::list_array_scaled:
    name: list_array_scaled
    Module: None
    Signature: list_array_scaled(shape_list, scale)
    function_exe_cmd: pymdp.jax.utils.list_array_scaled(shape_list, scale)
    Parameters: ['shape_list', 'scale']
    Returns: Vector
    Start Line: 42
    End Line: 50
    Complexity: 2
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Creates a list of 1-D jax arrays filled with scale, with shapes given by shape_list[i]
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/jax/utils.py::list_array_uniform:
    name: list_array_uniform
    Module: None
    Signature: list_array_uniform(shape_list)
    function_exe_cmd: pymdp.jax.utils.list_array_uniform(shape_list)
    Parameters: ['shape_list']
    Returns: Vector
    Start Line: 22
    End Line: 31
    Complexity: 2
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Creates a list of jax arrays representing uniform Categorical
    distributions with shapes given by shape_list[i]. The shapes (elements of shape_list)
    can either be tuples or lists.
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/jax/utils.py::list_array_zeros:
    name: list_array_zeros
    Module: None
    Signature: list_array_zeros(shape_list)
    function_exe_cmd: pymdp.jax.utils.list_array_zeros(shape_list)
    Parameters: ['shape_list']
    Returns: Vector
    Start Line: 33
    End Line: 40
    Complexity: 2
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Creates a list of 1-D jax arrays filled with zeros, with shapes given by shape_list[i]
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/jax/utils.py::norm_dist:
    name: norm_dist
    Module: None
    Signature: norm_dist(dist)
    function_exe_cmd: pymdp.jax.utils.norm_dist(dist)
    Parameters: ['dist']
    Returns: Tensor
    Start Line: 18
    End Line: 20
    Complexity: 1
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Normalizes a Categorical probability distribution
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/learning.py::update_obs_likelihood_dirichlet:
    name: update_obs_likelihood_dirichlet
    Module: None
    Signature: update_obs_likelihood_dirichlet(pA, A, obs, qs, lr, modalities)
    function_exe_cmd: pymdp.learning.update_obs_likelihood_dirichlet(pA, A, obs, qs, lr, modalities)
    Parameters: ['pA', 'A', 'obs', 'qs', 'lr', 'modalities']
    Returns: None
    Start Line: 9
    End Line: 58
    Complexity: 3
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Update Dirichlet parameters of the observation likelihood distribution.

    Parameters
    -----------
    pA: ``numpy.ndarray`` of dtype object
        Prior Dirichlet parameters over observation model (same shape as ``A``)
    A: ``numpy.ndarray`` of dtype object
        Sensory likelihood mapping or 'observation model', mapping from hidden states to observations. Each element ``A[m]`` of
        stores an ``numpy.ndarray`` multidimensional array for observation modality ``m``, whose entries ``A[m][i, j, k, ...]`` store 
        the probability of observation level ``i`` given hidden state levels ``j, k, ...``
    obs: 1D ``numpy.ndarray``, ``numpy.ndarray`` of dtype object, ``int`` or ``tuple``
        The observation (generated by the environment). If single modality, this can be a 1D ``numpy.ndarray``
        (one-hot vector representation) or an ``int`` (observation index)
        If multi-modality, this can be ``numpy.ndarray`` of dtype object whose entries are 1D one-hot vectors,
        or a ``tuple`` (of ``int``)
    qs: 1D ``numpy.ndarray`` or ``numpy.ndarray`` of dtype object, default None
        Marginal posterior beliefs over hidden states at current timepoint.
    lr: float, default 1.0
        Learning rate, scale of the Dirichlet pseudo-count update.
    modalities: ``list``, default "all"
        Indices (ranging from 0 to ``n_modalities - 1``) of the observation modalities to include 
        in learning. Defaults to "all", meaning that modality-specific sub-arrays of ``pA``
        are all updated using the corresponding observations.

    Returns
    -----------
    qA: ``numpy.ndarray`` of dtype object
        Posterior Dirichlet parameters over observation model (same shape as ``A``), after having updated it with observations.
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/learning.py::update_obs_likelihood_dirichlet_factorized:
    name: update_obs_likelihood_dirichlet_factorized
    Module: None
    Signature: update_obs_likelihood_dirichlet_factorized(pA, A, obs, qs, A_factor_list, lr, modalities)
    function_exe_cmd: pymdp.learning.update_obs_likelihood_dirichlet_factorized(pA, A, obs, qs, A_factor_list, lr, modalities)
    Parameters: ['pA', 'A', 'obs', 'qs', 'A_factor_list', 'lr', 'modalities']
    Returns: None
    Start Line: 60
    End Line: 111
    Complexity: 3
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Update Dirichlet parameters of the observation likelihood distribution, in a case where the observation model is reduced (factorized) and only represents
    the conditional dependencies between the observation modalities and particular hidden state factors (whose indices are specified in each modality-specific entry of ``A_factor_list``)

    Parameters
    -----------
    pA: ``numpy.ndarray`` of dtype object
        Prior Dirichlet parameters over observation model (same shape as ``A``)
    A: ``numpy.ndarray`` of dtype object
        Sensory likelihood mapping or 'observation model', mapping from hidden states to observations. Each element ``A[m]`` of
        stores an ``numpy.ndarray`` multidimensional array for observation modality ``m``, whose entries ``A[m][i, j, k, ...]`` store 
        the probability of observation level ``i`` given hidden state levels ``j, k, ...``
    obs: 1D ``numpy.ndarray``, ``numpy.ndarray`` of dtype object, ``int`` or ``tuple``
        The observation (generated by the environment). If single modality, this can be a 1D ``numpy.ndarray``
        (one-hot vector representation) or an ``int`` (observation index)
        If multi-modality, this can be ``numpy.ndarray`` of dtype object whose entries are 1D one-hot vectors,
        or a ``tuple`` (of ``int``)
    qs: 1D ``numpy.ndarray`` or ``numpy.ndarray`` of dtype object, default None
        Marginal posterior beliefs over hidden states at current timepoint.
    A_factor_list: ``list`` of ``list`` of ``int``
        List of lists, where each list with index `m` contains the indices of the hidden states that observation modality `m` depends on.
    lr: float, default 1.0
        Learning rate, scale of the Dirichlet pseudo-count update.
    modalities: ``list``, default "all"
        Indices (ranging from 0 to ``n_modalities - 1``) of the observation modalities to include 
        in learning. Defaults to "all", meaning that modality-specific sub-arrays of ``pA``
        are all updated using the corresponding observations.

    Returns
    -----------
    qA: ``numpy.ndarray`` of dtype object
        Posterior Dirichlet parameters over observation model (same shape as ``A``), after having updated it with observations.
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/learning.py::update_state_likelihood_dirichlet:
    name: update_state_likelihood_dirichlet
    Module: None
    Signature: update_state_likelihood_dirichlet(pB, B, actions, qs, qs_prev, lr, factors)
    function_exe_cmd: pymdp.learning.update_state_likelihood_dirichlet(pB, B, actions, qs, qs_prev, lr, factors)
    Parameters: ['pB', 'B', 'actions', 'qs', 'qs_prev', 'lr', 'factors']
    Returns: None
    Start Line: 113
    End Line: 159
    Complexity: 3
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Update Dirichlet parameters of the transition distribution. 

    Parameters
    -----------
    pB: ``numpy.ndarray`` of dtype object
        Prior Dirichlet parameters over transition model (same shape as ``B``)
    B: ``numpy.ndarray`` of dtype object
        Dynamics likelihood mapping or 'transition model', mapping from hidden states at ``t`` to hidden states at ``t+1``, given some control state ``u``.
        Each element ``B[f]`` of this object array stores a 3-D tensor for hidden state factor ``f``, whose entries ``B[f][s, v, u]`` store the probability
        of hidden state level ``s`` at the current time, given hidden state level ``v`` and action ``u`` at the previous time.
    actions: 1D ``numpy.ndarray``
        A vector with length equal to the number of control factors, where each element contains the index of the action (for that control factor) performed at 
        a given timestep.
    qs: 1D ``numpy.ndarray`` or ``numpy.ndarray`` of dtype object
        Marginal posterior beliefs over hidden states at current timepoint.
    qs_prev: 1D ``numpy.ndarray`` or ``numpy.ndarray`` of dtype object
        Marginal posterior beliefs over hidden states at previous timepoint.
    lr: float, default ``1.0``
        Learning rate, scale of the Dirichlet pseudo-count update.
    factors: ``list``, default "all"
        Indices (ranging from 0 to ``n_factors - 1``) of the hidden state factors to include 
        in learning. Defaults to "all", meaning that factor-specific sub-arrays of ``pB``
        are all updated using the corresponding hidden state distributions and actions.

    Returns
    -----------
    qB: ``numpy.ndarray`` of dtype object
        Posterior Dirichlet parameters over transition model (same shape as ``B``), after having updated it with state beliefs and actions.
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/learning.py::update_state_likelihood_dirichlet_interactions:
    name: update_state_likelihood_dirichlet_interactions
    Module: None
    Signature: update_state_likelihood_dirichlet_interactions(pB, B, actions, qs, qs_prev, B_factor_list, lr, factors)
    function_exe_cmd: pymdp.learning.update_state_likelihood_dirichlet_interactions(pB, B, actions, qs, qs_prev, B_factor_list, lr, factors)
    Parameters: ['pB', 'B', 'actions', 'qs', 'qs_prev', 'B_factor_list', 'lr', 'factors']
    Returns: None
    Start Line: 161
    End Line: 210
    Complexity: 3
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Update Dirichlet parameters of the transition distribution, in the case when 'interacting' hidden state factors are present, i.e.
    the dynamics of a given hidden state factor `f` are no longer independent of the dynamics of other hidden state factors.

    Parameters
    -----------
    pB: ``numpy.ndarray`` of dtype object
        Prior Dirichlet parameters over transition model (same shape as ``B``)
    B: ``numpy.ndarray`` of dtype object
        Dynamics likelihood mapping or 'transition model', mapping from hidden states at ``t`` to hidden states at ``t+1``, given some control state ``u``.
        Each element ``B[f]`` of this object array stores a 3-D tensor for hidden state factor ``f``, whose entries ``B[f][s, v, u]`` store the probability
        of hidden state level ``s`` at the current time, given hidden state level ``v`` and action ``u`` at the previous time.
    actions: 1D ``numpy.ndarray``
        A vector with length equal to the number of control factors, where each element contains the index of the action (for that control factor) performed at 
        a given timestep.
    qs: 1D ``numpy.ndarray`` or ``numpy.ndarray`` of dtype object
        Marginal posterior beliefs over hidden states at current timepoint.
    qs_prev: 1D ``numpy.ndarray`` or ``numpy.ndarray`` of dtype object
        Marginal posterior beliefs over hidden states at previous timepoint.
    B_factor_list: ``list`` of ``list`` of ``int``
        A list of lists, where each element ``B_factor_list[f]`` is a list of indices of hidden state factors that that are needed to predict the dynamics of hidden state factor ``f``.
    lr: float, default ``1.0``
        Learning rate, scale of the Dirichlet pseudo-count update.
    factors: ``list``, default "all"
        Indices (ranging from 0 to ``n_factors - 1``) of the hidden state factors to include 
        in learning. Defaults to "all", meaning that factor-specific sub-arrays of ``pB``
        are all updated using the corresponding hidden state distributions and actions.

    Returns
    -----------
    qB: ``numpy.ndarray`` of dtype object
        Posterior Dirichlet parameters over transition model (same shape as ``B``), after having updated it with state beliefs and actions.
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/learning.py::update_state_prior_dirichlet:
    name: update_state_prior_dirichlet
    Module: None
    Signature: update_state_prior_dirichlet(pD, qs, lr, factors)
    function_exe_cmd: pymdp.learning.update_state_prior_dirichlet(pD, qs, lr, factors)
    Parameters: ['pD', 'qs', 'lr', 'factors']
    Returns: None
    Start Line: 212
    End Line: 249
    Complexity: 3
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Update Dirichlet parameters of the initial hidden state distribution 
    (prior beliefs about hidden states at the beginning of the inference window).

    Parameters
    -----------
    pD: ``numpy.ndarray`` of dtype object
        Prior Dirichlet parameters over initial hidden state prior (same shape as ``qs``)
    qs: 1D ``numpy.ndarray`` or ``numpy.ndarray`` of dtype object
        Marginal posterior beliefs over hidden states at current timepoint
    lr: float, default ``1.0``
        Learning rate, scale of the Dirichlet pseudo-count update.
    factors: ``list``, default "all"
        Indices (ranging from 0 to ``n_factors - 1``) of the hidden state factors to include 
        in learning. Defaults to "all", meaning that factor-specific sub-vectors of ``pD``
        are all updated using the corresponding hidden state distributions.

    Returns
    -----------
    qD: ``numpy.ndarray`` of dtype object
        Posterior Dirichlet parameters over initial hidden state prior (same shape as ``qs``), after having updated it with state beliefs.
    Description: 
    Description Embedding: 
    Comments: ['only update those state level indices that have some prior probability']

  ../pymdp/maths.py::calc_free_energy:
    name: calc_free_energy
    Module: None
    Signature: calc_free_energy(qs, prior, n_factors, likelihood)
    function_exe_cmd: pymdp.maths.calc_free_energy(qs, prior, n_factors, likelihood)
    Parameters: ['qs', 'prior', 'n_factors', 'likelihood']
    Returns: None
    Start Line: 396
    End Line: 410
    Complexity: 3
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Calculate variational free energy
    @TODO Primarily used in FPI algorithm, needs to be made general
    Description: 
    Description Embedding: 
    Comments: ['Neg-entropy of posterior marginal H(q[f])', 'Cross entropy of posterior marginal with prior marginal H(q[f],p[f])']

  ../pymdp/maths.py::compute_accuracy:
    name: compute_accuracy
    Module: None
    Signature: compute_accuracy(log_likelihood, qs)
    function_exe_cmd: pymdp.maths.compute_accuracy(log_likelihood, qs)
    Parameters: ['log_likelihood', 'qs']
    Returns: None
    Start Line: 382
    End Line: 393
    Complexity: 1
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Function that computes the accuracy term of the variational free energy. This is essentially a stripped down version of `spm_dot` above,
    with fewer conditions / dimension handling in the beginning.
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/maths.py::dirichlet_log_evidence:
    name: dirichlet_log_evidence
    Module: None
    Signature: dirichlet_log_evidence(q_dir, p_dir, r_dir)
    function_exe_cmd: pymdp.maths.dirichlet_log_evidence(q_dir, p_dir, r_dir)
    Parameters: ['q_dir', 'p_dir', 'r_dir']
    Returns: None
    Start Line: 335
    End Line: 361
    Complexity: 1
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Bayesian model reduction and log evidence calculations for Dirichlet hyperparameters
    This is a NumPY translation of the MATLAB function `spm_MDP_log_evidence.m` from the
    DEM package of spm. 

    Description (adapted from MATLAB docstring)
    This function computes the negative log evidence of a reduced model of a
    Categorical distribution parameterised in terms of Dirichlet hyperparameters 
    (i.e., concentration parameters encoding probabilities). It uses Bayesian model reduction 
    to evaluate the evidence for models with and without a particular parameter.
    Arguments:
    ===========
    `q_dir` [1D np.ndarray]: sufficient statistics of posterior of full model
    `p_dir` [1D np.ndarray]: sufficient statistics of prior of full model
    `r_dir` [1D np.ndarray]: sufficient statistics of prior of reduced model
    Returns:
    ==========
    `F` [float]: free energy or (negative) log evidence of reduced model
    `s_dir` [1D np.ndarray]: sufficient statistics of reduced posterior
    Description: 
    Description Embedding: 
    Comments: ['change in free energy or log model evidence']

  ../pymdp/maths.py::dot_likelihood:
    name: dot_likelihood
    Module: None
    Signature: dot_likelihood(A, obs)
    function_exe_cmd: pymdp.maths.dot_likelihood(A, obs)
    Parameters: ['A', 'obs']
    Returns: None
    Start Line: 239
    End Line: 252
    Complexity: 2
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: None
    Description: 
    Description Embedding: 
    Comments: ['check to see if `LL` is a scalar']

  ../pymdp/maths.py::entropy:
    name: entropy
    Module: None
    Signature: entropy(A)
    function_exe_cmd: pymdp.maths.entropy(A)
    Parameters: ['A']
    Returns: None
    Start Line: 592
    End Line: 608
    Complexity: 3
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Compute the entropy term H of the likelihood matrix,
    i.e. one entropy value per column
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/maths.py::factor_dot_flex:
    name: factor_dot_flex
    Module: None
    Signature: factor_dot_flex(M, xs, dims, keep_dims)
    function_exe_cmd: pymdp.maths.factor_dot_flex(M, xs, dims, keep_dims)
    Parameters: ['M', 'xs', 'dims', 'keep_dims']
    Returns: None
    Start Line: 109
    End Line: 129
    Complexity: 2
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Dot product of a multidimensional array with `x`.

    Parameters
    ----------
    - `M` [numpy.ndarray] - tensor
    - 'xs' [list of numpyr.ndarray] - list of tensors
    - 'dims' [list of tuples] - list of dimensions of xs tensors in tensor M
    - 'keep_dims' [tuple] - tuple of integers denoting dimesions to keep
    Returns 
    -------
    - `Y` [1D numpy.ndarray] - the result of the dot product
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/maths.py::get_joint_likelihood:
    name: get_joint_likelihood
    Module: None
    Signature: get_joint_likelihood(A, obs, num_states)
    function_exe_cmd: pymdp.maths.get_joint_likelihood(A, obs, num_states)
    Parameters: ['A', 'obs', 'num_states']
    Returns: None
    Start Line: 255
    End Line: 264
    Complexity: 3
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: None
    Description: 
    Description Embedding: 
    Comments: ['deal with single modality case']

  ../pymdp/maths.py::get_joint_likelihood_seq:
    name: get_joint_likelihood_seq
    Module: None
    Signature: get_joint_likelihood_seq(A, obs, num_states)
    function_exe_cmd: pymdp.maths.get_joint_likelihood_seq(A, obs, num_states)
    Parameters: ['A', 'obs', 'num_states']
    Returns: None
    Start Line: 267
    End Line: 271
    Complexity: 2
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: None
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/maths.py::get_joint_likelihood_seq_by_modality:
    name: get_joint_likelihood_seq_by_modality
    Module: None
    Signature: get_joint_likelihood_seq_by_modality(A, obs, num_states)
    function_exe_cmd: pymdp.maths.get_joint_likelihood_seq_by_modality(A, obs, num_states)
    Parameters: ['A', 'obs', 'num_states']
    Returns: None
    Start Line: 273
    End Line: 288
    Complexity: 3
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Returns joint likelihoods for each modality separately
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/maths.py::kl_div:
    name: kl_div
    Module: None
    Signature: kl_div(P, Q)
    function_exe_cmd: pymdp.maths.kl_div(P, Q)
    Parameters: ['P', 'Q']
    Returns: None
    Start Line: 575
    End Line: 590
    Complexity: 2
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Parameters
    ----------
    P : Categorical probability distribution
    Q : Categorical probability distribution

    Returns
    -------
    The KL-divergence of P and Q
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/maths.py::softmax:
    name: softmax
    Module: None
    Signature: softmax(dist)
    function_exe_cmd: pymdp.maths.softmax(dist)
    Parameters: ['dist']
    Returns: None
    Start Line: 363
    End Line: 371
    Complexity: 1
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Computes the softmax function on a set of values
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/maths.py::softmax_obj_arr:
    name: softmax_obj_arr
    Module: None
    Signature: softmax_obj_arr(arr)
    function_exe_cmd: pymdp.maths.softmax_obj_arr(arr)
    Parameters: ['arr']
    Returns: None
    Start Line: 373
    End Line: 380
    Complexity: 2
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: None
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/maths.py::spm_MDP_G:
    name: spm_MDP_G
    Module: None
    Signature: spm_MDP_G(A, x)
    function_exe_cmd: pymdp.maths.spm_MDP_G(A, x)
    Parameters: ['A', 'x']
    Returns: None
    Start Line: 517
    End Line: 573
    Complexity: 5
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Calculates the Bayesian surprise in the same way as spm_MDP_G.m does in 
    the original matlab code.

    Parameters
    ----------
    A (numpy ndarray or array-object):
        array assigning likelihoods of observations/outcomes under the various 
        hidden state configurations

    x (numpy ndarray or array-object):
        Categorical distribution presenting probabilities of hidden states 
        (this can also be interpreted as the predictive density over hidden 
        states/causes if you're calculating the expected Bayesian surprise)

    Returns
    -------
    G (float):
        the (expected or not) Bayesian surprise under the density specified by x --
        namely, this scores how much an expected observation would update beliefs 
        about hidden states x, were it to be observed.
    Description: 
    Description Embedding: 
    Comments: ['Probability distribution over the hidden causes: i.e., Q(x)', 'Accumulate expectation of entropy: i.e., E_{Q(o, x)}[lnP(o|x)] = E_{P(o|x)Q(x)}[lnP(o|x)] = E_{Q(x)}[P(o|x)lnP(o|x)] = E_{Q(x)}[H[P(o|x)]]', 'Probability over outcomes for this combination of causes', 'Subtract negative entropy of expectations: i.e., E_{Q(o)}[lnQ(o)]', 'type: ignore']

  ../pymdp/maths.py::spm_betaln:
    name: spm_betaln
    Module: None
    Signature: spm_betaln(z)
    function_exe_cmd: pymdp.maths.spm_betaln(z)
    Parameters: ['z']
    Returns: None
    Start Line: 329
    End Line: 333
    Complexity: 1
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Log of the multivariate beta function of a vector.
     @NOTE this function computes across columns if `z` is a matrix
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/maths.py::spm_calc_neg_ambig:
    name: spm_calc_neg_ambig
    Module: None
    Signature: spm_calc_neg_ambig(A, x)
    function_exe_cmd: pymdp.maths.spm_calc_neg_ambig(A, x)
    Parameters: ['A', 'x']
    Returns: None
    Start Line: 464
    End Line: 515
    Complexity: 5
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Function that just calculates the negativity ambiguity part of the state information gain, using the same method used in 
    spm_MDP_G.m in the original matlab code.

    Parameters
    ----------
    A (numpy ndarray or array-object):
        array assigning likelihoods of observations/outcomes under the various 
        hidden state configurations

    x (numpy ndarray or array-object):
        Categorical distribution presenting probabilities of hidden states 
        (this can also be interpreted as the predictive density over hidden 
        states/causes if you're calculating the expected Bayesian surprise)

    Returns
    -------
    G (float):
        the negative ambiguity (negative entropy of the likelihood of observations given hidden states, expected under current posterior over hidden states)
    Description: 
    Description Embedding: 
    Comments: ['Probability distribution over the hidden causes: i.e., Q(x)', 'Accumulate expectation of entropy: i.e., E_{Q(o, x)}[lnP(o|x)] = E_{P(o|x)Q(x)}[lnP(o|x)] = E_{Q(x)}[P(o|x)lnP(o|x)] = E_{Q(x)}[H[P(o|x)]]', 'Probability over outcomes for this combination of causes']

  ../pymdp/maths.py::spm_calc_qo_entropy:
    name: spm_calc_qo_entropy
    Module: None
    Signature: spm_calc_qo_entropy(A, x)
    function_exe_cmd: pymdp.maths.spm_calc_qo_entropy(A, x)
    Parameters: ['A', 'x']
    Returns: None
    Start Line: 412
    End Line: 462
    Complexity: 5
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Function that just calculates the entropy part of the state information gain, using the same method used in 
    spm_MDP_G.m in the original matlab code.

    Parameters
    ----------
    A (numpy ndarray or array-object):
        array assigning likelihoods of observations/outcomes under the various 
        hidden state configurations

    x (numpy ndarray or array-object):
        Categorical distribution presenting probabilities of hidden states 
        (this can also be interpreted as the predictive density over hidden 
        states/causes if you're calculating the expected Bayesian surprise)

    Returns
    -------
    H (float):
        the entropy of the marginal distribution over observations/outcomes
    Description: 
    Description Embedding: 
    Comments: ['Probability distribution over the hidden causes: i.e., Q(x)', 'Accumulate expectation of entropy: i.e., E_{Q(o, x)}[lnP(o|x)] = E_{P(o|x)Q(x)}[lnP(o|x)] = E_{Q(x)}[P(o|x)lnP(o|x)] = E_{Q(x)}[H[P(o|x)]]', 'Probability over outcomes for this combination of causes', 'Compute entropy of expectations: i.e., -E_{Q(o)}[lnQ(o)]']

  ../pymdp/maths.py::spm_cross:
    name: spm_cross
    Module: None
    Signature: spm_cross(x, y)
    function_exe_cmd: pymdp.maths.spm_cross(x, y)
    Parameters: ['x', 'y']
    Returns: None
    Start Line: 197
    End Line: 237
    Complexity: 11
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Multi-dimensional outer product

    Parameters
    ----------
    - `x` [np.ndarray] || [Categorical] (optional)
        The values to perfrom the outer-product with. If empty, then the outer-product 
        is taken between x and itself. If y is not empty, then outer product is taken 
        between x and the various dimensions of y.
    - `args` [np.ndarray] || [Categorical] (optional)
        Remaining arrays to perform outer-product with. These extra arrays are recursively 
        multiplied with the 'initial' outer product (that between X and x).

    Returns
    -------
    - `z` [np.ndarray] || [Categorical]
          The result of the outer-product
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/maths.py::spm_dot:
    name: spm_dot
    Module: None
    Signature: spm_dot(X, x, dims_to_omit)
    function_exe_cmd: pymdp.maths.spm_dot(X, x, dims_to_omit)
    Parameters: ['X', 'x', 'dims_to_omit']
    Returns: None
    Start Line: 19
    End Line: 56
    Complexity: 4
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Dot product of a multidimensional array with `x`. The dimensions in `dims_to_omit` 
    will not be summed across during the dot product

    Parameters
    ----------
    - `x` [1D numpy.ndarray] - either vector or array of arrays
        The alternative array to perform the dot product with
    - `dims_to_omit` [list :: int] (optional)
        Which dimensions to omit

    Returns 
    -------
    - `Y` [1D numpy.ndarray] - the result of the dot product
    Description: 
    Description Embedding: 
    Comments: ['Construct dims to perform dot product on', 'dims = list((np.arange(0, len(x)) + X.ndim - len(x)).astype(int))', 'dims = list(range(X.ndim))', 'check to see if `Y` is a scalar']

  ../pymdp/maths.py::spm_dot_classic:
    name: spm_dot_classic
    Module: None
    Signature: spm_dot_classic(X, x, dims_to_omit)
    function_exe_cmd: pymdp.maths.spm_dot_classic(X, x, dims_to_omit)
    Parameters: ['X', 'x', 'dims_to_omit']
    Returns: None
    Start Line: 59
    End Line: 107
    Complexity: 7
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Dot product of a multidimensional array with `x`. The dimensions in `dims_to_omit` 
    will not be summed across during the dot product

    Parameters
    ----------
    - `x` [1D numpy.ndarray] - either vector or array of arrays
        The alternative array to perform the dot product with
    - `dims_to_omit` [list :: int] (optional)
        Which dimensions to omit

    Returns 
    -------
    - `Y` [1D numpy.ndarray] - the result of the dot product
    Description: 
    Description Embedding: 
    Comments: ['Construct dims to perform dot product on', 'delete ignored dims', 'compute dot product', 'X = np.sum(X, axis=dims[d], keepdims=True)', 'Y = np.squeeze(X)', 'check to see if `Y` is a scalar']

  ../pymdp/maths.py::spm_dot_old:
    name: spm_dot_old
    Module: None
    Signature: spm_dot_old(X, x, dims_to_omit, obs_mode)
    function_exe_cmd: pymdp.maths.spm_dot_old(X, x, dims_to_omit, obs_mode)
    Parameters: ['X', 'x', 'dims_to_omit', 'obs_mode']
    Returns: None
    Start Line: 131
    End Line: 194
    Complexity: 8
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Dot product of a multidimensional array with `x`. The dimensions in `dims_to_omit` 
    will not be summed across during the dot product

    #TODO: we should look for an alternative to obs_mode

    Parameters
    ----------
    - `x` [1D numpy.ndarray] - either vector or array of arrays
        The alternative array to perform the dot product with
    - `dims_to_omit` [list :: int] (optional)
        Which dimensions to omit

    Returns 
    -------
    - `Y` [1D numpy.ndarray] - the result of the dot product
    Description: 
    Description Embedding: 
    Comments: ['TODO: we should look for an alternative to obs_mode', 'Construct dims to perform dot product on', 'delete ignored dims', 'compute dot product', 'X = np.sum(X, axis=dims[d], keepdims=True)', 'Y = np.squeeze(X)', 'check to see if `Y` is a scalar']

  ../pymdp/maths.py::spm_log_obj_array:
    name: spm_log_obj_array
    Module: None
    Signature: spm_log_obj_array(obj_arr)
    function_exe_cmd: pymdp.maths.spm_log_obj_array(obj_arr)
    Parameters: ['obj_arr']
    Returns: None
    Start Line: 306
    End Line: 315
    Complexity: 2
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Applies `spm_log_single` to multiple elements of a numpy object array
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/maths.py::spm_log_single:
    name: spm_log_single
    Module: None
    Signature: spm_log_single(arr)
    function_exe_cmd: pymdp.maths.spm_log_single(arr)
    Parameters: ['arr']
    Returns: None
    Start Line: 300
    End Line: 304
    Complexity: 1
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Adds small epsilon value to an array before natural logging it
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/maths.py::spm_norm:
    name: spm_norm
    Module: None
    Signature: spm_norm(A)
    function_exe_cmd: pymdp.maths.spm_norm(A)
    Parameters: ['A']
    Returns: None
    Start Line: 291
    End Line: 298
    Complexity: 1
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Returns normalization of Categorical distribution, 
    stored in the columns of A.
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/maths.py::spm_wnorm:
    name: spm_wnorm
    Module: None
    Signature: spm_wnorm(A)
    function_exe_cmd: pymdp.maths.spm_wnorm(A)
    Parameters: ['A']
    Returns: None
    Start Line: 317
    End Line: 326
    Complexity: 1
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Returns Expectation of logarithm of Dirichlet parameters over a set of 
    Categorical distributions, stored in the columns of A.
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/utils.py::build_xn_vn_array:
    name: build_xn_vn_array
    Module: None
    Signature: build_xn_vn_array(xn)
    function_exe_cmd: pymdp.utils.build_xn_vn_array(xn)
    Parameters: ['xn']
    Returns: None
    Start Line: 519
    End Line: 547
    Complexity: 8
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: This function constructs array-ified (not nested) versions
    of the posterior xn (beliefs) or vn (prediction error) arrays, that are separated 
    by iteration, hidden state factor, timepoint, and policy
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/utils.py::construct_controllable_B:
    name: construct_controllable_B
    Module: None
    Signature: construct_controllable_B(num_states, num_controls)
    function_exe_cmd: pymdp.utils.construct_controllable_B(num_states, num_controls)
    Parameters: ['num_states', 'num_controls']
    Returns: None
    Start Line: 165
    End Line: 180
    Complexity: 2
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Generates a fully controllable transition likelihood array, where each 
    action (control state) corresponds to a move to the n-th state from any 
    other state, for each control factor
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/utils.py::construct_full_a:
    name: construct_full_a
    Module: None
    Signature: construct_full_a(A_reduced, original_factor_idx, num_states)
    function_exe_cmd: pymdp.utils.construct_full_a(A_reduced, original_factor_idx, num_states)
    Parameters: ['A_reduced', 'original_factor_idx', 'num_states']
    Returns: None
    Start Line: 447
    End Line: 488
    Complexity: 2
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Utility function for reconstruction a full A matrix from a reduced A matrix, using known factor indices
    to tile out the reduced A matrix along the 'non-informative' dimensions
    Parameters:
    ==========
    - `A_reduced` [np.ndarray]:
        The reduced A matrix or likelihood array that encodes probabilistic relationship
        of the generative model between hidden state factors (lagging dimensions, columns, slices, etc...)
        and observations (leading dimension, rows). 
    - `original_factor_idx` [list]:
        List of hidden state indices in terms of the full hidden state factor list, that comprise
        the lagging dimensions of `A_reduced`
    - `num_states` [list]:
        The list of all the dimensionalities of hidden state factors in the full generative model.
        `A_reduced.shape[1:]` should be equal to `num_states[original_factor_idx]`
    Returns:
    =========
    - `A` [np.ndarray]:
        The full A matrix, containing all the lagging dimensions that correspond to hidden state factors, including
        those that are statistically independent of observations

    @ NOTE: This is the "inverse" of the reduce_a_matrix function, 
    i.e. `reduce_a_matrix(construct_full_a(A_reduced, original_factor_idx, num_states)) == A_reduced, original_factor_idx`
    Description: 
    Description Embedding: 
    Comments: ['dimensionality of the support of the likelihood distribution (i.e. the number of observation levels)', 'full dimensionality of the output (`A`)', 'these are the indices of the dimensions we need to fill for this modality', 'dimensionalities of the relevant factors', 'these are the slices that are filled out by the provided `A_reduced`', 'here we insert the correct values for the fill indices for this slice']

  ../pymdp/utils.py::convert_observation_array:
    name: convert_observation_array
    Module: None
    Signature: convert_observation_array(obs, num_obs)
    function_exe_cmd: pymdp.utils.convert_observation_array(obs, num_obs)
    Parameters: ['obs', 'num_obs']
    Returns: None
    Start Line: 349
    End Line: 395
    Complexity: 5
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Converts from SPM-style observation array to infer-actively one-hot object arrays.

    Parameters
    ----------
    - 'obs' [numpy 2-D nd.array]:
        SPM-style observation arrays are of shape (num_modalities, T), where each row 
        contains observation indices for a different modality, and columns indicate 
        different timepoints. Entries store the indices of the discrete observations 
        within each modality. 

    - 'num_obs' [list]:
        List of the dimensionalities of the observation modalities. `num_modalities` 
        is calculated as `len(num_obs)` in the function to determine whether we're 
        dealing with a single- or multi-modality 
        case.

    Returns
    ----------
    - `obs_t`[list]: 
        A list with length equal to T, where each entry of the list is either a) an object 
        array (in the case of multiple modalities) where each sub-array is a one-hot vector 
        with the observation for the correspond modality, or b) a 1D numpy array (in the case
        of one modality) that is a single one-hot vector encoding the observation for the 
        single modality.
    Description: 
    Description Embedding: 
    Comments: ['Initialise the output', 'Case of one modality', 'Subtract obs[g,t] by 1 to account for MATLAB vs. Python indexing', '(MATLAB is 1-indexed)']

  ../pymdp/utils.py::dirichlet_like:
    name: dirichlet_like
    Module: None
    Signature: dirichlet_like(template_categorical, scale)
    function_exe_cmd: pymdp.utils.dirichlet_like(template_categorical, scale)
    Parameters: ['template_categorical', 'scale']
    Returns: None
    Start Line: 182
    End Line: 201
    Complexity: 3
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Helper function to construct a Dirichlet distribution based on an existing Categorical distribution
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/utils.py::get_model_dimensions:
    name: get_model_dimensions
    Module: None
    Signature: get_model_dimensions(A, B, factorized)
    function_exe_cmd: pymdp.utils.get_model_dimensions(A, B, factorized)
    Parameters: ['A', 'B', 'factorized']
    Returns: None
    Start Line: 203
    End Line: 231
    Complexity: 8
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: None
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/utils.py::get_model_dimensions_from_labels:
    name: get_model_dimensions_from_labels
    Module: None
    Signature: get_model_dimensions_from_labels(model_labels)
    function_exe_cmd: pymdp.utils.get_model_dimensions_from_labels(model_labels)
    Parameters: ['model_labels']
    Returns: None
    Start Line: 233
    End Line: 250
    Complexity: 2
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: None
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/utils.py::initialize_empty_A:
    name: initialize_empty_A
    Module: None
    Signature: initialize_empty_A(num_obs, num_states)
    function_exe_cmd: pymdp.utils.initialize_empty_A(num_obs, num_states)
    Parameters: ['num_obs', 'num_states']
    Returns: None
    Start Line: 70
    End Line: 77
    Complexity: 1
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Initializes an empty observation likelihood array or `A` array using a list of observation-modality dimensions (`num_obs`)
    and hidden state factor dimensions (`num_states`)
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/utils.py::initialize_empty_B:
    name: initialize_empty_B
    Module: None
    Signature: initialize_empty_B(num_states, num_controls)
    function_exe_cmd: pymdp.utils.initialize_empty_B(num_states, num_controls)
    Parameters: ['num_states', 'num_controls']
    Returns: None
    Start Line: 79
    End Line: 86
    Complexity: 1
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Initializes an empty (controllable) transition likelihood array or `B` array using a list of hidden state factor dimensions (`num_states`)
    and control factor dimensions (`num_controls)
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/utils.py::insert_multiple:
    name: insert_multiple
    Module: None
    Signature: insert_multiple(s, indices, items)
    function_exe_cmd: pymdp.utils.insert_multiple(s, indices, items)
    Parameters: ['s', 'indices', 'items']
    Returns: None
    Start Line: 397
    End Line: 400
    Complexity: 2
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: None
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/utils.py::is_normalized:
    name: is_normalized
    Module: None
    Signature: is_normalized(dist)
    function_exe_cmd: pymdp.utils.is_normalized(dist)
    Parameters: ['dist']
    Returns: None
    Start Line: 267
    End Line: 283
    Complexity: 3
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Utility function for checking whether a single distribution or set of conditional categorical distributions is normalized.
    Returns True if all distributions integrate to 1.0
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/utils.py::is_obj_array:
    name: is_obj_array
    Module: None
    Signature: is_obj_array(arr)
    function_exe_cmd: pymdp.utils.is_obj_array(arr)
    Parameters: ['arr']
    Returns: None
    Start Line: 285
    End Line: 286
    Complexity: 1
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: None
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/utils.py::norm_dist:
    name: norm_dist
    Module: None
    Signature: norm_dist(dist)
    function_exe_cmd: pymdp.utils.norm_dist(dist)
    Parameters: ['dist']
    Returns: None
    Start Line: 254
    End Line: 256
    Complexity: 1
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Normalizes a Categorical probability distribution (or set of them) assuming sufficient statistics are stored in leading dimension
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/utils.py::norm_dist_obj_arr:
    name: norm_dist_obj_arr
    Module: None
    Signature: norm_dist_obj_arr(obj_arr)
    function_exe_cmd: pymdp.utils.norm_dist_obj_arr(obj_arr)
    Parameters: ['obj_arr']
    Returns: None
    Start Line: 258
    End Line: 265
    Complexity: 2
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Normalizes a multi-factor or -modality collection of Categorical probability distributions, assuming sufficient statistics of each conditional distribution
    are stored in the leading dimension
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/utils.py::obj_array:
    name: obj_array
    Module: None
    Signature: obj_array(num_arr)
    function_exe_cmd: pymdp.utils.obj_array(num_arr)
    Parameters: ['num_arr']
    Returns: None
    Start Line: 54
    End Line: 58
    Complexity: 1
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Creates a generic object array with the desired number of sub-arrays, given by `num_arr`
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/utils.py::obj_array_from_list:
    name: obj_array_from_list
    Module: None
    Signature: obj_array_from_list(list_input)
    function_exe_cmd: pymdp.utils.obj_array_from_list(list_input)
    Parameters: ['list_input']
    Returns: None
    Start Line: 295
    End Line: 302
    Complexity: 2
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Takes a list of `numpy.ndarray` and converts them to a `numpy.ndarray` of `dtype = object`
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/utils.py::obj_array_ones:
    name: obj_array_ones
    Module: None
    Signature: obj_array_ones(shape_list, scale)
    function_exe_cmd: pymdp.utils.obj_array_ones(shape_list, scale)
    Parameters: ['shape_list', 'scale']
    Returns: None
    Start Line: 99
    End Line: 104
    Complexity: 2
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: None
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/utils.py::obj_array_uniform:
    name: obj_array_uniform
    Module: None
    Signature: obj_array_uniform(shape_list)
    function_exe_cmd: pymdp.utils.obj_array_uniform(shape_list)
    Parameters: ['shape_list']
    Returns: None
    Start Line: 88
    End Line: 97
    Complexity: 2
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Creates a numpy object array whose sub-arrays are uniform Categorical
    distributions with shapes given by shape_list[i]. The shapes (elements of shape_list)
    can either be tuples or lists.
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/utils.py::obj_array_zeros:
    name: obj_array_zeros
    Module: None
    Signature: obj_array_zeros(shape_list)
    function_exe_cmd: pymdp.utils.obj_array_zeros(shape_list)
    Parameters: ['shape_list']
    Returns: None
    Start Line: 60
    End Line: 68
    Complexity: 2
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Creates a numpy object array whose sub-arrays are 1-D vectors
    filled with zeros, with shapes given by shape_list[i]
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/utils.py::onehot:
    name: onehot
    Module: None
    Signature: onehot(value, num_values)
    function_exe_cmd: pymdp.utils.onehot(value, num_values)
    Parameters: ['value', 'num_values']
    Returns: None
    Start Line: 106
    End Line: 109
    Complexity: 1
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: None
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/utils.py::plot_beliefs:
    name: plot_beliefs
    Module: None
    Signature: plot_beliefs(belief_dist, title)
    function_exe_cmd: pymdp.utils.plot_beliefs(belief_dist, title)
    Parameters: ['belief_dist', 'title']
    Returns: None
    Start Line: 549
    End Line: 560
    Complexity: 1
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Utility function that plots a bar chart of a categorical probability distribution,
    with each bar height corresponding to the probability of one of the elements of the categorical
    probability vector.
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/utils.py::plot_likelihood:
    name: plot_likelihood
    Module: None
    Signature: plot_likelihood(A, title)
    function_exe_cmd: pymdp.utils.plot_likelihood(A, title)
    Parameters: ['A', 'title']
    Returns: None
    Start Line: 562
    End Line: 572
    Complexity: 1
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Utility function that shows a heatmap of a 2-D likelihood (hidden causes in the columns, observations in the rows),
    with hotter colors indicating higher probability.
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/utils.py::process_observation:
    name: process_observation
    Module: None
    Signature: process_observation(obs, num_modalities, num_observations)
    function_exe_cmd: pymdp.utils.process_observation(obs, num_modalities, num_observations)
    Parameters: ['obs', 'num_modalities', 'num_observations']
    Returns: None
    Start Line: 318
    End Line: 347
    Complexity: 9
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Helper function for formatting observations    
    USAGE NOTES:
    - If `obs` is a 1D numpy array, it must be a one-hot vector, where one entry (the entry of the observation) is 1.0 
    and all other entries are 0. This therefore assumes it's a single modality observation. If these conditions are met, then
    this function will return `obs` unchanged. Otherwise, it'll throw an error.
    - If `obs` is an int, it assumes this is a single modality observation, whose observation index is given by the value of `obs`. This function will convert
    it to be a one hot vector.
    - If `obs` is a list, it assumes this is a multiple modality observation, whose len is equal to the number of observation modalities,
    and where each entry `obs[m]` is the index of the observation, for that modality. This function will convert it into an object array
    of one-hot vectors.
    - If `obs` is a tuple, same logic as applies for list (see above).
    - if `obs` is a numpy object array (array of arrays), this function will return `obs` unchanged.
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/utils.py::process_observation_seq:
    name: process_observation_seq
    Module: None
    Signature: process_observation_seq(obs_seq, n_modalities, n_observations)
    function_exe_cmd: pymdp.utils.process_observation_seq(obs_seq, n_modalities, n_observations)
    Parameters: ['obs_seq', 'n_modalities', 'n_observations']
    Returns: None
    Start Line: 304
    End Line: 316
    Complexity: 2
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Helper function for formatting observations    

        Observations can either be `int` (converted to one-hot)
        or `tuple` (obs for each modality), or `list` (obs for each modality)
        If list, the entries could be object arrays of one-hots, in which
        case this function returns `obs_seq` as is.
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/utils.py::random_A_matrix:
    name: random_A_matrix
    Module: None
    Signature: random_A_matrix(num_obs, num_states, A_factor_list)
    function_exe_cmd: pymdp.utils.random_A_matrix(num_obs, num_states, A_factor_list)
    Parameters: ['num_obs', 'num_states', 'A_factor_list']
    Returns: None
    Start Line: 111
    End Line: 129
    Complexity: 5
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: None
    Description: 
    Description Embedding: 
    Comments: ['lagging_dimensions = [ns for i, ns in enumerate(num_states) if i in A_factor_list[modality]] # enforces sortedness of A_factor_list']

  ../pymdp/utils.py::random_B_matrix:
    name: random_B_matrix
    Module: None
    Signature: random_B_matrix(num_states, num_controls, B_factor_list)
    function_exe_cmd: pymdp.utils.random_B_matrix(num_states, num_controls, B_factor_list)
    Parameters: ['num_states', 'num_controls', 'B_factor_list']
    Returns: None
    Start Line: 131
    End Line: 149
    Complexity: 5
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: None
    Description: 
    Description Embedding: 
    Comments: ['factor_shape = (num_states[factor], num_states[factor], num_controls[factor])']

  ../pymdp/utils.py::random_single_categorical:
    name: random_single_categorical
    Module: None
    Signature: random_single_categorical(shape_list)
    function_exe_cmd: pymdp.utils.random_single_categorical(shape_list)
    Parameters: ['shape_list']
    Returns: None
    Start Line: 151
    End Line: 163
    Complexity: 2
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Creates a random 1-D categorical distribution (or set of 1-D categoricals, e.g. multiple marginals of different factors) and returns them in an object array
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/utils.py::reduce_a_matrix:
    name: reduce_a_matrix
    Module: None
    Signature: reduce_a_matrix(A)
    function_exe_cmd: pymdp.utils.reduce_a_matrix(A)
    Parameters: ['A']
    Returns: None
    Start Line: 402
    End Line: 445
    Complexity: 7
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Utility function for throwing away dimensions (lagging dimensions, hidden state factors)
    of a particular A matrix that are independent of the observation. 
    Parameters:
    ==========
    - `A` [np.ndarray]:
        The A matrix or likelihood array that encodes probabilistic relationship
        of the generative model between hidden state factors (lagging dimensions, columns, slices, etc...)
        and observations (leading dimension, rows). 
    Returns:
    =========
    - `A_reduced` [np.ndarray]:
        The reduced A matrix, missing the lagging dimensions that correspond to hidden state factors
        that are statistically independent of observations
    - `original_factor_idx` [list]:
        List of the indices (in terms of the original dimensionality) of the hidden state factors
        that are maintained in the A matrix (and thus have an informative / non-degenerate relationship to observations
    Description: 
    Description Embedding: 
    Comments: ['the indices of the hidden state factors that are independent of the observation and thus marginalized away', "this means they're not independent"]

  ../pymdp/utils.py::sample:
    name: sample
    Module: None
    Signature: sample(probabilities)
    function_exe_cmd: pymdp.utils.sample(probabilities)
    Parameters: ['probabilities']
    Returns: None
    Start Line: 40
    End Line: 43
    Complexity: 1
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: None
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/utils.py::sample_obj_array:
    name: sample_obj_array
    Module: None
    Signature: sample_obj_array(arr)
    function_exe_cmd: pymdp.utils.sample_obj_array(arr)
    Parameters: ['arr']
    Returns: None
    Start Line: 45
    End Line: 52
    Complexity: 1
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Sample from set of Categorical distributions, stored in the sub-arrays of an object array
    Description: 
    Description Embedding: 
    Comments: None

  ../pymdp/utils.py::to_obj_array:
    name: to_obj_array
    Module: None
    Signature: to_obj_array(arr)
    function_exe_cmd: pymdp.utils.to_obj_array(arr)
    Parameters: ['arr']
    Returns: None
    Start Line: 288
    End Line: 293
    Complexity: 2
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: None
    Description: 
    Description Embedding: 
    Comments: None

APIs:

Tests:

  ../test/test_SPM_validation.py::test_BMR_SPM_a:
    name: test_BMR_SPM_a
    Module: None
    Signature: test_BMR_SPM_a(self)
    test_exe_cmd: test.test_SPM_validation.TestSPM.test_BMR_SPM_a(self)
    Parameters: ['self']
    Returns: None
    Start Line: 72
    End Line: 96
    Complexity: 3
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Validate output of pymdp's `dirichlet_log_evidence` function 
        against output of `spm_MDP_log_evidence` from DEM in SPM (MATLAB)
        Test `a` tests the log evidence calculations across for a single
        reduced model, stored in a vector `r_dir`
    Description: 
    Description Embedding: 
    Comments: ['create BMR example from MATLAB']

  ../test/test_SPM_validation.py::test_BMR_SPM_b:
    name: test_BMR_SPM_b
    Module: None
    Signature: test_BMR_SPM_b(self)
    test_exe_cmd: test.test_SPM_validation.TestSPM.test_BMR_SPM_b(self)
    Parameters: ['self']
    Returns: None
    Start Line: 98
    End Line: 117
    Complexity: 1
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Validate output of pymdp's `dirichlet_log_evidence` function 
        against output of `spm_MDP_log_evidence` from DEM in SPM (MATLAB). 
        Test `b` vectorizes the log evidence calculations across a _matrix_ of 
        reduced models, with one reduced model prior per column of the argument `r_dir`
    Description: 
    Description Embedding: 
    Comments: None

  ../test/test_SPM_validation.py::test_active_inference_SPM_1a:
    name: test_active_inference_SPM_1a
    Module: None
    Signature: test_active_inference_SPM_1a(self)
    test_exe_cmd: test.test_SPM_validation.TestSPM.test_active_inference_SPM_1a(self)
    Parameters: ['self']
    Returns: None
    Start Line: 15
    End Line: 70
    Complexity: 3
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Test against output of SPM_MDP_VB_X.m
        1A - one hidden state factor, one observation modality, backwards horizon = 3, policy_len = 1, policy-conditional prior
    Description: 
    Description Embedding: 
    Comments: None

  ../test/test_agent.py::test_actinfloop_factorized:
    name: test_actinfloop_factorized
    Module: None
    Signature: test_actinfloop_factorized(self)
    test_exe_cmd: test.test_agent.TestAgent.test_actinfloop_factorized(self)
    Parameters: ['self']
    Returns: None
    Start Line: 738
    End Line: 804
    Complexity: 8
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Test that an instance of the `Agent` class can be initialized and run
        with the fully-factorized generative model functions (including policy inference)
    Description: 
    Description Embedding: 
    Comments: ['need to have `save_belief_hist=True` for this to work']

  ../test/test_agent.py::test_agent_distributional_obs:
    name: test_agent_distributional_obs
    Module: None
    Signature: test_agent_distributional_obs(self)
    test_exe_cmd: test.test_agent.TestAgent.test_agent_distributional_obs(self)
    Parameters: ['self']
    Returns: None
    Start Line: 590
    End Line: 670
    Complexity: 8
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: VANILLA method (fixed point iteration) with one hidden state factor and one observation modality
    Description: 
    Description Embedding: 
    Comments: ['use a distributional observation', '@NOTE: `utils.obj_array_from_list` will make a nested list of object arrays if you only put in a list with one vector!!! Makes me think we should remove utils.obj_array_from_list potentially', 'use a distributional observation', 'use a distributional observation', '@NOTE: `utils.obj_array_from_list` will make a nested list of object arrays if you only put in a list with one vector!!! Makes me think we should remove utils.obj_array_from_list potentially', 'use a distributional observation']

  ../test/test_agent.py::test_agent_infer_states:
    name: test_agent_infer_states
    Module: None
    Signature: test_agent_infer_states(self)
    test_exe_cmd: test.test_agent.TestAgent.test_agent_infer_states(self)
    Parameters: ['self']
    Returns: None
    Start Line: 93
    End Line: 172
    Complexity: 8
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Test `infer_states` method of the Agent() class
    Description: 
    Description Embedding: 
    Comments: None

  ../test/test_agent.py::test_agent_init_without_control_fac_idx:
    name: test_agent_init_without_control_fac_idx
    Module: None
    Signature: test_agent_init_without_control_fac_idx(self)
    test_exe_cmd: test.test_agent.TestAgent.test_agent_init_without_control_fac_idx(self)
    Parameters: ['self']
    Returns: None
    Start Line: 23
    End Line: 39
    Complexity: 1
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Initialize instance of the agent class and pass in a custom `control_fac_idx`
    Description: 
    Description Embedding: 
    Comments: None

  ../test/test_agent.py::test_agent_with_A_learning_vanilla:
    name: test_agent_with_A_learning_vanilla
    Module: None
    Signature: test_agent_with_A_learning_vanilla(self)
    test_exe_cmd: test.test_agent.TestAgent.test_agent_with_A_learning_vanilla(self)
    Parameters: ['self']
    Returns: None
    Start Line: 204
    End Line: 249
    Complexity: 2
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Unit test for updating prior Dirichlet parameters over likelihood model (pA) with the ``Agent`` class,
        in the case that you're using "vanilla" inference mode.
    Description: 
    Description Embedding: 
    Comments: ['3 x 3, 2-dimensional grid world', 'get some true transition dynamics', 'instantiate the agent', 'time horizon', 'sample the next state given the true transition dynamics and the sampled action', 'compute the predicted update to the action-conditioned slice of qB', 'update qA using the agent function', 'check if the predicted update and the actual update are the same']

  ../test/test_agent.py::test_agent_with_A_learning_vanilla_factorized:
    name: test_agent_with_A_learning_vanilla_factorized
    Module: None
    Signature: test_agent_with_A_learning_vanilla_factorized(self)
    test_exe_cmd: test.test_agent.TestAgent.test_agent_with_A_learning_vanilla_factorized(self)
    Parameters: ['self']
    Returns: None
    Start Line: 251
    End Line: 294
    Complexity: 5
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Unit test for updating prior Dirichlet parameters over likelihood model (pA) with the ``Agent`` class,
        in the case that you're using "vanilla" inference mode. In this case, we encode sparse conditional dependencies by specifying
        a non-all-to-all `A_factor_list`, that specifies the subset of hidden state factors that different modalities depend on.
    Description: 
    Description Embedding: 
    Comments: ['instantiate the agent', 'time horizon', 'compute the predicted update to the action-conditioned slice of qB', 'update qA using the agent function', 'check if the predicted update and the actual update are the same']

  ../test/test_agent.py::test_agent_with_B_learning_vanilla:
    name: test_agent_with_B_learning_vanilla
    Module: None
    Signature: test_agent_with_B_learning_vanilla(self)
    test_exe_cmd: test.test_agent.TestAgent.test_agent_with_B_learning_vanilla(self)
    Parameters: ['self']
    Returns: None
    Start Line: 296
    End Line: 344
    Complexity: 3
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Unit test for updating prior Dirichlet parameters over transition model (pB) with the ``Agent`` class,
        in the case that you're using "vanilla" inference mode.
    Description: 
    Description Embedding: 
    Comments: ['3 x 3, 2-dimensional grid world', 'flat transition prior', 'instantiate the agent', 'get some true transition dynamics', 'time horizon', 'sample the next state given the true transition dynamics and the sampled action', 'compute the predicted update to the action-conditioned slice of qB', 'update qB using the agent function', 'check if the predicted update and the actual update are the same']

  ../test/test_agent.py::test_agent_with_D_learning_MMP:
    name: test_agent_with_D_learning_MMP
    Module: None
    Signature: test_agent_with_D_learning_MMP(self)
    test_exe_cmd: test.test_agent.TestAgent.test_agent_with_D_learning_MMP(self)
    Parameters: ['self']
    Returns: None
    Start Line: 434
    End Line: 503
    Complexity: 6
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Test updating prior Dirichlet parameters over initial hidden states (pD) with the agent class,
        in the case that you're using MMP inference and various combinations of Bayesian model averaging at the edge of the inference horizon vs. other possibilities
    Description: 
    Description Embedding: 
    Comments: ['1. Using Bayesian model average over hidden states at the edge of the inference horizon', 'get some random hidden state distribution', '2. Using policy-conditioned prior over hidden states at the edge of the inference horizon', 'get some random hidden state distribution', 'get beliefs about policies at the time at the beginning of the inference horizon', 'beliefs about hidden states at the first timestep of the inference horizon']

  ../test/test_agent.py::test_agent_with_D_learning_vanilla:
    name: test_agent_with_D_learning_vanilla
    Module: None
    Signature: test_agent_with_D_learning_vanilla(self)
    test_exe_cmd: test.test_agent.TestAgent.test_agent_with_D_learning_vanilla(self)
    Parameters: ['self']
    Returns: None
    Start Line: 346
    End Line: 432
    Complexity: 7
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Test updating prior Dirichlet parameters over initial hidden states (pD) with the ``Agent`` class,
        in the case that you're using "vanilla" inference mode.
    Description: 
    Description Embedding: 
    Comments: ['HMM mode', "1. Test that the updating works when `save_belief_hist` is True, and you don't need to pass in the beliefs about first hidden states", 'get some random hidden state distribution', '2. Test that the updating works when `save_belief_hist` is False, and you do have to pass in the beliefs about first hidden states', 'get some random hidden state distribution', '3. Same as test #1, except with learning on only certain hidden state factors. Also passed in a different learning rate', 'get some random hidden state distribution']

  ../test/test_agent.py::test_agent_with_factorized_inference:
    name: test_agent_with_factorized_inference
    Module: None
    Signature: test_agent_with_factorized_inference(self)
    test_exe_cmd: test.test_agent.TestAgent.test_agent_with_factorized_inference(self)
    Parameters: ['self']
    Returns: None
    Start Line: 672
    End Line: 705
    Complexity: 3
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Test that an instance of the `Agent` class can be initialized with a provided `A_factor_list` and run the factorized inference algorithm. Validate
        against an equivalent `Agent` whose `A` matrix represents the full set of (redundant) conditional dependence relationships.
    Description: 
    Description Embedding: 
    Comments: ['list of the factors that modality `m` does not depend on', 'broadcast or tile the reduced A matrix (`A_reduced`) along the dimensions of corresponding to `other_factors`']

  ../test/test_agent.py::test_agent_with_input_alpha:
    name: test_agent_with_input_alpha
    Module: None
    Signature: test_agent_with_input_alpha(self)
    test_exe_cmd: test.test_agent.TestAgent.test_agent_with_input_alpha(self)
    Parameters: ['self']
    Returns: None
    Start Line: 505
    End Line: 532
    Complexity: 1
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Test for passing in `alpha` (action sampling precision parameter) as argument to `agent.Agent()` constructor.
        Test two cases to make sure alpha scaling is working properly, by comparing entropies of action marginals 
        after computing posterior over actions in cases where alpha is passed in as two different values to the Agent constructor.
    Description: 
    Description Embedding: 
    Comments: None

  ../test/test_agent.py::test_agent_with_interactions_in_B:
    name: test_agent_with_interactions_in_B
    Module: None
    Signature: test_agent_with_interactions_in_B(self)
    test_exe_cmd: test.test_agent.TestAgent.test_agent_with_interactions_in_B(self)
    Parameters: ['self']
    Returns: None
    Start Line: 707
    End Line: 736
    Complexity: 4
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Test that an instance of the `Agent` class can be initialized with a provided `B_factor_list` and run a time loop of active inferece
    Description: 
    Description Embedding: 
    Comments: None

  ../test/test_agent.py::test_agent_with_sampling_mode:
    name: test_agent_with_sampling_mode
    Module: None
    Signature: test_agent_with_sampling_mode(self)
    test_exe_cmd: test.test_agent.TestAgent.test_agent_with_sampling_mode(self)
    Parameters: ['self']
    Returns: None
    Start Line: 534
    End Line: 563
    Complexity: 1
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Test for passing in `sampling_mode` argument to `agent.Agent()` constructor, which determines whether you sample
        from posterior marginal over actions ("marginal", default) or posterior over policies ("full")
    Description: 
    Description Embedding: 
    Comments: None

  ../test/test_agent.py::test_agent_with_stochastic_action_unidimensional_control:
    name: test_agent_with_stochastic_action_unidimensional_control
    Module: None
    Signature: test_agent_with_stochastic_action_unidimensional_control(self)
    test_exe_cmd: test.test_agent.TestAgent.test_agent_with_stochastic_action_unidimensional_control(self)
    Parameters: ['self']
    Returns: None
    Start Line: 565
    End Line: 588
    Complexity: 1
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Test stochastic action sampling in case that one of the control states is one-dimensional, within the agent
        method `sample_action()`.
        Due to a call to probabilities.squeeze() in an earlier version of utils.sample(), this was throwing an
        error due to the inability to use np.random.multinomial on an array with undefined length (an 'unsized' array)
    Description: 
    Description Embedding: 
    Comments: None

  ../test/test_agent.py::test_mmp_active_inference:
    name: test_mmp_active_inference
    Module: None
    Signature: test_mmp_active_inference(self)
    test_exe_cmd: test.test_agent.TestAgent.test_mmp_active_inference(self)
    Parameters: ['self']
    Returns: None
    Start Line: 174
    End Line: 202
    Complexity: 2
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Tests to make sure whole active inference loop works (with various past and future
        inference/policy horizons).
    Description: 
    Description Embedding: 
    Comments: ['just randomly generate observations at each timestep, no generative process']

  ../test/test_agent.py::test_reset_agent_MMP_wBMA:
    name: test_reset_agent_MMP_wBMA
    Module: None
    Signature: test_reset_agent_MMP_wBMA(self)
    test_exe_cmd: test.test_agent.TestAgent.test_reset_agent_MMP_wBMA(self)
    Parameters: ['self']
    Returns: None
    Start Line: 59
    End Line: 75
    Complexity: 1
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Ensure the `reset` method of Agent() using the new refactor is working as intended, 
        using the `MMP` argument to `inference_algo`, and `use_BMA` equal to True
    Description: 
    Description Embedding: 
    Comments: None

  ../test/test_agent.py::test_reset_agent_MMP_wPSP:
    name: test_reset_agent_MMP_wPSP
    Module: None
    Signature: test_reset_agent_MMP_wPSP(self)
    test_exe_cmd: test.test_agent.TestAgent.test_reset_agent_MMP_wPSP(self)
    Parameters: ['self']
    Returns: None
    Start Line: 77
    End Line: 91
    Complexity: 1
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Ensure the `reset` method of Agent() using the new refactor is working as intended, 
        using the `MMP` argument to `inference_algo`, and `policy-separated prior` equal to True
    Description: 
    Description Embedding: 
    Comments: None

  ../test/test_agent.py::test_reset_agent_VANILLA:
    name: test_reset_agent_VANILLA
    Module: None
    Signature: test_reset_agent_VANILLA(self)
    test_exe_cmd: test.test_agent.TestAgent.test_reset_agent_VANILLA(self)
    Parameters: ['self']
    Returns: None
    Start Line: 41
    End Line: 57
    Complexity: 1
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Ensure the `reset` method of Agent() using the new refactor is working as intended, 
        using the `VANILLA` argument to `inference_algo`
    Description: 
    Description Embedding: 
    Comments: None

  ../test/test_agent_jax.py::test_vmappable_agent_methods:
    name: test_vmappable_agent_methods
    Module: None
    Signature: test_vmappable_agent_methods(self)
    test_exe_cmd: test.test_agent_jax.TestAgentJax.test_vmappable_agent_methods(self)
    Parameters: ['self']
    Returns: None
    Start Line: 23
    End Line: 59
    Complexity: 2
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: None
    Description: 
    Description Embedding: 
    Comments: ['validate that the method broadcasted properly']

  ../test/test_control.py::test_deterministic_action_sampling_equal_value:
    name: test_deterministic_action_sampling_equal_value
    Module: None
    Signature: test_deterministic_action_sampling_equal_value(self)
    test_exe_cmd: test.test_control.TestControl.test_deterministic_action_sampling_equal_value(self)
    Parameters: ['self']
    Returns: None
    Start Line: 1662
    End Line: 1679
    Complexity: 1
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Test `deterministic` action sampling in the case that multiple actions have the same probability. 
        Desired behavior is that actions are randomly sampled from the subset of total actions that have the highest (but equal) probability.
    Description: 
    Description Embedding: 
    Comments: None

  ../test/test_control.py::test_deterministic_policy_selection_equal_value:
    name: test_deterministic_policy_selection_equal_value
    Module: None
    Signature: test_deterministic_policy_selection_equal_value(self)
    test_exe_cmd: test.test_control.TestControl.test_deterministic_policy_selection_equal_value(self)
    Parameters: ['self']
    Returns: None
    Start Line: 1681
    End Line: 1698
    Complexity: 1
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Test `deterministic` action sampling in the case that multiple actions have the same probability. 
        Desired behavior is that actions are randomly sampled from the subset of total actions that have the highest (but equal) probability.
    Description: 
    Description Embedding: 
    Comments: None

  ../test/test_control.py::test_expected_utility:
    name: test_expected_utility
    Module: None
    Signature: test_expected_utility(self)
    test_exe_cmd: test.test_control.TestControl.test_expected_utility(self)
    Parameters: ['self']
    Returns: None
    Start Line: 321
    End Line: 389
    Complexity: 3
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Test for the expected utility function, for a simple single factor generative model 
        where there are imbalances in the preferences for different outcomes. Test for both single
        timestep policy horizons and multiple timestep policy horizons (planning)
    Description: 
    Description Embedding: 
    Comments: ['Single timestep', 'Single observation modality', 'Create noiseless identity A matrix', 'Create imbalance in preferences for observations', 'Compute expected utility of policies', 'One policy entails going to state 0 two times in a row, and then state 2 at the end', 'Another policy entails going to state 1 three times in a row', 'single observation modality', 'create noiseless identity A matrix', 'create imbalance in preferences for observations', 'This test is designed to illustrate the emergence of planning by', 'using the time-integral of the expected free energy.', 'Even though the first observation (index 0) is the most preferred, the policy', 'that frequents this observation the most is actually not optimal, because that policy', 'terminates in a less preferred state by timestep 3.']

  ../test/test_control.py::test_get_expected_obs_factorized:
    name: test_get_expected_obs_factorized
    Module: None
    Signature: test_get_expected_obs_factorized(self)
    test_exe_cmd: test.test_control.TestControl.test_get_expected_obs_factorized(self)
    Parameters: ['self']
    Returns: None
    Start Line: 167
    End Line: 212
    Complexity: 4
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Test the new version of `get_expected_obs` that includes sparse dependencies of `A` array on hidden state factors (not all observation modalities depend on all hidden state factors)
    Description: 
    Description Embedding: 
    Comments: ['need to wrap `qs` in list because `get_expected_obs_factorized` expects a list of `qs` (representing multiple timesteps)', 'need to wrap `qs` in list because `get_expected_obs` expects a list of `qs` (representing multiple timesteps)', 'need to extract first index of `qo_test` and `qo_val` because `get_expected_obs_factorized` returns a list of `qo` (representing multiple timesteps)', 'need to wrap `qs` in list because `get_expected_obs_factorized` expects a list of `qs` (representing multiple timesteps)', 'list of the factors that modality `m` does not depend on', 'broadcast or tile the reduced A matrix (`A_reduced`) along the dimensions of corresponding to `other_factors`', 'need to wrap `qs` in list because `get_expected_obs` expects a list of `qs` (representing multiple timesteps)', 'need to extract first index of `qo_test` and `qo_val` because `get_expected_obs_factorized` returns a list of `qo` (representing multiple timesteps)']

  ../test/test_control.py::test_get_expected_states:
    name: test_get_expected_states
    Module: None
    Signature: test_get_expected_states(self)
    test_exe_cmd: test.test_control.TestControl.test_get_expected_states(self)
    Parameters: ['self']
    Returns: None
    Start Line: 18
    End Line: 100
    Complexity: 11
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Tests the refactored (Categorical-less) version of `get_expected_states`
    Description: 
    Description Embedding: 
    Comments: None

  ../test/test_control.py::test_get_expected_states_and_obs:
    name: test_get_expected_states_and_obs
    Module: None
    Signature: test_get_expected_states_and_obs(self)
    test_exe_cmd: test.test_control.TestControl.test_get_expected_states_and_obs(self)
    Parameters: ['self']
    Returns: None
    Start Line: 214
    End Line: 319
    Complexity: 11
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Tests the refactored (Categorical-less) versions of `get_expected_states` and `get_expected_obs` together
    Description: 
    Description Embedding: 
    Comments: ['validation qs_pi', 'validation qo_pi', 'validation qs_pi', 'validation qo_pi', 'validation qs_pi', 'validation qo_pi']

  ../test/test_control.py::test_get_expected_states_interactions_multi_factor:
    name: test_get_expected_states_interactions_multi_factor
    Module: None
    Signature: test_get_expected_states_interactions_multi_factor(self)
    test_exe_cmd: test.test_control.TestControl.test_get_expected_states_interactions_multi_factor(self)
    Parameters: ['self']
    Returns: None
    Start Line: 121
    End Line: 142
    Complexity: 1
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Test the new version of `get_expected_states` that includes `B` array inter-factor dependencies, 
        in the case where there are two hidden state factors: one that depends on itself and another that depends on both itself and the other factor.
    Description: 
    Description Embedding: 
    Comments: ['how to compute equivalent of `spm_dot(B[...,past_action], qs)`']

  ../test/test_control.py::test_get_expected_states_interactions_multi_factor_independent:
    name: test_get_expected_states_interactions_multi_factor_independent
    Module: None
    Signature: test_get_expected_states_interactions_multi_factor_independent(self)
    test_exe_cmd: test.test_control.TestControl.test_get_expected_states_interactions_multi_factor_independent(self)
    Parameters: ['self']
    Returns: None
    Start Line: 144
    End Line: 165
    Complexity: 2
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Test the new version of `get_expected_states` that includes `B` array inter-factor dependencies, 
        in the case where there are multiple hidden state factors, but they all only depend on themselves
    Description: 
    Description Embedding: 
    Comments: ['each factor only depends on itself']

  ../test/test_control.py::test_get_expected_states_interactions_single_factor:
    name: test_get_expected_states_interactions_single_factor
    Module: None
    Signature: test_get_expected_states_interactions_single_factor(self)
    test_exe_cmd: test.test_control.TestControl.test_get_expected_states_interactions_single_factor(self)
    Parameters: ['self']
    Returns: None
    Start Line: 102
    End Line: 119
    Complexity: 1
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Test the new version of `get_expected_states` that includes `B` array inter-factor dependencies, in case a of trivial single factor
    Description: 
    Description Embedding: 
    Comments: None

  ../test/test_control.py::test_pA_info_gain:
    name: test_pA_info_gain
    Module: None
    Signature: test_pA_info_gain(self)
    test_exe_cmd: test.test_control.TestControl.test_pA_info_gain(self)
    Parameters: ['self']
    Returns: None
    Start Line: 568
    End Line: 618
    Complexity: 3
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Test the pA_info_gain function. Demonstrates operation
        by manipulating shape of the Dirichlet priors over likelihood parameters
        (pA), which affects information gain for different expected observations
    Description: 
    Description Embedding: 
    Comments: ['start with a precise initial state', 'single timestep', 'single observation modality', 'create noiseless identity A matrix', 'create prior over dirichlets such that there is a skew', 'in the parameters about the likelihood mapping from the', 'second hidden state (index 1) to observations, such that', 'Observation 0 is believed to be more likely than the other, conditioned on State 1.', 'Therefore sampling observations conditioned on State 1 would afford high info gain', 'about parameters, for that part of the likelhood distribution.']

  ../test/test_control.py::test_pB_info_gain:
    name: test_pB_info_gain
    Module: None
    Signature: test_pB_info_gain(self)
    test_exe_cmd: test.test_control.TestControl.test_pB_info_gain(self)
    Parameters: ['self']
    Returns: None
    Start Line: 620
    End Line: 661
    Complexity: 3
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Test the pB_info_gain function. Demonstrates operation
        by manipulating shape of the Dirichlet priors over likelihood parameters
        (pB), which affects information gain for different states
    Description: 
    Description Embedding: 
    Comments: ['start with a precise initial state', 'create prior over dirichlets such that there is a skew', 'in the parameters about the likelihood mapping from the', 'hidden states to hidden states under the second action,', 'such that hidden state 0 is considered to be more likely than the other,', 'given the action in question', 'Therefore taking that action would yield an expected state that afford', 'high information gain about that part of the likelihood distribution.', '', 'single timestep']

  ../test/test_control.py::test_sample_action:
    name: test_sample_action
    Module: None
    Signature: test_sample_action(self)
    test_exe_cmd: test.test_control.TestControl.test_sample_action(self)
    Parameters: ['self']
    Returns: None
    Start Line: 1415
    End Line: 1580
    Complexity: 1
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Tests the refactored (Categorical-less) version of `sample_action`
    Description: 
    Description Embedding: 
    Comments: ['One policy entails going to state 0 two times in a row, and then state 2 at the end', 'Another policy entails going to state 1 three times in a row', 'create noiseless identity A matrix', 'create imbalance in preferences for observations', 'This test is designed to illustrate the emergence of planning by', 'using the time-integral of the expected free energy.', 'Even though the first observation (index 0) is the most preferred, the policy', 'that frequents this observation the most is actually not optimal, because that policy', 'terminates in a less preferred state by timestep 3.']

  ../test/test_control.py::test_sample_policy:
    name: test_sample_policy
    Module: None
    Signature: test_sample_policy(self)
    test_exe_cmd: test.test_control.TestControl.test_sample_policy(self)
    Parameters: ['self']
    Returns: None
    Start Line: 1582
    End Line: 1602
    Complexity: 2
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Tests the action selection function where policies are sampled directly from posterior over policies `q_pi`
    Description: 
    Description Embedding: 
    Comments: None

  ../test/test_control.py::test_state_info_gain:
    name: test_state_info_gain
    Module: None
    Signature: test_state_info_gain(self)
    test_exe_cmd: test.test_control.TestControl.test_state_info_gain(self)
    Parameters: ['self']
    Returns: None
    Start Line: 391
    End Line: 465
    Complexity: 4
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Test the states_info_gain function. 
        Function is tested by manipulating uncertainty in the likelihood matrices (A or B)
        in a ways that alternatively change the resolvability of uncertainty
        This is done with A) an imprecise expected state and a precise sensory mapping, 
        and B) high ambiguity and imprecise sensory mapping.
    Description: 
    Description Embedding: 
    Comments: ['start with a precise initial state', 'add some uncertainty into the consequences of the second policy, which', 'leads to increased epistemic value of observations, in case of pursuing', 'that policy -- this of course depends on a precise observation likelihood model', '"noise-ify" the consequences of the 1-th action', 'single timestep', 'single observation modality', 'create noiseless identity A matrix', 'store the Bayesian surprise / epistemic values of states here (AKA state info gain)', "now we 'undo' the epistemic bonus of the second policy by making the A matrix", 'totally ambiguous; thus observations cannot resolve uncertainty about hidden states.', "In this case, uncertainty in the posterior beliefs induced by Policy 1 doesn't tip the balance", 'of epistemic value, because uncertainty is irresolveable either way.', 'create noiseless identity A matrix', 'add some uncertainty into the consequences of the both policies', '"noise-ify" the consequences of the 0-th action, but to a lesser extent than the 1-th action', '"noise-ify" the consequences of the 1-th action', 'Although in the presence of a precise likelihood mapping,', 'Policy 1 would be preferred (due to higher resolve-able uncertainty, introduced by a noisier action-dependent B matrix),', 'if the expected observation likelihood of being in state 1 (the most likely consequence of Policy 1) is not precise, then', 'Policy 0 (which has more probability loaded over state 0) will have more resolveable uncertainty, due to the', 'higher precision of the A matrix over that column (column 0, which is identity). Even though the expected density over states', 'is less noisy for policy 0.']

  ../test/test_control.py::test_state_info_gain_factorized:
    name: test_state_info_gain_factorized
    Module: None
    Signature: test_state_info_gain_factorized(self)
    test_exe_cmd: test.test_control.TestControl.test_state_info_gain_factorized(self)
    Parameters: ['self']
    Returns: None
    Start Line: 467
    End Line: 530
    Complexity: 11
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Unit test the `calc_states_info_gain_factorized` function by qualitatively checking that in the T-Maze (contextual bandit)
        example, the state info gain is higher for the policy that leads to visiting the cue, which is higher than state info gain
        for visiting the bandit arm, which in turn is higher than the state info gain for the policy that leads to staying in the start state.
    Description: 
    Description Embedding: 
    Comments: ['cue statistics', 'bandit statistics (mapping between reward-state (first hidden state factor) and rewards (Good vs Bad))', "agent believes it's in the start state", "agent believes it's in the arm-visiting state", "agent believes it's in the cue-visiting state"]

  ../test/test_control.py::test_stochastic_action_unidimensional_control:
    name: test_stochastic_action_unidimensional_control
    Module: None
    Signature: test_stochastic_action_unidimensional_control(self)
    test_exe_cmd: test.test_control.TestControl.test_stochastic_action_unidimensional_control(self)
    Parameters: ['self']
    Returns: None
    Start Line: 1645
    End Line: 1660
    Complexity: 1
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Test stochastic action sampling in case that one of the control states is one-dimensional.
        Due to a call to probabilities.squeeze() in an earlier version of utils.sample(), this was throwing an
        error due to the inability to use np.random.multinomial on an array with undefined length (an 'unsized' array)
    Description: 
    Description Embedding: 
    Comments: None

  ../test/test_control.py::test_temporal_C_matrix:
    name: test_temporal_C_matrix
    Module: None
    Signature: test_temporal_C_matrix(self)
    test_exe_cmd: test.test_control.TestControl.test_temporal_C_matrix(self)
    Parameters: ['self']
    Returns: None
    Start Line: 812
    End Line: 966
    Complexity: 12
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Unit-tests for preferences that change over time
    Description: 
    Description Embedding: 
    Comments: ['C vectors for modalities 0 is time-dependent', 'C vectors for modalities 1 is time-independent']

  ../test/test_control.py::test_update_posterior_policies_factorized:
    name: test_update_posterior_policies_factorized
    Module: None
    Signature: test_update_posterior_policies_factorized(self)
    test_exe_cmd: test.test_control.TestControl.test_update_posterior_policies_factorized(self)
    Parameters: ['self']
    Returns: None
    Start Line: 1378
    End Line: 1413
    Complexity: 1
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Test new update_posterior_policies_factorized function, just to make sure it runs through and outputs correct shapes
    Description: 
    Description Embedding: 
    Comments: None

  ../test/test_control.py::test_update_posterior_policies_pA_infogain:
    name: test_update_posterior_policies_pA_infogain
    Module: None
    Signature: test_update_posterior_policies_pA_infogain(self)
    test_exe_cmd: test.test_control.TestControl.test_update_posterior_policies_pA_infogain(self)
    Parameters: ['self']
    Returns: None
    Start Line: 1101
    End Line: 1239
    Complexity: 4
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Tests the refactored (Categorical-less) version of `update_posterior_policies`, using only the information gain (about likelihood parameters) component of the expected free energy
    Description: 
    Description Embedding: 
    Comments: None

  ../test/test_control.py::test_update_posterior_policies_pB_infogain:
    name: test_update_posterior_policies_pB_infogain
    Module: None
    Signature: test_update_posterior_policies_pB_infogain(self)
    test_exe_cmd: test.test_control.TestControl.test_update_posterior_policies_pB_infogain(self)
    Parameters: ['self']
    Returns: None
    Start Line: 1241
    End Line: 1376
    Complexity: 4
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Tests the refactored (Categorical-less) version of `update_posterior_policies`, using only the information gain (about transition likelihood parameters) component of the expected free energy
    Description: 
    Description Embedding: 
    Comments: None

  ../test/test_control.py::test_update_posterior_policies_states_infogain:
    name: test_update_posterior_policies_states_infogain
    Module: None
    Signature: test_update_posterior_policies_states_infogain(self)
    test_exe_cmd: test.test_control.TestControl.test_update_posterior_policies_states_infogain(self)
    Parameters: ['self']
    Returns: None
    Start Line: 969
    End Line: 1099
    Complexity: 5
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Tests the refactored (Categorical-less) version of `update_posterior_policies`, using only the information gain (about states) component of the expected free energy
    Description: 
    Description Embedding: 
    Comments: None

  ../test/test_control.py::test_update_posterior_policies_utility:
    name: test_update_posterior_policies_utility
    Module: None
    Signature: test_update_posterior_policies_utility(self)
    test_exe_cmd: test.test_control.TestControl.test_update_posterior_policies_utility(self)
    Parameters: ['self']
    Returns: None
    Start Line: 663
    End Line: 810
    Complexity: 7
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Tests the refactored (Categorical-less) version of `update_posterior_policies`, using only the expected utility component of the expected free energy
    Description: 
    Description Embedding: 
    Comments: None

  ../test/test_control.py::test_update_posterior_policies_withE_vector:
    name: test_update_posterior_policies_withE_vector
    Module: None
    Signature: test_update_posterior_policies_withE_vector(self)
    test_exe_cmd: test.test_control.TestControl.test_update_posterior_policies_withE_vector(self)
    Parameters: ['self']
    Returns: None
    Start Line: 1604
    End Line: 1643
    Complexity: 1
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Test update posterior policies in the case that there is a prior over policies
    Description: 
    Description Embedding: 
    Comments: None

  ../test/test_control_jax.py::generate_model_params:
    name: generate_model_params
    Module: None
    Signature: generate_model_params()
    test_exe_cmd: test.test_control_jax.generate_model_params()
    Parameters: None
    Returns: None
    Start Line: 25
    End Line: 48
    Complexity: 2
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Generate random model dimensions
    Description: 
    Description Embedding: 
    Comments: None

  ../test/test_control_jax.py::test_get_expected_obs_factorized:
    name: test_get_expected_obs_factorized
    Module: None
    Signature: test_get_expected_obs_factorized(self)
    test_exe_cmd: test.test_control_jax.TestControlJax.test_get_expected_obs_factorized(self)
    Parameters: ['self']
    Returns: None
    Start Line: 52
    End Line: 70
    Complexity: 3
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Tests the jax-ified version of computations of expected observations under some hidden states and policy
    Description: 
    Description Embedding: 
    Comments: ['need to wrap `qs` in list because `get_expected_obs_factorized` expects a list of `qs` (representing multiple timesteps)', 'need to extract first index of `qo_validation` because `get_expected_obs_factorized` returns a list of `qo` (representing multiple timesteps)']

  ../test/test_control_jax.py::test_info_gain_factorized:
    name: test_info_gain_factorized
    Module: None
    Signature: test_info_gain_factorized(self)
    test_exe_cmd: test.test_control_jax.TestControlJax.test_info_gain_factorized(self)
    Parameters: ['self']
    Returns: None
    Start Line: 72
    End Line: 140
    Complexity: 6
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Unit test the `calc_states_info_gain_factorized` function by qualitatively checking that in the T-Maze (contextual bandit)
        example, the state info gain is higher for the policy that leads to visiting the cue, which is higher than state info gain
        for visiting the bandit arm, which in turn is higher than the state info gain for the policy that leads to staying in the start state.
    Description: 
    Description Embedding: 
    Comments: ['cue statistics', 'bandit statistics (mapping between reward-state (first hidden state factor) and rewards (Good vs Bad))', "agent believes it's in the start state", "agent believes it's in the arm-visiting state", "agent believes it's in the cue-visiting state"]

  ../test/test_demos.py::test_agent_demo:
    name: test_agent_demo
    Module: None
    Signature: test_agent_demo(self)
    test_exe_cmd: test.test_demos.TestDemos.test_agent_demo(self)
    Parameters: ['self']
    Returns: None
    Start Line: 16
    End Line: 54
    Complexity: 4
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: This unit test runs a more concise version of the
        code in the `agent_demo.ipynb` tutorial Jupyter notebook and the `agent_demo` Python script 
        to make sure the code works whenever we change something.
    Description: 
    Description Embedding: 
    Comments: ['transition/observation matrices characterising the generative process', 'initial state', 'number of timesteps in the simulation', 'initial observation -- no evidence for which arm is rewarding, neutral reward observation, and see themselves in the starting state', "initial (true) state -- the reward condition is highly rewarding, and the true position in the 'start' position", 'update agent', 'update environment']

  ../test/test_demos.py::test_gridworld_activeinference:
    name: test_gridworld_activeinference
    Module: None
    Signature: test_gridworld_activeinference(self)
    test_exe_cmd: test.test_demos.TestDemos.test_gridworld_activeinference(self)
    Parameters: ['self']
    Returns: None
    Start Line: 256
    End Line: 472
    Complexity: 8
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: This unit test runs the a concise version of the code in the `gridworld_tutorial_1.ipynb` tutorial notebook to make sure it works if things are changed
    Description: 
    Description Embedding: 
    Comments: ['@NOTE: we use the `spm_log_single` helper function from the `maths` sub-library of pymdp. This is a numerically stable version of np.log()', 'def plot_empirical_prior(B):', 'fig, axes = plt.subplots(3,2, figsize=(8, 10))', "actions = ['UP', 'RIGHT', 'DOWN', 'LEFT', 'STAY']", 'count = 0', 'for i in range(3):', 'for j in range(2):', 'if count >= 5:', 'break', 'g = sns.heatmap(B[:,:,count], cmap="OrRd", linewidth=2.5, cbar=False, ax=axes[i,j])', 'g.set_title(actions[count])', 'count += 1', 'fig.delaxes(axes.flatten()[5])', 'plt.tight_layout()', 'def plot_transition(B):', 'fig, axes = plt.subplots(2,3, figsize = (15,8))', 'a = list(actions.keys())', 'count = 0', 'for i in range(dim-1):', 'for j in range(dim):', 'if count >= 5:', 'break', 'g = sns.heatmap(B[:,:,count], cmap = "OrRd", linewidth = 2.5, cbar = False, ax = axes[i,j], xticklabels=labels, yticklabels=labels)', 'g.set_title(a[count])', 'count +=1', 'fig.delaxes(axes.flatten()[5])', 'plt.tight_layout()', 'plot_likelihood(A)', 'plot_transition(B)', 'def perform_inference(likelihood, prior):', 'return softmax(log_stable(likelihood) + log_stable(prior))', 'plot_beliefs(Qs)', 'plot_beliefs(C)', 'initialize expected free energy at 0', 'loop over policy', 'get action entailed by the policy at timestep `t`', 'work out expected state, given the action', 'work out expected observations, given the action', 'get entropy', 'get predicted divergence', 'divergence = np.sum(Qo_pi * (log_stable(Qo_pi) - log_stable(C)), axis=0)', 'compute the expected uncertainty or ambiguity', 'increment the expected free energy counter for the policy, using the expected free energy at this timestep', 'initialize the negative expected free energy', 'loop over every possible policy and compute the EFE of each policy', 'get distribution over policies', 'initialize probabilites of control states (convert from policies to actions)', 'sum probabilites of control states or actions', 'control state specified by policy', 'add probability of policy', 'normalize action marginal', 'sample control from action marginal', 'number of time steps', 'n_actions = env.n_control', 'length of policies we consider', 'this function generates all possible combinations of policies', 'reset environment', 'loop over time', 'infer which action to take', 'perform action in the environment and update the environment', 'infer new hidden state (this is the same equation as above but with PyMDP functions)', 'plot_beliefs(Qs, "Beliefs (Qs) at time {}".format(t))', 'self.assertEqual(np.argmax(Qs), REWARD_LOCATION) # @NOTE: This is not always true due to stochastic samplign!!!']

  ../test/test_demos.py::test_gridworld_genmodel_construction:
    name: test_gridworld_genmodel_construction
    Module: None
    Signature: test_gridworld_genmodel_construction(self)
    test_exe_cmd: test.test_demos.TestDemos.test_gridworld_genmodel_construction(self)
    Parameters: ['self']
    Returns: None
    Start Line: 190
    End Line: 241
    Complexity: 5
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: This unit test runs the a concise version of the code in the `gridworld_tutorial_1.ipynb` tutorial notebook to make sure it works if things are changed
    Description: 
    Description Embedding: 
    Comments: ["rows are the y-coordinate, columns are the x-coordinate -- so we index into the grid we'll be visualizing using '[y, x]'", 'plot_likelihood(A)']

  ../test/test_demos.py::test_tmaze_demo:
    name: test_tmaze_demo
    Module: None
    Signature: test_tmaze_demo(self)
    test_exe_cmd: test.test_demos.TestDemos.test_tmaze_demo(self)
    Parameters: ['self']
    Returns: None
    Start Line: 62
    End Line: 115
    Complexity: 5
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: This unit test runs the a concise version of the code in the `tmaze_demo.ipynb` tutorial notebook to make sure it works if things are changed
    Description: 
    Description Embedding: 
    Comments: ['probabilities used in the original SPM T-maze demo', "plot_likelihood(A_gp[1][:,:,0],'Reward Right')", "plot_likelihood(B_gp[1][:,:,0],'Reward Condition Transitions')", 'make a copy of the true observation likelihood to initialize the observation model', 'make a copy of the true transition likelihood to initialize the transition model', 'plot_beliefs(agent.D[0],"Beliefs about initial location")', 'they like reward', "they don't like loss", 'number of timesteps', 'reset the environment and get an initial observation', 'these are useful for displaying read-outs during the loop over time', 'if the reward condition is Reward on RIGHT', "this tests that the cue observation is 'Cue Right' in case of 'Reward on Right' condition", 'if the reward condition is Reward on RIGHT', "this tests that the cue observation is 'Cue Left' in case of 'Reward on Left' condition"]

  ../test/test_demos.py::test_tmaze_learning_demo:
    name: test_tmaze_learning_demo
    Module: None
    Signature: test_tmaze_learning_demo(self)
    test_exe_cmd: test.test_demos.TestDemos.test_tmaze_learning_demo(self)
    Parameters: ['self']
    Returns: None
    Start Line: 120
    End Line: 188
    Complexity: 4
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: This unit test runs the a concise version of the code in the `tmaze_demo_learning.ipynb` tutorial notebook to make sure it works if things are changed
    Description: 
    Description Embedding: 
    Comments: ["the 'true' reward probabilities", 'this is a list of the indices of the hidden state factors that are controllable', 'this is a list of the modalities that you want to be learn-able', 'number of timesteps', 'reset the environment and get an initial observation', 'make sure they are learning the reward contingencies in the right general direction', "in case the reward condition is 'Reward on RIGHT'", "in case the reward condition is 'Reward on LEFT'"]

  ../test/test_fpi.py::test_factorized_fpi_multi_factor_multi_modality:
    name: test_factorized_fpi_multi_factor_multi_modality
    Module: None
    Signature: test_factorized_fpi_multi_factor_multi_modality(self)
    test_exe_cmd: test.test_fpi.TestFPI.test_factorized_fpi_multi_factor_multi_modality(self)
    Parameters: ['self']
    Returns: None
    Start Line: 94
    End Line: 125
    Complexity: 4
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Test the sparsified version of `run_vanilla_fpi`, named `run_vanilla_fpi_factorized`
        with multiple hidden state factors and multiple observation modalities.
    Description: 
    Description Embedding: 
    Comments: ['test it also without computing VFE (i.e. with `compute_vfe=False`)']

  ../test/test_fpi.py::test_factorized_fpi_multi_factor_multi_modality_with_condind:
    name: test_factorized_fpi_multi_factor_multi_modality_with_condind
    Module: None
    Signature: test_factorized_fpi_multi_factor_multi_modality_with_condind(self)
    test_exe_cmd: test.test_fpi.TestFPI.test_factorized_fpi_multi_factor_multi_modality_with_condind(self)
    Parameters: ['self']
    Returns: None
    Start Line: 127
    End Line: 161
    Complexity: 4
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Test the sparsified version of `run_vanilla_fpi`, named `run_vanilla_fpi_factorized`
        with multiple hidden state factors and multiple observation modalities, where some modalities only depend on some factors.
    Description: 
    Description Embedding: 
    Comments: ['list of the factors that modality `m` does not depend on', 'broadcast or tile the reduced A matrix (`A_reduced`) along the dimensions of corresponding to `other_factors`']

  ../test/test_fpi.py::test_factorized_fpi_multi_factor_one_modality:
    name: test_factorized_fpi_multi_factor_one_modality
    Module: None
    Signature: test_factorized_fpi_multi_factor_one_modality(self)
    test_exe_cmd: test.test_fpi.TestFPI.test_factorized_fpi_multi_factor_one_modality(self)
    Parameters: ['self']
    Returns: None
    Start Line: 69
    End Line: 92
    Complexity: 2
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Test the sparsified version of `run_vanilla_fpi`, named `run_vanilla_fpi_factorized`
        with multiple hidden state factors and one observation modality.
    Description: 
    Description Embedding: 
    Comments: None

  ../test/test_fpi.py::test_factorized_fpi_multi_factor_single_modality_with_condind:
    name: test_factorized_fpi_multi_factor_single_modality_with_condind
    Module: None
    Signature: test_factorized_fpi_multi_factor_single_modality_with_condind(self)
    test_exe_cmd: test.test_fpi.TestFPI.test_factorized_fpi_multi_factor_single_modality_with_condind(self)
    Parameters: ['self']
    Returns: None
    Start Line: 163
    End Line: 199
    Complexity: 4
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Test the sparsified version of `run_vanilla_fpi`, named `run_vanilla_fpi_factorized`
        with multiple hidden state factors and one observation modality, where the modality only depend on some factors.
    Description: 
    Description Embedding: 
    Comments: ['list of the factors that modality `m` does not depend on', 'broadcast or tile the reduced A matrix (`A_reduced`) along the dimensions of corresponding to `other_factors`']

  ../test/test_fpi.py::test_factorized_fpi_one_factor_multi_modality:
    name: test_factorized_fpi_one_factor_multi_modality
    Module: None
    Signature: test_factorized_fpi_one_factor_multi_modality(self)
    test_exe_cmd: test.test_fpi.TestFPI.test_factorized_fpi_one_factor_multi_modality(self)
    Parameters: ['self']
    Returns: None
    Start Line: 44
    End Line: 67
    Complexity: 2
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Test the sparsified version of `run_vanilla_fpi`, named `run_vanilla_fpi_factorized`
        with single hidden state factor and multiple observation modalities.
    Description: 
    Description Embedding: 
    Comments: None

  ../test/test_fpi.py::test_factorized_fpi_one_factor_one_modality:
    name: test_factorized_fpi_one_factor_one_modality
    Module: None
    Signature: test_factorized_fpi_one_factor_one_modality(self)
    test_exe_cmd: test.test_fpi.TestFPI.test_factorized_fpi_one_factor_one_modality(self)
    Parameters: ['self']
    Returns: None
    Start Line: 18
    End Line: 42
    Complexity: 1
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Test the sparsified version of `run_vanilla_fpi`, named `run_vanilla_fpi_factorized`
        with single hidden state factor and single observation modality.
    Description: 
    Description Embedding: 
    Comments: None

  ../test/test_inference.py::test_update_posterior_states:
    name: test_update_posterior_states
    Module: None
    Signature: test_update_posterior_states(self)
    test_exe_cmd: test.test_inference.TestInference.test_update_posterior_states(self)
    Parameters: ['self']
    Returns: None
    Start Line: 18
    End Line: 109
    Complexity: 6
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Tests the refactored version of `update_posterior_states`
    Description: 
    Description Embedding: 
    Comments: ["validate with a quick n' dirty implementation of FPI", 'initialize posterior and log prior', 'which axes to sum out']

  ../test/test_inference.py::test_update_posterior_states_factorized:
    name: test_update_posterior_states_factorized
    Module: None
    Signature: test_update_posterior_states_factorized(self)
    test_exe_cmd: test.test_inference.TestInference.test_update_posterior_states_factorized(self)
    Parameters: ['self']
    Returns: None
    Start Line: 142
    End Line: 174
    Complexity: 3
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Tests the version of `update_posterior_states` where an `mb_dict` is provided as an argument to factorize
        the fixed-point iteration (FPI) algorithm.
    Description: 
    Description Embedding: 
    Comments: ['list of the factors that modality `m` does not depend on', 'broadcast or tile the reduced A matrix (`A_reduced`) along the dimensions of corresponding to `other_factors`']

  ../test/test_inference.py::test_update_posterior_states_factorized_noVFE_compute:
    name: test_update_posterior_states_factorized_noVFE_compute
    Module: None
    Signature: test_update_posterior_states_factorized_noVFE_compute(self)
    test_exe_cmd: test.test_inference.TestInference.test_update_posterior_states_factorized_noVFE_compute(self)
    Parameters: ['self']
    Returns: None
    Start Line: 176
    End Line: 210
    Complexity: 3
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Tests the version of `update_posterior_states` where an `mb_dict` is provided as an argument to factorize
        the fixed-point iteration (FPI) algorithm.

        In this version, we always run the total number of iterations because we don't compute the variational free energy over the course of convergence/optimization.
    Description: 
    Description Embedding: 
    Comments: ['list of the factors that modality `m` does not depend on', 'broadcast or tile the reduced A matrix (`A_reduced`) along the dimensions of corresponding to `other_factors`']

  ../test/test_inference.py::test_update_posterior_states_factorized_single_factor:
    name: test_update_posterior_states_factorized_single_factor
    Module: None
    Signature: test_update_posterior_states_factorized_single_factor(self)
    test_exe_cmd: test.test_inference.TestInference.test_update_posterior_states_factorized_single_factor(self)
    Parameters: ['self']
    Returns: None
    Start Line: 111
    End Line: 140
    Complexity: 1
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Tests the version of `update_posterior_states` where an `mb_dict` is provided as an argument to factorize
        the fixed-point iteration (FPI) algorithm. Single factor version.
    Description: 
    Description Embedding: 
    Comments: None

  ../test/test_inference_jax.py::test_fixed_point_iteration_index_observations:
    name: test_fixed_point_iteration_index_observations
    Module: None
    Signature: test_fixed_point_iteration_index_observations(self)
    test_exe_cmd: test.test_inference_jax.TestInferenceJax.test_fixed_point_iteration_index_observations(self)
    Parameters: ['self']
    Returns: None
    Start Line: 184
    End Line: 231
    Complexity: 5
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Tests the jax-ified version of mean-field fixed-point iteration against the original NumPy version.
        In this version there are multiple hidden state factors and multiple observation modalities.

        Test the jax version with index-based observations (not one-hots)
    Description: 
    Description Embedding: 
    Comments: ['numpy version', 'set dF_tol to negative number so numpy version of FPI never stops early due to convergence', 'jax version', 'obs = [jnp.array(o_m) for o_m in obs]']

  ../test/test_inference_jax.py::test_fixed_point_iteration_multistate_multiobs:
    name: test_fixed_point_iteration_multistate_multiobs
    Module: None
    Signature: test_fixed_point_iteration_multistate_multiobs(self)
    test_exe_cmd: test.test_inference_jax.TestInferenceJax.test_fixed_point_iteration_multistate_multiobs(self)
    Parameters: ['self']
    Returns: None
    Start Line: 141
    End Line: 182
    Complexity: 4
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Tests the jax-ified version of mean-field fixed-point iteration against the original numpy version.
        In this version there are multiple hidden state factors and multiple observation modalities
    Description: 
    Description Embedding: 
    Comments: ['numpy version', 'set dF_tol to negative number so numpy version of FPI never stops early due to convergence', 'jax version']

  ../test/test_inference_jax.py::test_fixed_point_iteration_multistate_singleobs:
    name: test_fixed_point_iteration_multistate_singleobs
    Module: None
    Signature: test_fixed_point_iteration_multistate_singleobs(self)
    test_exe_cmd: test.test_inference_jax.TestInferenceJax.test_fixed_point_iteration_multistate_singleobs(self)
    Parameters: ['self']
    Returns: None
    Start Line: 100
    End Line: 138
    Complexity: 4
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Tests the jax-ified version of mean-field fixed-point iteration against the original numpy version.
        In this version there are multiple hidden state factors and a single observation modality
    Description: 
    Description Embedding: 
    Comments: ['numpy version', 'set dF_tol to negative number so numpy version of FPI never stops early due to convergence', 'jax version']

  ../test/test_inference_jax.py::test_fixed_point_iteration_singlestate_multiobs:
    name: test_fixed_point_iteration_singlestate_multiobs
    Module: None
    Signature: test_fixed_point_iteration_singlestate_multiobs(self)
    test_exe_cmd: test.test_inference_jax.TestInferenceJax.test_fixed_point_iteration_singlestate_multiobs(self)
    Parameters: ['self']
    Returns: None
    Start Line: 60
    End Line: 98
    Complexity: 4
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Tests the jax-ified version of mean-field fixed-point iteration against the original numpy version.
        In this version there is one hidden state factor and multiple observation modalities
    Description: 
    Description Embedding: 
    Comments: ['numpy version', 'set dF_tol to negative number so numpy version of FPI never stops early due to convergence', 'jax version']

  ../test/test_inference_jax.py::test_fixed_point_iteration_singlestate_singleobs:
    name: test_fixed_point_iteration_singlestate_singleobs
    Module: None
    Signature: test_fixed_point_iteration_singlestate_singleobs(self)
    test_exe_cmd: test.test_inference_jax.TestInferenceJax.test_fixed_point_iteration_singlestate_singleobs(self)
    Parameters: ['self']
    Returns: None
    Start Line: 20
    End Line: 58
    Complexity: 4
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Tests the jax-ified version of mean-field fixed-point iteration against the original numpy version.
        In this version there is one hidden state factor and one observation modality
    Description: 
    Description Embedding: 
    Comments: ['numpy version', 'set dF_tol to negative number so numpy version of FPI never stops early due to convergence', 'jax version']

  ../test/test_learning.py::test_prune_likelihoods:
    name: test_prune_likelihoods
    Module: None
    Signature: test_prune_likelihoods(self)
    test_exe_cmd: test.test_learning.TestLearning.test_prune_likelihoods(self)
    Parameters: ['self']
    Returns: None
    Start Line: 775
    End Line: 892
    Complexity: 4
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Test removing hidden state factor levels and/or observation levels from the likelihood arrays 
        of a generative model, using the `_prune_A` and `_prune_B` functions of the `learning` module
    Description: 
    Description Embedding: 
    Comments: None

  ../test/test_learning.py::test_prune_prior:
    name: test_prune_prior
    Module: None
    Signature: test_prune_prior(self)
    test_exe_cmd: test.test_learning.TestLearning.test_prune_prior(self)
    Parameters: ['self']
    Returns: None
    Start Line: 730
    End Line: 773
    Complexity: 2
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Test removing hidden state factor levels and/or observation levels from the priors vectors
        of a generative model, using the `_prune_prior` function of the `learning` module
    Description: 
    Description Embedding: 
    Comments: ['this could either be 4 hidden state levels, or 4 observation levels', 'this could either be 4 hidden state levels, or 4 observation levels', 'this could either be 4 hidden state levels, or 4 observation levels']

  ../test/test_learning.py::test_update_pA_diff_observation_formats:
    name: test_update_pA_diff_observation_formats
    Module: None
    Signature: test_update_pA_diff_observation_formats(self)
    test_exe_cmd: test.test_learning.TestLearning.test_update_pA_diff_observation_formats(self)
    Parameters: ['self']
    Returns: None
    Start Line: 194
    End Line: 251
    Complexity: 2
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Test for updating prior Dirichlet parameters over sensory likelihood (pA)
        in the case that observation is stored in various formats
    Description: 
    Description Embedding: 
    Comments: ['multiple observation modalities', 'now do the same for case of single modality']

  ../test/test_learning.py::test_update_pA_factorized:
    name: test_update_pA_factorized
    Module: None
    Signature: test_update_pA_factorized(self)
    test_exe_cmd: test.test_learning.TestLearning.test_update_pA_factorized(self)
    Parameters: ['self']
    Returns: None
    Start Line: 253
    End Line: 298
    Complexity: 3
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Test for `learning.update_obs_likelihood_dirichlet_factorized`, which is the learning function updating prior Dirichlet parameters over the sensory likelihood (pA) 
        in the case that the generative model is sparse and only some modalities depend on some hidden state factors
    Description: 
    Description Embedding: 
    Comments: ['sample some positive learning rate']

  ../test/test_learning.py::test_update_pA_multi_factor_all:
    name: test_update_pA_multi_factor_all
    Module: None
    Signature: test_update_pA_multi_factor_all(self)
    test_exe_cmd: test.test_learning.TestLearning.test_update_pA_multi_factor_all(self)
    Parameters: ['self']
    Returns: None
    Start Line: 107
    End Line: 138
    Complexity: 2
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Test for updating prior Dirichlet parameters over sensory likelihood (pA)
        in the case that all observation modalities are updated and the generative model 
        has multiple hidden state factors
    Description: 
    Description Embedding: 
    Comments: ['single observation modality', 'multiple observation modalities']

  ../test/test_learning.py::test_update_pA_multi_factor_one_modality:
    name: test_update_pA_multi_factor_one_modality
    Module: None
    Signature: test_update_pA_multi_factor_one_modality(self)
    test_exe_cmd: test.test_learning.TestLearning.test_update_pA_multi_factor_one_modality(self)
    Parameters: ['self']
    Returns: None
    Start Line: 140
    End Line: 165
    Complexity: 3
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Test for updating prior Dirichlet parameters over sensory likelihood (pA)
        in the case that ONE observation modalities is updated and the generative model 
        has multiple hidden state factors
    Description: 
    Description Embedding: 
    Comments: ['multiple observation modalities']

  ../test/test_learning.py::test_update_pA_multi_factor_some_modalities:
    name: test_update_pA_multi_factor_some_modalities
    Module: None
    Signature: test_update_pA_multi_factor_some_modalities(self)
    test_exe_cmd: test.test_learning.TestLearning.test_update_pA_multi_factor_some_modalities(self)
    Parameters: ['self']
    Returns: None
    Start Line: 167
    End Line: 192
    Complexity: 3
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Test for updating prior Dirichlet parameters over sensory likelihood (pA)
        in the case that SOME observation modalities are updated and the generative model 
        has multiple hidden state factors
    Description: 
    Description Embedding: 
    Comments: ['multiple observation modalities']

  ../test/test_learning.py::test_update_pA_single_factor_all:
    name: test_update_pA_single_factor_all
    Module: None
    Signature: test_update_pA_single_factor_all(self)
    test_exe_cmd: test.test_learning.TestLearning.test_update_pA_single_factor_all(self)
    Parameters: ['self']
    Returns: None
    Start Line: 10
    End Line: 45
    Complexity: 2
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Test for updating prior Dirichlet parameters over sensory likelihood (pA)
        in the case that all observation modalities are updated and the generative model 
        has a single hidden state factor
    Description: 
    Description Embedding: 
    Comments: None

  ../test/test_learning.py::test_update_pA_single_factor_one_modality:
    name: test_update_pA_single_factor_one_modality
    Module: None
    Signature: test_update_pA_single_factor_one_modality(self)
    test_exe_cmd: test.test_learning.TestLearning.test_update_pA_single_factor_one_modality(self)
    Parameters: ['self']
    Returns: None
    Start Line: 47
    End Line: 76
    Complexity: 3
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Test for updating prior Dirichlet parameters over sensory likelihood (pA)
        in the case that ONE observation modalities is updated and the generative model 
        has a single hidden state factor
    Description: 
    Description Embedding: 
    Comments: None

  ../test/test_learning.py::test_update_pA_single_factor_some_modalities:
    name: test_update_pA_single_factor_some_modalities
    Module: None
    Signature: test_update_pA_single_factor_some_modalities(self)
    test_exe_cmd: test.test_learning.TestLearning.test_update_pA_single_factor_some_modalities(self)
    Parameters: ['self']
    Returns: None
    Start Line: 78
    End Line: 105
    Complexity: 3
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Test for updating prior Dirichlet parameters over sensory likelihood (pA)
        in the case that some observation modalities are updated and the generative model 
        has a single hidden state factor
    Description: 
    Description Embedding: 
    Comments: None

  ../test/test_learning.py::test_update_pB_interactions:
    name: test_update_pB_interactions
    Module: None
    Signature: test_update_pB_interactions(self)
    test_exe_cmd: test.test_learning.TestLearning.test_update_pB_interactions(self)
    Parameters: ['self']
    Returns: None
    Start Line: 601
    End Line: 662
    Complexity: 4
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Test for `learning.update_state_likelihood_dirichlet_factorized`, which is the learning function updating prior Dirichlet parameters over the transition likelihood (pB) 
        in the case that there are allowable interactions between hidden state factors, i.e. the dynamics of factor `f` may depend on more than just its control factor and its own state.
    Description: 
    Description Embedding: 
    Comments: ['sample some positive learning rate', 'sample some positive learning rate']

  ../test/test_learning.py::test_update_pB_multi_factor_no_actions_all_factors:
    name: test_update_pB_multi_factor_no_actions_all_factors
    Module: None
    Signature: test_update_pB_multi_factor_no_actions_all_factors(self)
    test_exe_cmd: test.test_learning.TestLearning.test_update_pB_multi_factor_no_actions_all_factors(self)
    Parameters: ['self']
    Returns: None
    Start Line: 356
    End Line: 386
    Complexity: 2
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Test for updating prior Dirichlet parameters over transition likelihood (pB)
        in the case that there are mulitple hidden state factors, and there 
        are no actions. All factors are updated.
    Description: 
    Description Embedding: 
    Comments: None

  ../test/test_learning.py::test_update_pB_multi_factor_no_actions_one_factor:
    name: test_update_pB_multi_factor_no_actions_one_factor
    Module: None
    Signature: test_update_pB_multi_factor_no_actions_one_factor(self)
    test_exe_cmd: test.test_learning.TestLearning.test_update_pB_multi_factor_no_actions_one_factor(self)
    Parameters: ['self']
    Returns: None
    Start Line: 388
    End Line: 422
    Complexity: 3
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Test for updating prior Dirichlet parameters over transition likelihood (pB)
        in the case that there are mulitple hidden state factors, and there 
        are no actions. One factor is updated
    Description: 
    Description Embedding: 
    Comments: None

  ../test/test_learning.py::test_update_pB_multi_factor_no_actions_some_factors:
    name: test_update_pB_multi_factor_no_actions_some_factors
    Module: None
    Signature: test_update_pB_multi_factor_no_actions_some_factors(self)
    test_exe_cmd: test.test_learning.TestLearning.test_update_pB_multi_factor_no_actions_some_factors(self)
    Parameters: ['self']
    Returns: None
    Start Line: 424
    End Line: 458
    Complexity: 3
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Test for updating prior Dirichlet parameters over transition likelihood (pB)
        in the case that there are mulitple hidden state factors, and there 
        are no actions. Some factors are updated.
    Description: 
    Description Embedding: 
    Comments: None

  ../test/test_learning.py::test_update_pB_multi_factor_some_controllable_some_factors:
    name: test_update_pB_multi_factor_some_controllable_some_factors
    Module: None
    Signature: test_update_pB_multi_factor_some_controllable_some_factors(self)
    test_exe_cmd: test.test_learning.TestLearning.test_update_pB_multi_factor_some_controllable_some_factors(self)
    Parameters: ['self']
    Returns: None
    Start Line: 565
    End Line: 599
    Complexity: 3
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Test for updating prior Dirichlet parameters over transition likelihood (pB)
        in the case that there are mulitple hidden state factors, some of which 
        are controllable. Some factors are updated.
    Description: 
    Description Embedding: 
    Comments: None

  ../test/test_learning.py::test_update_pB_multi_factor_with_actions_all_factors:
    name: test_update_pB_multi_factor_with_actions_all_factors
    Module: None
    Signature: test_update_pB_multi_factor_with_actions_all_factors(self)
    test_exe_cmd: test.test_learning.TestLearning.test_update_pB_multi_factor_with_actions_all_factors(self)
    Parameters: ['self']
    Returns: None
    Start Line: 460
    End Line: 491
    Complexity: 2
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Test for updating prior Dirichlet parameters over transition likelihood (pB)
        in the case that there are mulitple hidden state factors, and there 
        are actions. All factors are updated
    Description: 
    Description Embedding: 
    Comments: None

  ../test/test_learning.py::test_update_pB_multi_factor_with_actions_one_factor:
    name: test_update_pB_multi_factor_with_actions_one_factor
    Module: None
    Signature: test_update_pB_multi_factor_with_actions_one_factor(self)
    test_exe_cmd: test.test_learning.TestLearning.test_update_pB_multi_factor_with_actions_one_factor(self)
    Parameters: ['self']
    Returns: None
    Start Line: 493
    End Line: 527
    Complexity: 3
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Test for updating prior Dirichlet parameters over transition likelihood (pB)
        in the case that there are mulitple hidden state factors, and there 
        are actions. One factor is updated
    Description: 
    Description Embedding: 
    Comments: None

  ../test/test_learning.py::test_update_pB_multi_factor_with_actions_some_factors:
    name: test_update_pB_multi_factor_with_actions_some_factors
    Module: None
    Signature: test_update_pB_multi_factor_with_actions_some_factors(self)
    test_exe_cmd: test.test_learning.TestLearning.test_update_pB_multi_factor_with_actions_some_factors(self)
    Parameters: ['self']
    Returns: None
    Start Line: 529
    End Line: 563
    Complexity: 3
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Test for updating prior Dirichlet parameters over transition likelihood (pB)
        in the case that there are mulitple hidden state factors, and there 
        are actions. Some factors are updated
    Description: 
    Description Embedding: 
    Comments: None

  ../test/test_learning.py::test_update_pB_single_factor_no_actions:
    name: test_update_pB_single_factor_no_actions
    Module: None
    Signature: test_update_pB_single_factor_no_actions(self)
    test_exe_cmd: test.test_learning.TestLearning.test_update_pB_single_factor_no_actions(self)
    Parameters: ['self']
    Returns: None
    Start Line: 300
    End Line: 326
    Complexity: 1
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Test for updating prior Dirichlet parameters over transition likelihood (pB)
        in the case that the one and only hidden state factor is updated, and there 
        are no actions.
    Description: 
    Description Embedding: 
    Comments: ["this is how we encode the fact that there aren't any actions"]

  ../test/test_learning.py::test_update_pB_single_factor_with_actions:
    name: test_update_pB_single_factor_with_actions
    Module: None
    Signature: test_update_pB_single_factor_with_actions(self)
    test_exe_cmd: test.test_learning.TestLearning.test_update_pB_single_factor_with_actions(self)
    Parameters: ['self']
    Returns: None
    Start Line: 328
    End Line: 354
    Complexity: 1
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Test for updating prior Dirichlet parameters over transition likelihood (pB)
        in the case that the one and only hidden state factor is updated, and there 
        are actions.
    Description: 
    Description Embedding: 
    Comments: None

  ../test/test_learning.py::test_update_pD:
    name: test_update_pD
    Module: None
    Signature: test_update_pD(self)
    test_exe_cmd: test.test_learning.TestLearning.test_update_pD(self)
    Parameters: ['self']
    Returns: None
    Start Line: 665
    End Line: 728
    Complexity: 5
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Test updating prior Dirichlet parameters over initial hidden states (pD). 
        Tests different cases
        1. Multiple vs. single hidden state factor
        2. One factor vs. several factors vs. all factors learned
    Description: 
    Description Embedding: 
    Comments: ['1. Single hidden state factor', '2. Multiple hidden state factors', '3. Multiple hidden state factors, only some learned']

  ../test/test_learning_jax.py::test_update_observation_likelihood_factorized:
    name: test_update_observation_likelihood_factorized
    Module: None
    Signature: test_update_observation_likelihood_factorized(self)
    test_exe_cmd: test.test_learning_jax.TestLearningJax.test_update_observation_likelihood_factorized(self)
    Parameters: ['self']
    Returns: None
    Start Line: 85
    End Line: 147
    Complexity: 4
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Testing JAX-ified version of updating Dirichlet posterior over observation likelihood parameters (qA is posterior, pA is prior, and A is expectation
        of likelihood wrt to current posterior over A, i.e. $A = E_{Q(A)}[P(o|s,A)]$.

        This is the factorized version where only some hidden state factors drive each modality (i.e. A_dependencies is a list of lists of hidden state factors)
    Description: 
    Description Embedding: 
    Comments: ['create numpy arrays to test numpy version of learning', 'create A matrix initialization (expected initial value of P(o|s, A)) and prior over A (pA)', 'create random observations', 'create random state posterior', 'run numpy version of learning']

  ../test/test_learning_jax.py::test_update_observation_likelihood_fullyconnected:
    name: test_update_observation_likelihood_fullyconnected
    Module: None
    Signature: test_update_observation_likelihood_fullyconnected(self)
    test_exe_cmd: test.test_learning_jax.TestLearningJax.test_update_observation_likelihood_fullyconnected(self)
    Parameters: ['self']
    Returns: None
    Start Line: 21
    End Line: 83
    Complexity: 4
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Testing JAX-ified version of updating Dirichlet posterior over observation likelihood parameters (qA is posterior, pA is prior, and A is expectation
        of likelihood wrt to current posterior over A, i.e. $A = E_{Q(A)}[P(o|s,A)]$.

        This is the so-called 'fully-connected' version where all hidden state factors drive each modality (i.e. A_dependencies is a list of lists of hidden state factors)
    Description: 
    Description Embedding: 
    Comments: ['create numpy arrays to test numpy version of learning', 'create A matrix initialization (expected initial value of P(o|s, A)) and prior over A (pA)', 'create random observations', 'create random state posterior', 'run numpy version of learning']

  ../test/test_message_passing_jax.py::make_A_full:
    name: make_A_full
    Module: None
    Signature: make_A_full(A_reduced, A_dependencies, num_obs, num_states)
    test_exe_cmd: test.test_message_passing_jax.make_A_full(A_reduced, A_dependencies, num_obs, num_states)
    Parameters: ['A_reduced', 'A_dependencies', 'num_obs', 'num_states']
    Returns: np.ndarray
    Start Line: 62
    End Line: 80
    Complexity: 2
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: Given a reduced A matrix, `A_reduced`, and a list of dependencies between hidden state factors and observation modalities, `A_dependencies`,
    return a full A matrix, `A_full`, where `A_full[m]` is the full A matrix for modality `m`. This means all redundant conditional independencies
    between observation modalities `m` and all hidden state factors (i.e. `range(len(num_states))`) are represented as lagging dimensions in `A_full`.
    Description: 
    Description Embedding: 
    Comments: ['initialize the full likelihood tensor (ALL modalities might depend on ALL factors)', 'indices of all hidden state factors', 'Step 1. Extract the list of the factors that modality `m` does NOT depend on', 'Step 2. broadcast or tile the reduced A matrix (`A_reduced`) along the dimensions of corresponding to `non_dependent_factors`, to give it the full shape of `(num_obs[m], *num_states)`']

  ../test/test_message_passing_jax.py::make_model_configs:
    name: make_model_configs
    Module: None
    Signature: make_model_configs(source_seed, num_models)
    test_exe_cmd: test.test_message_passing_jax.make_model_configs(source_seed, num_models)
    Parameters: ['source_seed', 'num_models']
    Returns: Dict
    Start Line: 30
    End Line: 60
    Complexity: 2
    Is Method: False
    Is Async: False
    Decorators: None
    Docstring: None
    Description: 
    Description Embedding: 
    Comments: ['list of total numbers of hidden state factors per model', 'this is the number of factors that each modality depends on', 'this is the number of factors that each factor depends on']

  ../test/test_message_passing_jax.py::test_fixed_point_iteration:
    name: test_fixed_point_iteration
    Module: None
    Signature: test_fixed_point_iteration(self)
    test_exe_cmd: test.test_message_passing_jax.TestMessagePassing.test_fixed_point_iteration(self)
    Parameters: ['self']
    Returns: None
    Start Line: 84
    End Line: 111
    Complexity: 4
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: None
    Description: 
    Description Embedding: 
    Comments: ['numpy version', 'set dF_tol to negative number so numpy version of FPI never stops early due to convergence', 'jax version']

  ../test/test_message_passing_jax.py::test_fixed_point_iteration_factorized_fullyconnected:
    name: test_fixed_point_iteration_factorized_fullyconnected
    Module: None
    Signature: test_fixed_point_iteration_factorized_fullyconnected(self)
    test_exe_cmd: test.test_message_passing_jax.TestMessagePassing.test_fixed_point_iteration_factorized_fullyconnected(self)
    Parameters: ['self']
    Returns: None
    Start Line: 114
    End Line: 146
    Complexity: 4
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Test the factorized version of `run_vanilla_fpi`, named `run_factorized_fpi`
        with multiple hidden state factors and multiple observation modalities.
    Description: 
    Description Embedding: 
    Comments: ['initialize arrays in numpy version', 'jax version']

  ../test/test_message_passing_jax.py::test_fixed_point_iteration_factorized_sparsegraph:
    name: test_fixed_point_iteration_factorized_sparsegraph
    Module: None
    Signature: test_fixed_point_iteration_factorized_sparsegraph(self)
    test_exe_cmd: test.test_message_passing_jax.TestMessagePassing.test_fixed_point_iteration_factorized_sparsegraph(self)
    Parameters: ['self']
    Returns: None
    Start Line: 148
    End Line: 187
    Complexity: 4
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Test the factorized version of `run_vanilla_fpi`, named `run_factorized_fpi`
        with multiple hidden state factors and multiple observation modalities, and with sparse conditional dependence relationships between hidden states
        and observation modalities
    Description: 
    Description Embedding: 
    Comments: ['jax version', 'create the full A matrix, where all hidden state factors are represented in the lagging dimensions of each sub-A array', 'jax version']

  ../test/test_message_passing_jax.py::test_marginal_message_passing:
    name: test_marginal_message_passing
    Module: None
    Signature: test_marginal_message_passing(self)
    test_exe_cmd: test.test_message_passing_jax.TestMessagePassing.test_marginal_message_passing(self)
    Parameters: ['self']
    Returns: None
    Start Line: 189
    End Line: 241
    Complexity: 3
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: None
    Description: 
    Description Embedding: 
    Comments: ['create a version of a_deps_i where each sub-list is sorted', 'move observations into leading dimensions', 'move u_t to the rightmost axis of the array', 's_t+1 to the leading dimension of the array', '# create a policy-dependent sequence of B matrices, but now we store the sequence dimension (action indices) in the first dimension (0th dimension is still batch dimension)']

  ../test/test_mmp.py::test_mmp_a:
    name: test_mmp_a
    Module: None
    Signature: test_mmp_a(self)
    test_exe_cmd: test.test_mmp.MMP.test_mmp_a(self)
    Parameters: ['self']
    Returns: None
    Start Line: 24
    End Line: 62
    Complexity: 3
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Testing our SPM-ified version of `run_MMP` with
            1 hidden state factor & 1 outcome modality, at a random fixed
            timestep during the generative process
    Description: 
    Description Embedding: 
    Comments: None

  ../test/test_mmp.py::test_mmp_b:
    name: test_mmp_b
    Module: None
    Signature: test_mmp_b(self)
    test_exe_cmd: test.test_mmp.MMP.test_mmp_b(self)
    Parameters: ['self']
    Returns: None
    Start Line: 64
    End Line: 95
    Complexity: 2
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Testing our SPM-ified version of `run_MMP` with
        2 hidden state factors & 2 outcome modalities, at a random fixed
        timestep during the generative process
    Description: 
    Description Embedding: 
    Comments: None

  ../test/test_mmp.py::test_mmp_c:
    name: test_mmp_c
    Module: None
    Signature: test_mmp_c(self)
    test_exe_cmd: test.test_mmp.MMP.test_mmp_c(self)
    Parameters: ['self']
    Returns: None
    Start Line: 97
    End Line: 129
    Complexity: 2
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Testing our SPM-ified version of `run_MMP` with
         2 hidden state factors & 2 outcome modalities, at the very first
         timestep of the generative process (boundary condition test). So there 
         are no previous actions
    Description: 
    Description Embedding: 
    Comments: ['prev_actions = mat_contents["previous_actions"].astype("int64") - 1', 'prev_actions = prev_actions[(max(0, curr_t - t_horizon)) :, :]']

  ../test/test_mmp.py::test_mmp_d:
    name: test_mmp_d
    Module: None
    Signature: test_mmp_d(self)
    test_exe_cmd: test.test_mmp.MMP.test_mmp_d(self)
    Parameters: ['self']
    Returns: None
    Start Line: 131
    End Line: 171
    Complexity: 3
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Testing our SPM-ified version of `run_MMP` with
        2 hidden state factors & 2 outcome modalities, at the final
        timestep of the generative process (boundary condition test)
        @NOTE: mmp_d.mat test has issues with the prediction errors. But the future messages are 
        totally fine (even at the last timestep of variational iteration.
    Description: 
    Description Embedding: 
    Comments: None

  ../test/test_utils.py::test_obj_array_from_list:
    name: test_obj_array_from_list
    Module: None
    Signature: test_obj_array_from_list(self)
    test_exe_cmd: test.test_utils.TestUtils.test_obj_array_from_list(self)
    Parameters: ['self']
    Returns: None
    Start Line: 17
    End Line: 25
    Complexity: 1
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Tests `obj_array_from_list`
    Description: 
    Description Embedding: 
    Comments: ['make arrays with same leading dimensions. naive method trigger numpy broadcasting error.']

  ../test/test_wrappers.py::test_get_model_dimensions_from_labels:
    name: test_get_model_dimensions_from_labels
    Module: None
    Signature: test_get_model_dimensions_from_labels(self)
    test_exe_cmd: test.test_wrappers.TestWrappers.test_get_model_dimensions_from_labels(self)
    Parameters: ['self']
    Returns: None
    Start Line: 8
    End Line: 55
    Complexity: 1
    Is Method: True
    Is Async: False
    Decorators: None
    Docstring: Tests model dimension extraction from labels including observations, states and actions.
    Description: 
    Description Embedding: 
    Comments: None